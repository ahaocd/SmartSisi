#!/usr/bin/env python3
"""
ğŸ§  Sisiäººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå™¨
æ”¶é›†æ‰€æœ‰ä¿¡æ¯ï¼Œç”ŸæˆçœŸæ­£äººæ€§åŒ–çš„æç¤ºè¯

ä¿¡æ¯æ”¶é›†æºï¼š
1. FunASR - è¯­éŸ³è¯†åˆ«ç»“æœ
2. YAMNet - éŸ³é¢‘ç¯å¢ƒåˆ†æ (éŸ³ä¹ã€å™ªéŸ³ã€äººæ•°)
3. Mem0 - å†å²è®°å¿†
4. RAG - ç›¸å…³çŸ¥è¯†
5. ç”¨æˆ·ç”»åƒ - è¯´è¯äººç‰¹å¾
6. æƒ…å¢ƒåˆ†æ - å½“å‰ç¯å¢ƒ

ä½¿ç”¨Kimi K2æ¨¡å‹ç”Ÿæˆäººæ€§åŒ–æç¤ºè¯
"""

import os
import sys
import json
import time
import logging
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from dataclasses import dataclass
import asyncio

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from utils import util

# è®¾ç½®æ—¥å¿—
def setup_human_prompt_logger():
    logger = logging.getLogger('human_prompt_generator')
    logger.setLevel(logging.INFO)
    
    log_dir = Path(util.ensure_log_dir("brain"))
    
    handler = logging.FileHandler(log_dir / "human_prompt_generator.log", encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s [äººæ€§åŒ–æç¤ºè¯] %(message)s')
    handler.setFormatter(formatter)
    
    if not logger.handlers:
        logger.addHandler(handler)
    
    return logger

human_logger = setup_human_prompt_logger()

@dataclass
class AudioEnvironment:
    """éŸ³é¢‘ç¯å¢ƒåˆ†æ"""
    has_music: bool = False
    music_type: str = "unknown"  # å¤å…¸ã€æµè¡Œã€æ‘‡æ»šç­‰
    noise_level: float = 0.0     # 0-1å™ªéŸ³ç­‰çº§
    people_count: int = 1        # ä¼°è®¡äººæ•°
    environment_type: str = "quiet"  # quiet, noisy, crowded, music
    audio_quality: float = 1.0   # éŸ³é¢‘è´¨é‡

@dataclass
class SpeakerProfile:
    """è¯´è¯äººç”»åƒ"""
    speaker_id: str = "unknown"
    familiarity: float = 0.0     # ç†Ÿæ‚‰åº¦
    interaction_count: int = 0   # äº¤äº’æ¬¡æ•°
    personality_traits: List[str] = None  # æ€§æ ¼ç‰¹å¾
    preferences: List[str] = None         # åå¥½
    mood_history: List[str] = None        # æƒ…ç»ªå†å²

@dataclass
class ContextInfo:
    """ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    current_time: str = ""
    conversation_topic: str = ""
    last_responses: List[str] = None
    user_intent: str = ""
    emotional_state: str = "neutral"

class SisiHumanPromptGenerator:
    """ğŸ§  Sisiäººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.config = self._load_config()
        self.kimi_client = None
        self._initialize_kimi()
        
        # äººæ€§åŒ–å›åº”æ¨¡æ¿
        self.human_responses = {
            "music_detected": [
                "å“‡ï¼Œè¿™é¦–æ­Œä¸é”™å‘¢ï¼",
                "å¬èµ·æ¥å¾ˆæœ‰æ„Ÿè§‰~",
                "è¿™ä¸ªèŠ‚å¥æˆ‘å–œæ¬¢",
                "éŸ³ä¹å“å‘³ä¸é”™å“¦"
            ],
            "noisy_environment": [
                "å¥½åµå•Šï¼Œè°åœ¨ä¸€ç›´å•°å—¦ï¼Ÿ",
                "è¿™ä¹ˆåµæ€ä¹ˆèŠå¤©å•Š",
                "èƒ½ä¸èƒ½å®‰é™ç‚¹ï¼Ÿ",
                "åµæ­»äº†ï¼Œè¯´è¯éƒ½å¬ä¸æ¸…"
            ],
            "crowded_situation": [
                "äººå¥½å¤šå•Š",
                "è¿™ä¹ˆçƒ­é—¹ï¼Ÿ",
                "å¤§å®¶éƒ½åœ¨å¹²å˜›å‘¢",
                "äººå£°é¼æ²¸çš„"
            ],
            "singing_detected": [
                "å”±å¾—ä¸é”™å‘¢ï¼",
                "æœ‰æ‰åå“¦~",
                "ç»§ç»­ç»§ç»­ï¼",
                "å£°éŸ³å¾ˆå¥½å¬"
            ]
        }
        
        human_logger.info("ğŸ§  Sisiäººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½® - ä»system.confè¯»å–å‰è„‘ç³»ç»Ÿé…ç½®"""
        try:
            import configparser

            # è¯»å–å‰è„‘ç³»ç»Ÿé…ç½®
            config_parser = configparser.ConfigParser()
            config_parser.read("system.conf", encoding='utf-8')

            # ä»system.confè¯»å–åŠ¨æ€æç¤ºè¯ç”Ÿæˆå™¨é…ç½®
            api_key = config_parser.get('key', 'prompt_generator_api_key', fallback='')
            base_url = config_parser.get('key', 'prompt_generator_base_url', fallback='https://api.siliconflow.cn/v1')
            model = config_parser.get('key', 'prompt_generator_model', fallback='moonshotai/Kimi-K2-Instruct')

            return {
                "provider": "siliconflow",
                "api_key": api_key,
                "base_url": base_url,
                "models": {
                    "prompt_generator": {
                        "llm_model": model
                    }
                }
            }
        except Exception as e:
            human_logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
            return self._default_config()
    
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            "provider": "siliconflow",
            "api_key": "",
            "base_url": "https://api.siliconflow.cn/v1",
            "models": {
                "prompt_generator": {
                    "llm_model": "moonshotai/Kimi-K2-Instruct"
                }
            }
        }
    
    def _initialize_kimi(self):
        """åˆå§‹åŒ–Kimi K2æ¨¡å‹"""
        try:
            import openai
            self.kimi_client = openai.OpenAI(
                api_key=self.config["api_key"],
                base_url=self.config["base_url"]
            )
            human_logger.info("âœ… Kimi K2æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            human_logger.error(f"âŒ Kimi K2åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def analyze_audio_environment(self, audio_path: str) -> AudioEnvironment:
        """åˆ†æéŸ³é¢‘ç¯å¢ƒ - è°ƒç”¨YAMNet"""
        env = AudioEnvironment()
        
        try:
            # è¿™é‡Œè°ƒç”¨YAMNetåˆ†æ
            # æ¨¡æ‹Ÿåˆ†æç»“æœ
            import random
            
            # æ£€æµ‹éŸ³ä¹
            music_classes = ["music", "singing", "piano", "guitar", "drums"]
            noise_classes = ["crowd", "traffic", "construction", "wind"]
            
            # ç®€å•æ¨¡æ‹Ÿ - å®é™…éœ€è¦è°ƒç”¨YAMNet
            if random.random() > 0.7:
                env.has_music = True
                env.music_type = random.choice(["æµè¡Œ", "å¤å…¸", "æ‘‡æ»š", "æ°‘è°£"])
            
            env.noise_level = random.uniform(0.1, 0.8)
            env.people_count = random.randint(1, 5)
            
            if env.noise_level > 0.6:
                env.environment_type = "noisy"
            elif env.people_count > 2:
                env.environment_type = "crowded"
            elif env.has_music:
                env.environment_type = "music"
            else:
                env.environment_type = "quiet"
            
            human_logger.info(f"ğŸµ éŸ³é¢‘ç¯å¢ƒåˆ†æ: {env.environment_type}, å™ªéŸ³{env.noise_level:.2f}")
            
        except Exception as e:
            human_logger.error(f"âŒ éŸ³é¢‘ç¯å¢ƒåˆ†æå¤±è´¥: {e}")
        
        return env
    
    def get_speaker_profile(self, speaker_id: str) -> SpeakerProfile:
        """è·å–è¯´è¯äººç”»åƒ - ä»è®°å¿†ç³»ç»Ÿ"""
        profile = SpeakerProfile(speaker_id=speaker_id)
        
        try:
            # è¿™é‡Œè°ƒç”¨Mem0è·å–ç”¨æˆ·ç”»åƒ
            # æ¨¡æ‹Ÿæ•°æ®
            profile.familiarity = 0.8
            profile.interaction_count = 15
            profile.personality_traits = ["æ´»æ³¼", "å¹½é»˜", "ç›´æ¥"]
            profile.preferences = ["éŸ³ä¹", "èŠå¤©", "å¼€ç©ç¬‘"]
            profile.mood_history = ["å¼€å¿ƒ", "å…´å¥‹", "å¥½å¥‡"]
            
            human_logger.info(f"ğŸ‘¤ è¯´è¯äººç”»åƒ: {speaker_id} - ç†Ÿæ‚‰åº¦{profile.familiarity:.2f}")
            
        except Exception as e:
            human_logger.error(f"âŒ è¯´è¯äººç”»åƒè·å–å¤±è´¥: {e}")
        
        return profile
    
    def get_context_info(self, text: str) -> ContextInfo:
        """è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = ContextInfo()
        
        try:
            import datetime
            context.current_time = datetime.datetime.now().strftime("%H:%M")
            
            # ç®€å•æ„å›¾è¯†åˆ«
            if any(word in text for word in ["å”±", "æ­Œ", "éŸ³ä¹"]):
                context.user_intent = "music_related"
            elif any(word in text for word in ["åµ", "å®‰é™", "å£°éŸ³"]):
                context.user_intent = "noise_complaint"
            else:
                context.user_intent = "general_chat"
            
            # æƒ…ç»ªè¯†åˆ«
            if any(word in text for word in ["å¼€å¿ƒ", "é«˜å…´", "å“ˆå“ˆ"]):
                context.emotional_state = "happy"
            elif any(word in text for word in ["çƒ¦", "åµ", "è®¨åŒ"]):
                context.emotional_state = "annoyed"
            else:
                context.emotional_state = "neutral"
            
            human_logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡åˆ†æ: {context.user_intent} - {context.emotional_state}")
            
        except Exception as e:
            human_logger.error(f"âŒ ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
        
        return context
    
    def generate_human_prompt(self, 
                            text: str, 
                            audio_path: str, 
                            speaker_id: str,
                            module_type: str) -> str:
        """ç”Ÿæˆäººæ€§åŒ–æç¤ºè¯ - ä¸»å‡½æ•°"""
        
        start_time = time.time()
        
        # 1. æ”¶é›†æ‰€æœ‰ä¿¡æ¯
        audio_env = self.analyze_audio_environment(audio_path)
        speaker_profile = self.get_speaker_profile(speaker_id)
        context_info = self.get_context_info(text)
        
        # 2. æ„å»ºä¿¡æ¯åŒ…
        info_package = {
            "ç”¨æˆ·è¾“å…¥": text,
            "éŸ³é¢‘ç¯å¢ƒ": {
                "ç¯å¢ƒç±»å‹": audio_env.environment_type,
                "æœ‰éŸ³ä¹": audio_env.has_music,
                "éŸ³ä¹ç±»å‹": audio_env.music_type,
                "å™ªéŸ³ç­‰çº§": audio_env.noise_level,
                "äººæ•°ä¼°è®¡": audio_env.people_count
            },
            "è¯´è¯äººç”»åƒ": {
                "ç†Ÿæ‚‰åº¦": speaker_profile.familiarity,
                "äº¤äº’æ¬¡æ•°": speaker_profile.interaction_count,
                "æ€§æ ¼ç‰¹å¾": speaker_profile.personality_traits,
                "åå¥½": speaker_profile.preferences
            },
            "ä¸Šä¸‹æ–‡": {
                "æ—¶é—´": context_info.current_time,
                "æ„å›¾": context_info.user_intent,
                "æƒ…ç»ª": context_info.emotional_state
            },
            "ç›®æ ‡æ¨¡å—": module_type
        }
        
        # 3. è°ƒç”¨Kimi K2ç”Ÿæˆäººæ€§åŒ–æç¤ºè¯
        human_prompt = self._call_kimi_k2(info_package)
        
        processing_time = round((time.time() - start_time) * 1000, 2)
        human_logger.info(f"ğŸ§  äººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå®Œæˆ ({processing_time}ms)")
        
        return human_prompt
    
    def _call_kimi_k2(self, info_package: Dict) -> str:
        """è°ƒç”¨Kimi K2ç”Ÿæˆæç¤ºè¯"""
        
        if not self.kimi_client:
            return self._fallback_prompt_generation(info_package)
        
        try:
            # æ„å»ºç»™Kimi K2çš„æç¤ºè¯
            system_prompt = """ä½ æ˜¯Sisiçš„äººç±»è¯­è¨€ç‰¹å¾åˆ†æå™¨å’Œæç¤ºè¯ç”Ÿæˆå™¨ã€‚

åŸºäºäººç±»è¯­è¨€å­¦å’Œå¿ƒç†å­¦ç‰¹å¾æ¡†æ¶ç”Ÿæˆæç¤ºè¯ï¼š

## äººç±»è¯­è¨€ç‰¹å¾æ¡†æ¶ï¼š

### 1. è¯­è¨€ä¹ æƒ¯ç‰¹å¾
- å£è¯­åŒ–è¡¨è¾¾ï¼šç”¨"å’‹æ ·"è€Œä¸æ˜¯"å¦‚ä½•"
- è¯­æ°”è¯ä½¿ç”¨ï¼šå•Šã€å‘¢ã€å“¦ã€å—¯ç­‰
- çœç•¥å’Œç®€åŒ–ï¼šè¯´"å¥½çš„"è€Œä¸æ˜¯"å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†"
- é‡å¤å¼ºè°ƒï¼šçœŸçš„çœŸçš„ã€å¥½å¥½å¥½

### 2. æƒ…æ„Ÿè¡¨è¾¾ç‰¹å¾
- æƒ…ç»ªè¯æ±‡ï¼šå¼€å¿ƒâ†’çˆ½ã€ç”Ÿæ°”â†’çƒ¦æ­»äº†
- è¯­è°ƒå˜åŒ–ï¼šç–‘é—®ã€æ„Ÿå¹ã€å¹³è¿°
- æƒ…æ„Ÿå¼ºåº¦ï¼šè½»å¾®ä¸æ»¡â†’ä¸¥é‡æŠ±æ€¨
- å…±æƒ…ååº”ï¼šç†è§£ã€å®‰æ…°ã€å…±é¸£

### 3. ç¤¾äº¤äº’åŠ¨ç‰¹å¾
- å…³ç³»è·ç¦»ï¼šç†Ÿäººéšæ„ã€é™Œç”Ÿäººç¤¼è²Œ
- è¯é¢˜è½¬æ¢ï¼šè‡ªç„¶è¿‡æ¸¡ã€çªç„¶è½¬æŠ˜
- äº’åŠ¨æœŸå¾…ï¼šå¸Œæœ›å›åº”ã€å¯»æ±‚è®¤åŒ
- ç¤¾äº¤ç¤¼ä»ªï¼šé—®å€™ã€é“è°¢ã€é“æ­‰

### 4. è®¤çŸ¥å¤„ç†ç‰¹å¾
- è”æƒ³æ€ç»´ï¼šä»Aæƒ³åˆ°Bå†åˆ°C
- ç»éªŒå¯¹æ¯”ï¼šè¿™ä¸ªåƒä¹‹å‰é‚£ä¸ª
- ç›´è§‰åˆ¤æ–­ï¼šæ„Ÿè§‰ã€è§‰å¾—ã€å¥½åƒ
- è®°å¿†æå–ï¼šæƒ³èµ·æ¥äº†ã€è®°å¾—

### 5. ç¯å¢ƒé€‚åº”ç‰¹å¾
- åœºæ™¯æ„ŸçŸ¥ï¼šå®‰é™æ—¶è½»å£°ã€åµé—¹æ—¶å¤§å£°
- æ°›å›´åŒ¹é…ï¼šä¸¥è‚ƒåœºåˆæ­£å¼ã€è½»æ¾æ—¶éšæ„
- æ—¶é—´æ„ŸçŸ¥ï¼šæ—©ä¸Šé—®å¥½ã€æ™šä¸Šé“åˆ«
- æ–‡åŒ–èƒŒæ™¯ï¼šæœ¬åœ°åŒ–è¡¨è¾¾ä¹ æƒ¯

è¯·åŸºäºè¿™ä¸ªæ¡†æ¶åˆ†æç”¨æˆ·è¾“å…¥å’Œç¯å¢ƒä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆäººç±»è¯­è¨€ç‰¹å¾çš„æç¤ºè¯ã€‚"""

            user_content = f"""ä¿¡æ¯åŒ…ï¼š
{json.dumps(info_package, ensure_ascii=False, indent=2)}

è¯·ä¸º{info_package['ç›®æ ‡æ¨¡å—']}ç”Ÿæˆäººæ€§åŒ–æç¤ºè¯ã€‚"""

            response = self.kimi_client.chat.completions.create(
                model=self.config["models"]["prompt_generator"]["llm_model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                temperature=0.8,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            human_logger.error(f"âŒ Kimi K2è°ƒç”¨å¤±è´¥: {e}")
            return self._fallback_prompt_generation(info_package)
    
    def _fallback_prompt_generation(self, info_package: Dict) -> str:
        """å¤‡ç”¨æç¤ºè¯ç”Ÿæˆ"""
        audio_env = info_package["éŸ³é¢‘ç¯å¢ƒ"]
        speaker_info = info_package["è¯´è¯äººç”»åƒ"]
        context = info_package["ä¸Šä¸‹æ–‡"]
        
        # æ ¹æ®ç¯å¢ƒé€‰æ‹©äººæ€§åŒ–å›åº”
        human_reactions = []
        
        if audio_env["ç¯å¢ƒç±»å‹"] == "noisy":
            human_reactions.extend(self.human_responses["noisy_environment"])
        elif audio_env["ç¯å¢ƒç±»å‹"] == "music":
            human_reactions.extend(self.human_responses["music_detected"])
        elif audio_env["ç¯å¢ƒç±»å‹"] == "crowded":
            human_reactions.extend(self.human_responses["crowded_situation"])
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        prompt = f"""[{info_package['ç›®æ ‡æ¨¡å—']}æç¤ºè¯]

å½“å‰æƒ…å†µï¼š{audio_env['ç¯å¢ƒç±»å‹']}ç¯å¢ƒï¼Œ{context['æƒ…ç»ª']}æƒ…ç»ª
ç”¨æˆ·ç‰¹å¾ï¼šç†Ÿæ‚‰åº¦{speaker_info['ç†Ÿæ‚‰åº¦']:.1f}ï¼Œ{speaker_info['æ€§æ ¼ç‰¹å¾']}
æ—¶é—´ï¼š{context['æ—¶é—´']}

ç”¨æˆ·è¯´ï¼š{info_package['ç”¨æˆ·è¾“å…¥']}

äººæ€§åŒ–å›åº”é£æ ¼ï¼š{', '.join(human_reactions[:2]) if human_reactions else 'è‡ªç„¶éšæ„'}

è¦æ±‚ï¼šåƒçœŸäººä¸€æ ·å›åº”ï¼Œä¸è¦åƒæœºå™¨äººã€‚"""

        return prompt

# å…¨å±€å®ä¾‹
_human_prompt_generator = None

def get_human_prompt_generator():
    """è·å–äººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå™¨å®ä¾‹"""
    global _human_prompt_generator
    if _human_prompt_generator is None:
        _human_prompt_generator = SisiHumanPromptGenerator()
    return _human_prompt_generator

def generate_human_prompts_for_modules(text: str, audio_path: str, speaker_id: str) -> Dict[str, str]:
    """ä¸ºä¸‰ä¸ªæ¨¡å—ç”Ÿæˆäººæ€§åŒ–æç¤ºè¯"""
    generator = get_human_prompt_generator()
    
    return {
        "quick_response": generator.generate_human_prompt(text, audio_path, speaker_id, "å¿«é€Ÿå“åº”æ¨¡å—"),
        "optimization_station": generator.generate_human_prompt(text, audio_path, speaker_id, "ä¼˜åŒ–ç«™"),
        "subscription_station": generator.generate_human_prompt(text, audio_path, speaker_id, "è®¢é˜…ç«™")
    }

def test_human_prompt_generator():
    """æµ‹è¯•äººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå™¨"""
    print("ğŸ§ª æµ‹è¯•Sisiäººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå™¨")
    print("="*60)
    
    generator = get_human_prompt_generator()
    
    test_cases = [
        ("å¥½åµå•Šï¼Œèƒ½ä¸èƒ½å®‰é™ç‚¹ï¼Ÿ", "noisy_audio.wav", "speaker_01"),
        ("è¿™é¦–æ­Œä¸é”™å‘¢", "music_audio.wav", "speaker_02"),
        ("ä½ å¥½ï¼Œæˆ‘æ˜¯æ–°ç”¨æˆ·", "quiet_audio.wav", "speaker_03")
    ]
    
    for text, audio_path, speaker_id in test_cases:
        print(f"\nğŸ’¬ æµ‹è¯•è¾“å…¥: {text}")
        
        prompts = generate_human_prompts_for_modules(text, audio_path, speaker_id)
        
        for module, prompt in prompts.items():
            print(f"\nğŸ¯ {module}:")
            print(prompt[:200] + "..." if len(prompt) > 200 else prompt)
    
    print("\nğŸ‰ äººæ€§åŒ–æç¤ºè¯ç”Ÿæˆå™¨æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_human_prompt_generator()
