#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠ¨æ€å¤§æ¨¡å‹ä¸­æ¢
åŠŸèƒ½ï¼šæŠ½å–éŸ³é¢‘åˆ†æ+è®°å¿†+RAGï¼Œæ³¨å…¥åˆ°å¿«é€Ÿå“åº”æ¨¡å‹(liusisi.py)
"""

import json
import time
import logging
import threading
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import configparser
import requests
from pathlib import Path
from utils import config_util as cfg

@dataclass
class DynamicContext:
    """åŠ¨æ€ä¸Šä¸‹æ–‡æ•°æ®"""
    audio_summary: str          # éŸ³é¢‘åˆ†ææ‘˜è¦
    memory_context: str         # è®°å¿†ä¸Šä¸‹æ–‡
    rag_context: str           # RAGçŸ¥è¯†ä¸Šä¸‹æ–‡
    interaction_suggestions: str # äº¤äº’å»ºè®®
    emotional_state: str       # æƒ…æ„ŸçŠ¶æ€åˆ†æ
    confidence: float          # æ•´ä½“ç½®ä¿¡åº¦
    timestamp: float           # ç”Ÿæˆæ—¶é—´æˆ³

class DynamicContextHub:
    """åŠ¨æ€å¤§æ¨¡å‹ä¸­æ¢"""
    
    def __init__(self, config_path: str = "system.conf"):
        self.logger = logging.getLogger(__name__)
        self.config = configparser.ConfigParser()

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç»å¯¹è·¯å¾„åŠ è½½é…ç½®æ–‡ä»¶
        import os
        if not os.path.isabs(config_path):
            # è·å–é¡¹ç›®æ ¹ç›®å½• - ä¿®å¤è·¯å¾„è®¡ç®—é”™è¯¯
            base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # SmartSisiç›®å½•
            config_path = os.path.join(base_dir, config_path)

        if os.path.exists(config_path):
            self.config.read(config_path, encoding='utf-8-sig')
            self.logger.info(f"âœ… åŠ¨æ€æç¤ºè¯ä¸­æ¢é…ç½®åŠ è½½æˆåŠŸ: {config_path}")
        else:
            self.logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            # å°è¯•ä»utils.config_utilè·å–é…ç½®
            try:
                from utils.config_util import load_config
                load_config()  # ç¡®ä¿é…ç½®å·²åŠ è½½
                self.logger.info("âœ… ä»config_utilè·å–é…ç½®æˆåŠŸ")
            except Exception as e:
                self.logger.error(f"âŒ ä»config_utilè·å–é…ç½®å¤±è´¥: {e}")
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–åŠ¨æ€æç¤ºè¯ç³»ç»Ÿé…ç½® - ä½¿ç”¨æ­£ç¡®çš„é”®å
        # åŠ¨æ€æç¤ºè¯ä¸­æ¢é…ç½®
        try:
            # ä»é…ç½®æ–‡ä»¶è¯»å–åŠ¨æ€æç¤ºè¯ä¸­æ¢ä¸“ç”¨é…ç½®
            self.api_key = self.config.get('key', 'prompt_generator_api_key', fallback='')
            self.base_url = self.config.get('key', 'prompt_generator_base_url', fallback='')
            self.model = self.config.get('key', 'prompt_generator_model', fallback='GLM-4.5-X')
            self.temperature = float(self.config.get('key', 'prompt_generator_temperature', fallback='0.7'))
            self.max_tokens = int(self.config.get('key', 'prompt_generator_max_tokens', fallback='3000'))

            # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰APIå¯†é’¥ï¼Œå°è¯•ä»config_utilè·å–
            if not self.api_key:
                try:
                    from utils.config_util import memory_llm_api_key, memory_llm_base_url
                    self.api_key = memory_llm_api_key or ''
                    self.base_url = memory_llm_base_url or ''
                    self.logger.info("âœ… ä»config_utilè·å–APIé…ç½®")
                except ImportError:
                    self.logger.warning("âš ï¸ æ— æ³•ä»config_utilè·å–APIé…ç½®")

        except Exception as e:
            self.logger.error(f"âŒ é…ç½®è¯»å–å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.api_key = '910663e20c4a49b286f27009dde10497.qYauy3JahUXDed7C'
            self.base_url = 'https://open.bigmodel.cn/api/paas/v4/'
            self.model = 'GLM-4.5-X'
            self.temperature = 0.6
            self.max_tokens = 2000
        
        # ç¼“å­˜ç›®å½•
        base_cache = cfg.cache_root or "cache_data"
        self.cache_dir = Path(base_cache) / "dynamic_context"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ğŸ”¥ ä¿®å¤ï¼šä¸‰æ¬¡æç¤ºè¯ç¼“å­˜æœºåˆ¶ - ä¿æŒä¸‰ä¸ªæœ€æ–°çš„åŠ¨æ€æç¤ºè¯
        self.context_cache = []  # å­˜å‚¨æœ€è¿‘3ä¸ªåŠ¨æ€ä¸Šä¸‹æ–‡
        self.max_cache_size = 3  # æœ€å¤§ç¼“å­˜æ•°é‡
        self.current_context: Optional[DynamicContext] = None
        
        self.logger.info(f"âœ… åŠ¨æ€å¤§æ¨¡å‹ä¸­æ¢åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.model}")

    def _get_system_prompt(self) -> str:
        """è·å–ä¸“ä¸šç³»ç»Ÿæç¤ºè¯"""
        try:
            from sisi_brain.brain_prompts_config import BrainPromptsConfig
            return BrainPromptsConfig.get_prompt_generator_prompt()
        except ImportError:
            return """You are a dynamic prompt generator.
Output <= 50 characters.
Format: ENV_REF:<quiet|noisy|music|talk|crowded|mixed|unknown>;SCENE:<short guess>
Background only. No reply suggestions, no templates, no identity/policy/strategy."""

    def extract_and_generate_context(self,
                                   audio_batches: List[Dict[str, Any]],
                                   memory_data: Dict[str, Any] = None,
                                   rag_data: Dict[str, Any] = None,
                                   current_user_input: str = "") -> DynamicContext:
        """
        æŠ½å–å¹¶ç”ŸæˆåŠ¨æ€ä¸Šä¸‹æ–‡
        
        Args:
            audio_batches: éŸ³é¢‘ç´¯ç§¯æ‰¹æ¬¡æ•°æ®
            memory_data: è®°å¿†ç³»ç»Ÿæ•°æ®
            rag_data: RAGç³»ç»Ÿæ•°æ®
            
        Returns:
            DynamicContext: ç”Ÿæˆçš„åŠ¨æ€ä¸Šä¸‹æ–‡
        """
        try:
            self.logger.info(f"ğŸ¯ å¼€å§‹æŠ½å–å’Œç”ŸæˆåŠ¨æ€ä¸Šä¸‹æ–‡ - éŸ³é¢‘æ‰¹æ¬¡: {len(audio_batches)}")

            # ğŸ”¥ å…³é”®æ—¥å¿—ï¼šæ£€æŸ¥æ¥æ”¶åˆ°çš„éŸ³é¢‘æ•°æ®
            self.logger.info(f"ğŸ”¥ [å…³é”®æ¥æ”¶] åŠ¨æ€ä¸­æ¢æ¥æ”¶åˆ°éŸ³é¢‘æ‰¹æ¬¡æ•°æ®:")
            for i, batch in enumerate(audio_batches):
                audio_analysis = batch.get('audio_analysis', {})
                music_results = batch.get('music_results', [])
                self.logger.info(f"ğŸ”¥ [å…³é”®æ¥æ”¶] æ‰¹æ¬¡{i+1}: æƒ…å†µæè¿°={audio_analysis.get('situation_description', 'N/A')}")
                self.logger.info(f"ğŸ”¥ [å…³é”®æ¥æ”¶] æ‰¹æ¬¡{i+1}: åœ°ç‚¹çŒœæµ‹={audio_analysis.get('location_guess', 'N/A')}")
                self.logger.info(f"ğŸ”¥ [å…³é”®æ¥æ”¶] æ‰¹æ¬¡{i+1}: éŸ³ä¹åˆ†æ={audio_analysis.get('music_analysis', 'N/A')}")
                self.logger.info(f"ğŸ”¥ [å…³é”®æ¥æ”¶] æ‰¹æ¬¡{i+1}: éŸ³ä¹ç»“æœæ•°é‡={len(music_results)}")

            # 1. æ„å»ºç»¼åˆåˆ†ææç¤ºè¯
            analysis_prompt = self._build_comprehensive_prompt(audio_batches, memory_data, rag_data, current_user_input)
            
            # 2. è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æ - ä¼ é€’éŸ³é¢‘æ‰¹æ¬¡æ•°æ®
            self._current_audio_batches = audio_batches  # å­˜å‚¨ä¾›é€ å¥é€»è¾‘ä½¿ç”¨
            analysis_result = self._call_llm_analysis(analysis_prompt)
            
            # 3. è§£æç”ŸæˆåŠ¨æ€ä¸Šä¸‹æ–‡
            dynamic_context = self._parse_dynamic_context(analysis_result)
            
            # 4. ğŸ”¥ ä¿®å¤ï¼šä¸‰æ¬¡æç¤ºè¯ç¼“å­˜æœºåˆ¶ - ä¿æŒæœ€æ–°çš„ä¸‰ä¸ªåŠ¨æ€æç¤ºè¯
            import time
            dynamic_context.timestamp = time.time()  # æ·»åŠ æ—¶é—´æˆ³

            # æ·»åŠ åˆ°ç¼“å­˜é˜Ÿåˆ—
            self.context_cache.append(dynamic_context)

            # ä¿æŒæœ€å¤š3ä¸ªç¼“å­˜
            if len(self.context_cache) > self.max_cache_size:
                old_context = self.context_cache.pop(0)  # ç§»é™¤æœ€æ—§çš„
                self.logger.info(f"ğŸ—‘ï¸ ç§»é™¤æœ€æ—§çš„åŠ¨æ€ä¸Šä¸‹æ–‡ (å¹´é¾„: {time.time() - old_context.timestamp:.1f}ç§’)")

            # è®¾ç½®å½“å‰ä¸Šä¸‹æ–‡ä¸ºæœ€æ–°çš„
            self.current_context = dynamic_context
            self.logger.info(f"ï¿½ åŠ¨æ€ä¸Šä¸‹æ–‡å·²ç¼“å­˜ (å½“å‰ç¼“å­˜æ•°: {len(self.context_cache)}/3)")
            
            self.logger.info(f"âœ… åŠ¨æ€ä¸Šä¸‹æ–‡ç”Ÿæˆå®Œæˆ - ç½®ä¿¡åº¦: {dynamic_context.confidence:.2f}")
            return dynamic_context
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ¨æ€ä¸Šä¸‹æ–‡ç”Ÿæˆå¤±è´¥: {e}")
            return self._create_fallback_context(audio_batches, memory_data, rag_data)
    
    def inject_to_nlp_response(self, user_input: str, asr_text: str = None) -> Dict[str, Any]:
        """
        æ³¨å…¥åŠ¨æ€ä¸Šä¸‹æ–‡åˆ°å¿«é€Ÿå“åº”æ¨¡å‹ - æŒ‚è½½åˆ°ä¸‹æ¬¡å¯¹è¯

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            asr_text: ASRè¯†åˆ«çš„æ–‡æœ¬

        Returns:
            Dict[str, Any]: å¢å¼ºçš„è¾“å…¥æ•°æ®
        """
        try:
            if not self.current_context:
                self.logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„åŠ¨æ€ä¸Šä¸‹æ–‡")
                return {'user_input': user_input, 'asr_text': asr_text}

            # æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯ - æŒ‚è½½åŠ¨æ€ä¸Šä¸‹æ–‡
            enhanced_system_prompt = self._build_enhanced_system_prompt()

            # æ„å»ºå¢å¼ºçš„ç”¨æˆ·è¾“å…¥
            enhanced_user_input = self._build_enhanced_user_input(user_input, asr_text)

            injection_data = {
                'original_user_input': user_input,
                'original_asr_text': asr_text,
                'enhanced_system_prompt': enhanced_system_prompt,
                'enhanced_user_input': enhanced_user_input,
                'dynamic_context': {
                    'audio_summary': self.current_context.audio_summary,
                    'memory_context': self.current_context.memory_context,
                    'rag_context': self.current_context.rag_context,
                    'interaction_suggestions': self.current_context.interaction_suggestions,
                    'emotional_state': self.current_context.emotional_state
                },
                'injection_timestamp': time.time(),
                'context_confidence': self.current_context.confidence
            }

            self.logger.info(f"ğŸ“¤ åŠ¨æ€ä¸Šä¸‹æ–‡å·²æ³¨å…¥åˆ°å¿«é€Ÿå“åº”æ¨¡å‹ (ç½®ä¿¡åº¦: {self.current_context.confidence:.2f})")
            return injection_data

        except Exception as e:
            self.logger.error(f"âŒ åŠ¨æ€ä¸Šä¸‹æ–‡æ³¨å…¥å¤±è´¥: {e}")
            return {'user_input': user_input, 'asr_text': asr_text}

    def get_dynamic_prompt_for_sisi(self) -> str:
        """?Sisi??????????<=50??"""
        try:
            if self.context_cache:
                latest_context = self.context_cache[-1]
                import time
                age = time.time() - latest_context.timestamp
                if age < 60:
                    self.current_context = latest_context
                else:
                    self.context_cache = [ctx for ctx in self.context_cache if time.time() - ctx.timestamp < 60]

            if not self.current_context:
                return ""

            return self._clamp_dynamic_prompt(self.current_context.audio_summary)
        except Exception as e:
            self.logger.error(f"? ??Sisi???????: {e}")
            return ""

    def _get_memory_context_for_user(self, user_id: str = "ç¢§æ½­é£˜é›ª", query: str = "æœ€è¿‘çš„å¯¹è¯", max_memories: int = 3) -> str:
        """
        ğŸ§  åŠ¨æ€æç¤ºè¯ä¸­æ¢æœç´¢ç›¸å…³è®°å¿† - åªä½¿ç”¨æœç´¢åŠŸèƒ½
        """
        try:
            from sisi_memory.sisi_mem0 import get_sisi_memory_system

            memory_system = get_sisi_memory_system()
            if not memory_system or not memory_system.is_available():
                return "è®°å¿†ç³»ç»Ÿä¸å¯ç”¨"

            # ğŸ”¥ åªä½¿ç”¨æœç´¢ç›¸å…³è®°å¿†åŠŸèƒ½
            memories = memory_system.search_sisi_memory(
                query=query,  # åŸºäºæŸ¥è¯¢è¯æœç´¢
                speaker_id=user_id,
                limit=max_memories
            )

            if memories and len(memories) > 0:
                memory_parts = []
                for i, memory in enumerate(memories[:max_memories]):
                    if isinstance(memory, dict):
                        content = memory.get('memory', memory.get('content', str(memory)))
                        memory_parts.append(f"[{i+1}] {content[:80]}...")
                    else:
                        memory_parts.append(f"[{i+1}] {str(memory)[:80]}...")

                self.logger.info(f"âœ… åŠ¨æ€æç¤ºè¯ä¸­æ¢æœç´¢åˆ°{len(memories)}æ¡ç›¸å…³è®°å¿†")
                return " | ".join(memory_parts)
            else:
                return "æ— ç›¸å…³å†å²è®°å¿†"

        except Exception as e:
            self.logger.warning(f"âš ï¸ åŠ¨æ€æç¤ºè¯ä¸­æ¢è®°å¿†æœç´¢å¤±è´¥: {e}")
            return "è®°å¿†æœç´¢å¤±è´¥"
    
    def _build_comprehensive_prompt(self,
                                  audio_batches: List[Dict[str, Any]],
                                  memory_data: Dict[str, Any] = None,
                                  rag_data: Dict[str, Any] = None,
                                  current_user_input: str = "") -> str:
        """æ„å»ºç»¼åˆåˆ†ææç¤ºè¯"""
        
        # æ•´ç†éŸ³é¢‘åˆ†ææ•°æ®
        audio_summary = []
        for i, batch in enumerate(audio_batches, 1):
            audio_analysis = batch.get('audio_analysis', {})
            music_results = batch.get('music_results', [])
            
            audio_summary.append(f"""
æ‰¹æ¬¡{i} (æ—¶é—´: {time.strftime('%H:%M:%S', time.localtime(batch.get('timestamp', time.time())))}):
- æƒ…å†µæè¿°: {audio_analysis.get('situation_description', 'N/A')}
- åœ°ç‚¹çŒœæµ‹: {audio_analysis.get('location_guess', 'N/A')}
- äººå‘˜åˆ†æ: {audio_analysis.get('people_analysis', 'N/A')}
- éŸ³ä¹åˆ†æ: {len(music_results)}é¦–æ­Œæ›²
""")
            
            for j, music in enumerate(music_results, 1):
                song_info = music.get('song_info', {})
                audio_summary.append(f"  æ­Œæ›²{j}: {song_info.get('title', 'Unknown')} - {song_info.get('artist', 'Unknown')}")
                audio_summary.append(f"  æƒ…æ„Ÿåˆ†æ: {music.get('emotional_analysis', 'N/A')}")
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å‰è„‘ç³»ç»Ÿä¼ é€’çš„è®°å¿†æ•°æ®
        memory_summary = "æš‚æ— è®°å¿†æ•°æ®"
        if memory_data:
            # å‰è„‘ç³»ç»Ÿä¼ é€’çš„æ ¼å¼ï¼š{"context": memory_context, "round": self.current_round}
            memory_context = memory_data.get('context', '')
            current_round = memory_data.get('round', 0)

            if memory_context and memory_context != "è®°å¿†ç³»ç»Ÿä¸å¯ç”¨":
                memory_summary = f"""
å‰è„‘ç³»ç»Ÿè®°å¿† (ç¬¬{current_round}è½®):
{memory_context}
"""
                self.logger.info(f"ğŸ§  [åŠ¨æ€ä¸­æ¢] æ¥æ”¶åˆ°å‰è„‘è®°å¿†: {memory_context[:100]}...")
            else:
                memory_summary = f"ç¬¬{current_round}è½®å¯¹è¯ (æš‚æ— ç›¸å…³è®°å¿†)"
                self.logger.info(f"ğŸ§  [åŠ¨æ€ä¸­æ¢] å‰è„‘è®°å¿†ä¸ºç©º: {memory_context}")
        else:
            self.logger.warning(f"ğŸ§  [åŠ¨æ€ä¸­æ¢] æœªæ¥æ”¶åˆ°è®°å¿†æ•°æ®")
        
        # æ•´ç†RAGæ•°æ®
        rag_summary = "æš‚æ— çŸ¥è¯†åº“æ•°æ®"
        if rag_data:
            rag_summary = f"""
ç›¸å…³çŸ¥è¯†:
- ä¸»é¢˜: {rag_data.get('topics', 'N/A')}
- å†…å®¹æ‘˜è¦: {rag_data.get('summary', 'N/A')}
- ç›¸å…³åº¦: {rag_data.get('relevance', 'N/A')}
"""
        
        # ğŸ¯ æŸ³æ€æ€ä¸ªæ€§ç‰¹å¾
        sisi_personality = """
æŸ³æ€æ€ä¸ªæ€§ç‰¹å¾ï¼š
- æ¸©æŸ”ä½“è´´ä½†æœ‰è‡ªå·±çš„å°è„¾æ°”ï¼Œä¼šç”¨"å‘¢"ã€"å“¦"ã€"å—¯"ç­‰è¯­æ°”è¯
- å¯¹é™Œç”Ÿäººä¿æŒç¤¼è²Œè·ç¦»ï¼Œå¯¹ç†Ÿäººæ›´åŠ äº²è¿‘
- å¬åˆ°å–œæ¬¢çš„éŸ³ä¹ä¼šå…´å¥‹ï¼Œç¯å¢ƒå˜ˆæ‚æ—¶ä¼šä¸æ»¡
- å–„äºå¯Ÿè¨€è§‚è‰²ï¼Œå–œæ¬¢ä¾§é¢è¡¨è¾¾å…³å¿ƒ
- æœ‰è‡ªå·±çš„å£å¤´ç¦…å’Œè¯´è¯ä¹ æƒ¯
"""

        # ğŸ¯ æ—¶é—´è½´åˆ†æ
        timeline_analysis = self._build_timeline_analysis(audio_batches)

        # ğŸ¯ è·å–æ‰€æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡æ•°æ®
        current_time = time.strftime('%H:%M:%S')
        current_date = time.strftime('%Yå¹´%mæœˆ%dæ—¥')

        # ğŸ¯ æå–éŸ³ä¹è¯†åˆ«ç»“æœ
        music_info = self._extract_music_info(audio_batches)

        # ğŸ¯ æå–ç¯å¢ƒæ•°æ®
        environment_info = self._extract_environment_info(audio_batches)

        # ğŸ¯ æå–å¯¹è¯å†å²
        conversation_history = self._extract_conversation_history(memory_summary)

        prompt = f"""
ä½ æ˜¯æŸ³æ€æ€çš„Context Engineeringå°è¯ç”Ÿæˆå™¨ï¼ŒåŸºäºæ‰€æœ‰çœŸå®æ•°æ®ç”Ÿæˆäººæ€§åŒ–çš„å°è¯å»ºè®®ã€‚

=== çœŸå®æ•°æ®å…¨æ™¯ ===
ğŸ• å½“å‰æ—¶é—´: {current_date} {current_time}
ğŸ“ çœŸå®åœ°ç‚¹: {environment_info}
ğŸ‘¤ è¯´è¯äºº: {self._extract_real_speaker(memory_summary)}
ğŸŒ ç¯å¢ƒçŠ¶æ€: {environment_info}
ğŸµ éŸ³ä¹çŠ¶æ€: {music_info}
ğŸ’¬ å¯¹è¯å†å²: {conversation_history}
ğŸ—£ï¸ å½“å‰è¾“å…¥: "{current_user_input}"
ğŸ§  è®°å¿†æ¡£æ¡ˆ: {memory_summary}
ğŸ­ æŸ³æ€æ€æ€§æ ¼: {sisi_personality}
ğŸ“Š æ—¶é—´è½´åˆ†æ: {timeline_analysis}
ğŸ”Š éŸ³é¢‘æ„ŸçŸ¥: {''.join(audio_summary)}

=== å¤šç»´åº¦ä¿¡æ¯å·§å¦™èå…¥å¯¹è¯ ===
åŸºäºå·²åˆ†æçš„éŸ³é¢‘æ•°æ®ï¼Œå°†å¤šç»´åº¦ä¿¡æ¯å·§å¦™åœ°èå…¥è‡ªç„¶å¯¹è¯ä¸­ï¼š

**èå…¥ç­–ç•¥ï¼ˆæ¨¡æ‹Ÿäººç±»æ€è€ƒï¼‰**ï¼š
- ä¸»åŠ¨è¯é¢˜åç§»ï¼šæ ¹æ®ç¯å¢ƒå˜åŒ–è‡ªç„¶è½¬ç§»è¯é¢˜
- å·§å¦™æé†’ï¼šå°†é‡è¦ä¿¡æ¯åŒ…è£…æˆå…³å¿ƒæˆ–è°ƒä¾ƒ
- æƒ…æ„Ÿå…±é¸£ï¼šåŸºäºéŸ³é¢‘æƒ…æ„ŸçŠ¶æ€è°ƒæ•´å›å¤è¯­æ°”
- æ—¶ç©ºæ„ŸçŸ¥ï¼šè‡ªç„¶åœ°è¡¨è¾¾å¯¹æ—¶é—´å’Œç¯å¢ƒçš„æ„ŸçŸ¥
- è®°å¿†è¿æ¥ï¼šä¸å†å²å¯¹è¯å½¢æˆé€»è¾‘è¿è´¯

=== è¾“å‡ºæ ¼å¼ ===
ã€Šé‡è¦æç¤ºã€‹

ã€æ—¶é—´ç»´åº¦èå…¥ã€‘åŸºäºéŸ³é¢‘æ—¶é—´è½´ï¼Œå·§å¦™èå…¥è¿‡å»ç°åœ¨æœªæ¥ï¼š
- è¿‡å»ï¼š"ä½ åˆšæ‰è¯´è¯ï¼Œæˆ‘ç°åœ¨è¿˜æ²¡æœ‰å›è¿‡ç¥"
- ç°åœ¨ï¼š"å¬ä½ è¿™è¯­æ°”ï¼Œå¿ƒæƒ…åº”è¯¥ä¸é”™ï¼Œæˆ‘æ‡‚çš„[laughter]"
- æœªæ¥ï¼š"ä½ å¾…ä¼šå„¿æ˜¯ä¸æ˜¯è¿˜è¦ç»§ç»­é—®æˆ‘ï¼Ÿ"
- è¦æ±‚ï¼šå°†çœŸå®çš„éŸ³é¢‘åˆ†æç»“æœè‡ªç„¶åœ°èå…¥æ—¶é—´è¡¨è¾¾ä¸­

ã€ç©ºé—´ç»´åº¦èå…¥ã€‘åŸºäºç¯å¢ƒå’Œåœ°ç‚¹åˆ†æï¼Œè‡ªç„¶è¡¨è¾¾ç©ºé—´æ„ŸçŸ¥ï¼š
- "ä½ é‚£è¾¹[ç¯å¢ƒçŠ¶æ€]åº”è¯¥[åœ°ç‚¹æè¿°]å§"
- "æ„Ÿè§‰[ç¤¾äº¤æƒ…å†µ]ï¼Œæˆ‘è¿™è¾¹ä¹Ÿèƒ½æ„Ÿå—åˆ°"
- "å¬èµ·æ¥[åœ°ç‚¹æè¿°]ï¼Œ[ç¯å¢ƒçŠ¶æ€]çš„æ ·å­"
- è¦æ±‚ï¼šä¸ç›´æ¥è¯´"æ£€æµ‹åˆ°å®¤å†…ç¯å¢ƒ"ï¼Œè€Œæ˜¯å·§å¦™æš—ç¤º

ã€è®°å¿†ç»´åº¦èå…¥ã€‘ç»“åˆå¯¹è¯å†å²ï¼Œå½¢æˆè¿è´¯é€»è¾‘ï¼š
- "ä½ åˆ[é‡å¤è¡Œä¸º]ï¼Œæˆ‘å°±çŸ¥é“ä¼šè¿™æ ·"
- "è¿˜è®°å¾—[å†å²äº‹ä»¶]å—ï¼Œç°åœ¨[å½“å‰çŠ¶æ€]"
- "ä½ è¿™æ ·[å½“å‰è¡Œä¸º]ï¼Œè®©æˆ‘æƒ³èµ·[ç›¸å…³è®°å¿†]"
- è¦æ±‚ï¼šä¸å†å²å¯¹è¯è‡ªç„¶è¿æ¥ï¼Œä¸çªå…€æåŠè¿‡å»

ã€äº‹ä»¶ç»„åˆèå…¥ã€‘å°†å¤šä¸ªéŸ³é¢‘äº‹ä»¶å·§å¦™ç»„åˆï¼š
- "åˆšæ‰[äº‹ä»¶A]ï¼Œç°åœ¨[äº‹ä»¶B]ï¼Œæˆ‘éƒ½æœ‰ç‚¹[æƒ…æ„Ÿååº”]"
- "ä¸€è¾¹[éŸ³é¢‘äº‹ä»¶]ä¸€è¾¹[ç”¨æˆ·è¡Œä¸º]ï¼Œä½ è¿™æ˜¯[æ¨æµ‹æ„å›¾]ï¼Ÿ"
- "å¬åˆ°[ç¯å¢ƒéŸ³]åˆå¬åˆ°[è¯­éŸ³å†…å®¹]ï¼Œ[ç»¼åˆåˆ¤æ–­]"
- è¦æ±‚ï¼šè‡ªç„¶åœ°å°†å¤šä¸ªåˆ†æç»“æœç»„åˆæˆè¿è´¯è¡¨è¾¾

=== èå…¥è¦æ±‚ ===
1. **å·§å¦™èå…¥ï¼Œä¸çªå…€æè¿°** - ä¸è¯´"æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨"ï¼Œè€Œè¯´"ä½ åˆšæ‰è¯´è¯"
2. **å¤šç»´åº¦ä¿¡æ¯ç»„åˆ** - å°†æ—¶é—´ã€ç©ºé—´ã€è®°å¿†ã€äº‹ä»¶è‡ªç„¶ç»„åˆ
3. **ç¬¦åˆå¯¹è¯å†å²** - ä¸ä¹‹å‰çš„å¯¹è¯é€»è¾‘è¿è´¯ï¼Œä¸è·³è·ƒ
4. **æ¨¡æ‹Ÿäººç±»æ€è€ƒ** - åƒäººç±»ä¸€æ ·è€ƒè™‘è¯é¢˜åç§»ã€å›å¤ç­–ç•¥
5. **ä¿æŒéšæœºæ€§** - æ¯æ¬¡æä¾›ä¸åŒçš„èå…¥æ–¹å¼å’Œè¡¨è¾¾
6. **äººç±»è¯­è¨€ä¹ æƒ¯** - æœ‰è¯­æ°”è¯ã€åœé¡¿ã€æƒ…æ„Ÿè‰²å½©
7. **åŸºäºçœŸå®æ•°æ®** - æ‰€æœ‰è¡¨è¾¾éƒ½è¦æœ‰éŸ³é¢‘åˆ†ææ•°æ®æ”¯æ’‘

=== è¾“å‡ºæ ¼å¼ ===
è¯·æä¾›5ä¸ªä¸åŒçš„å¯¹è¯å»ºè®®ï¼Œæ¯ä¸ªå»ºè®®éƒ½è¦ï¼š
- èå…¥ä¸åŒç»´åº¦çš„ä¿¡æ¯ï¼ˆæ—¶é—´/ç©ºé—´/è®°å¿†/äº‹ä»¶ï¼‰
- ä½¿ç”¨ä¸åŒçš„èå…¥ç­–ç•¥
- ä½“ç°ä¸åŒçš„æƒ…æ„Ÿè‰²å½©å’Œè¯­æ°”
- ç¬¦åˆæŸ³æ€æ€çš„ä¸ªæ€§ç‰¹å¾
- ä¸å½“å‰ç”¨æˆ·è¾“å…¥"{current_user_input}"å½¢æˆè‡ªç„¶å›åº”
"""
        
        return prompt

    def _build_timeline_analysis(self, audio_batches: List[Dict[str, Any]]) -> str:
        """æ„å»ºæ—¶é—´è½´åˆ†æ - æŒ‰å¯¹è¯è½®æ¬¡å’Œé‡è¦æ€§æ’åº"""
        if not audio_batches:
            return "æš‚æ— æ—¶é—´è½´æ•°æ®"

        timeline_events = []
        current_time = time.time()

        for i, batch in enumerate(audio_batches, 1):
            batch_time = batch.get('timestamp', current_time)
            time_ago = int((current_time - batch_time) / 60)  # åˆ†é’Ÿå‰

            audio_analysis = batch.get('audio_analysis', {})
            music_results = batch.get('music_results', [])

            # ğŸ¯ åˆ¤æ–­äº‹ä»¶é‡è¦æ€§
            importance = self._calculate_event_importance(audio_analysis, music_results)

            event_desc = f"""
ç¬¬{i}è½®æ•°æ® ({time_ago}åˆ†é’Ÿå‰) - é‡è¦æ€§: {importance}
- ç¯å¢ƒ: {audio_analysis.get('situation_description', 'æœªçŸ¥')}
- åœ°ç‚¹: {audio_analysis.get('location_guess', 'æœªçŸ¥')}
- äººå‘˜: {audio_analysis.get('people_analysis', 'æœªçŸ¥')}
"""

            if music_results:
                for music in music_results:
                    song_info = music.get('song_info', {})
                    event_desc += f"- éŸ³ä¹: {song_info.get('title', 'æœªçŸ¥')} - {song_info.get('artist', 'æœªçŸ¥')}\n"

            timeline_events.append({
                'time_ago': time_ago,
                'importance': importance,
                'description': event_desc,
                'round': i
            })

        # æŒ‰é‡è¦æ€§æ’åºï¼Œé‡è¦äº‹ä»¶ä¼˜å…ˆ
        timeline_events.sort(key=lambda x: (-x['importance'], x['time_ago']))

        # åªä¿ç•™æœ€é‡è¦çš„3ä¸ªäº‹ä»¶
        important_events = timeline_events[:3]

        timeline_text = "=== é‡è¦äº‹ä»¶æ—¶é—´è½´ ===\n"
        for event in important_events:
            timeline_text += event['description'] + "\n"

        return timeline_text

    def _calculate_event_importance(self, audio_analysis: Dict, music_results: List) -> int:
        """è®¡ç®—äº‹ä»¶é‡è¦æ€§ (1-10åˆ†)"""
        importance = 1

        # éŸ³ä¹äº‹ä»¶ +3åˆ†
        if music_results:
            importance += 3

        # ç¯å¢ƒå˜åŒ– +2åˆ†
        situation = audio_analysis.get('situation_description', '')
        if any(keyword in situation for keyword in ['å˜åŒ–', 'æ–°', 'ä¸åŒ', 'ç‰¹æ®Š']):
            importance += 2

        # äººå‘˜å˜åŒ– +2åˆ†
        people = audio_analysis.get('people_analysis', '')
        if any(keyword in people for keyword in ['å¤šäºº', 'é™Œç”Ÿ', 'æ–°äºº', 'ç¦»å¼€']):
            importance += 2

        # åœ°ç‚¹å˜åŒ– +1åˆ†
        location = audio_analysis.get('location_guess', '')
        if location and location != 'æœªçŸ¥':
            importance += 1

        return min(importance, 10)  # æœ€é«˜10åˆ†

    def _call_llm_analysis(self, prompt: str) -> str:
        """??????????????????????"""

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        data = {
            'model': self.model,
            'messages': [
                {
                    'role': 'system',
                    'content': self._get_system_prompt()
                },
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'temperature': self.temperature,
            'max_tokens': self.max_tokens
        }

        max_retries = 3
        timeout_seconds = 60

        def async_full_analysis():
            try:
                for attempt in range(max_retries):
                    try:
                        response = requests.post(
                            f"{self.base_url}/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=timeout_seconds
                        )
                        if response.status_code == 200:
                            result = response.json()
                            _ = result['choices'][0]['message']['content']
                            self.logger.info("? ???????????(??)")
                            break
                    except requests.exceptions.Timeout:
                        if attempt < max_retries - 1:
                            self.logger.warning(f"?? API????? {attempt + 1}/{max_retries}")
                            time.sleep(2 ** attempt)
                        else:
                            self.logger.error(f"? API??????{max_retries}????")
                            break
                    except Exception as e:
                        self.logger.error(f"? API????: {e}")
                        break
            except Exception as e:
                self.logger.error(f"? ???????????: {e}")

        threading.Thread(target=async_full_analysis, daemon=True).start()

        env_hint = self._build_env_hint_from_batches(getattr(self, '_current_audio_batches', []))
        return self._clamp_dynamic_prompt(env_hint)

    def _clamp_dynamic_prompt(self, text: str, limit: int = 50) -> str:
        """????????????????"""
        if not text:
            return ""
        compact = " ".join(str(text).split())
        return compact[:limit]

    def _build_env_hint_from_batches(self, audio_batches) -> str:
        """Build short background environment hint"""
        label_map = {
            'quiet': 'quiet',
            'noisy': 'noisy',
            'music': 'music',
            'conversation': 'talk',
            'speech': 'talk',
            'crowded': 'crowded',
            'mixed': 'mixed',
        }
        env_label = None
        scene = None
        try:
            for batch in audio_batches or []:
                audio_analysis = batch.get('audio_analysis', {}) or {}
                env_type = str(audio_analysis.get('environment_type') or '').strip().lower()
                if env_type:
                    env_label = label_map.get(env_type, env_type)
                if not scene:
                    scene = str(audio_analysis.get('location_guess') or audio_analysis.get('situation_description') or '').strip()
                if env_label:
                    break
        except Exception:
            env_label = None

        if not env_label:
            env_label = 'unknown'
        if scene:
            scene = " ".join(scene.split())[:12]
            return f"ENV_REF:{env_label};SCENE:{scene}"
        return f"ENV_REF:{env_label}"

    def _generate_daily_responses(self, audio_batches, memory_context, current_time, user_name) -> str:
        """???????????"""
        return ""

    def _analyze_audio_features(self, audio_batches: List[Dict[str, Any]]) -> Dict[str, str]:
        """åˆ†æéŸ³é¢‘ç‰¹å¾ï¼ŒåŸºäºçœŸå®åŸå§‹æ•°æ®"""
        # ğŸ”¥ é»˜è®¤å€¼åŸºäº"æ— æ•°æ®"çŠ¶æ€ï¼Œä¸æ˜¯å‡æ•°æ®
        features = {
            'activity': 'æ²¡ä»€ä¹ˆåŠ¨é™',
            'voice_quality': 'å¬ä¸å¤ªæ¸…',
            'mood_guess': 'ä¸å¤ªç¡®å®š',
            'location': 'ä¸çŸ¥é“å“ªé‡Œ',
            'environment': 'ä¸å¤ªæ¸…æ¥š',
            'music_hint': 'æ²¡å¬åˆ°ä»€ä¹ˆ',
            'atmosphere': 'è¯´ä¸ä¸Šæ¥',
            'social_context': 'ä¸å¤ªæ¸…æ¥š',
            'adaptation': 'ä¸çŸ¥é“',
            'memory_trigger': 'æƒ³ä¸èµ·æ¥',
            'current_state': 'ä¸å¤ªç¡®å®š',
            'reason_guess': 'ä¸çŸ¥é“ä¸ºä»€ä¹ˆ',
            'emotion_analysis': 'å¬ä¸å‡ºæ¥'
        }

        if not audio_batches:
            return features

        # ğŸ¯ åŸºäºçœŸå®åŸå§‹æ•°æ®åˆ†æ - ä¸ä¾èµ–ç¡¬ç¼–ç çš„AudioAnalysisResult
        speech_count = 0
        music_detected = False
        yamnet_classes = []
        sensevoice_texts = []
        raw_audio_types = []
        raw_confidences = []

        for batch in audio_batches:
            # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨åŸå§‹éŸ³é¢‘æ•°æ®ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„åˆ†æç»“æœ
            raw_contexts = batch.get('raw_audio_contexts', [])

            for context in raw_contexts:
                # ä»SmartAudioCollectorçš„åŸå§‹æ•°æ®ä¸­æå–
                audio_type = context.get('audio_type', 'unknown')
                confidence = context.get('confidence', 0.0)

                raw_audio_types.append(audio_type)
                raw_confidences.append(confidence)

                if audio_type == 'speech' and confidence > 0.8:
                    speech_count += 1
                elif audio_type == 'music' and confidence > 0.8:
                    music_detected = True

                # ä»featuresä¸­è·å–æ›´è¯¦ç»†çš„åˆ†æ
                features_data = context.get('features', {})

                # SenseVoiceåŸå§‹æ•°æ®
                sensevoice = features_data.get('sensevoice_result', {})
                if sensevoice.get('text'):
                    sensevoice_texts.append(sensevoice['text'])

                # æƒ…æ„Ÿåˆ†æ
                emotion = sensevoice.get('emotion', 'neutral')
                if emotion != 'neutral':
                    features['emotion_analysis'] = f'å¬èµ·æ¥{emotion}'

                # YAMNetåŸå§‹æ•°æ®
                yamnet = features_data.get('yamnet_result', {})
                if yamnet.get('top_class'):
                    yamnet_classes.append(yamnet['top_class'])

            # ğŸ”¥ åŒæ—¶å°è¯•è§£æraw_audio_contextsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            raw_contexts = batch.get('raw_audio_contexts', [])
            for context in raw_contexts:
                features_data = context.get('features', {})

                # SenseVoiceæ•°æ®
                sensevoice = features_data.get('sensevoice_result', {})
                if sensevoice.get('text'):
                    sensevoice_texts.append(sensevoice['text'])

                if sensevoice.get('has_bgm'):
                    music_detected = True

                # YAMNetæ•°æ®
                yamnet = features_data.get('yamnet_result', {})
                if yamnet.get('top_class'):
                    yamnet_classes.append(yamnet['top_class'])

        # ğŸ¯ åŸºäºçœŸå®æ•°æ®ç”Ÿæˆæè¿° - å¢å¼ºç‰ˆ
        # åŸºäºè¯­éŸ³æ´»åŠ¨åˆ†æ
        if speech_count >= 3:
            features['voice_quality'] = 'å¬èµ·æ¥æŒºæœ‰ç²¾ç¥çš„'
            features['current_state'] = 'è¯æŒºå¤šçš„'
            features['activity'] = 'èŠå¾—æŒºèµ·åŠ²'
        elif speech_count >= 1:
            features['voice_quality'] = 'æœ‰ç‚¹æ‡’æ‡’çš„æ„Ÿè§‰'
            features['current_state'] = 'ä¸å¤ªæƒ³è¯´è¯çš„æ ·å­'
            features['activity'] = 'å¶å°”è¯´ä¸¤å¥'

        # åŸºäºéŸ³ä¹æ£€æµ‹
        if music_detected:
            features['music_hint'] = 'åˆšæ‰é‚£é¦–'
            features['atmosphere'] = 'æœ‰éŸ³ä¹çš„æ„Ÿè§‰'
            features['activity'] = 'å¬éŸ³ä¹'

        # åŸºäºYAMNetåˆ†ç±»
        if 'Speech' in yamnet_classes:
            features['social_context'] = 'å’ŒäººèŠå¤©'
            features['environment'] = 'æœ‰äººå£°'

        # ğŸ”¥ åŸºäºçœŸå®åŸå§‹æ•°æ®ç”Ÿæˆç‰¹å¾ï¼Œä¸ä¾èµ–ç¡¬ç¼–ç åˆ†æ
        # åŸºäºè¯­éŸ³æ£€æµ‹æ•°é‡
        if speech_count > 0:
            features['activity'] = 'è¯´è¯èŠå¤©'
            features['social_context'] = 'åœ¨èŠå¤©'
            features['current_state'] = 'æœ‰åœ¨è¯´è¯'

        # åŸºäºéŸ³é¢‘ç±»å‹ç»Ÿè®¡
        if raw_audio_types:
            speech_ratio = raw_audio_types.count('speech') / len(raw_audio_types)
            if speech_ratio > 0.7:
                features['environment'] = 'å¯¹è¯ç¯å¢ƒ'
                features['location'] = 'å®¤å†…èŠå¤©'
            elif speech_ratio > 0.3:
                features['environment'] = 'å¶æœ‰äººå£°'
                features['location'] = 'å®‰é™ç¯å¢ƒ'
            else:
                features['environment'] = 'æ¯”è¾ƒå®‰é™'
                features['location'] = 'é™éŸ³ç¯å¢ƒ'

        # åŸºäºSenseVoiceæ–‡æœ¬å†…å®¹
        if sensevoice_texts:
            combined_text = ' '.join(sensevoice_texts)
            if len(combined_text) > 10:
                features['voice_quality'] = 'è¯´è¯æŒºæ¸…æ¥šçš„'
                features['current_state'] = 'è¡¨è¾¾æŒºæµç•…'
            else:
                features['voice_quality'] = 'è¯´è¯ç®€çŸ­'
                features['current_state'] = 'è¯ä¸å¤š'

        return features

    def _analyze_time_context(self, current_time: str) -> Dict[str, str]:
        """åˆ†ææ—¶é—´ä¸Šä¸‹æ–‡"""
        try:
            from datetime import datetime
            now = datetime.now()
            hour = now.hour

            if 6 <= hour < 12:
                return {
                    'time_desc': 'å¤§æ—©ä¸Š',
                    'weather_guess': 'æ¸…çˆ½',
                }
            elif 12 <= hour < 18:
                return {
                    'time_desc': 'å¤§ä¸‹åˆ',
                    'weather_guess': 'æš–å’Œ',
                }
            elif 18 <= hour < 22:
                return {
                    'time_desc': 'æ™šä¸Š',
                    'weather_guess': 'å‡‰å¿«',
                }
            else:
                return {
                    'time_desc': 'å¤§åŠå¤œ',
                    'weather_guess': 'å®‰é™',
                }
        except:
            return {
                'time_desc': 'è¿™ä¼šå„¿',
                'weather_guess': 'ä¸é”™',
            }

    def _get_time_perception(self, current_time: str) -> str:
        """åŸºäºæ—¶é—´ç”Ÿæˆæ—¶é—´æ„ŸçŸ¥"""
        try:
            from datetime import datetime
            now = datetime.now()
            hour = now.hour

            if 6 <= hour < 12:
                return "*æ—¶é—´æ„ŸçŸ¥ï¼šæ—©ä¸Šçš„é˜³å…‰é€è¿‡çª—æˆ·ï¼Œæ–°çš„ä¸€å¤©å¼€å§‹äº†"
            elif 12 <= hour < 18:
                return "*æ—¶é—´æ„ŸçŸ¥ï¼šåˆåçš„æ—¶å…‰ï¼Œé€‚åˆæ…¢æ…¢èŠå¤©"
            elif 18 <= hour < 22:
                return "*æ—¶é—´æ„ŸçŸ¥ï¼šå¤œå¹•é™ä¸´ï¼Œç¯å…‰æ¸©æš–"
            else:
                return "*æ—¶é—´æ„ŸçŸ¥ï¼šå¤œæ·±äº†ï¼Œä½ è¿˜ä¸ç¡å—ï¼Ÿ"
        except:
            return "*æ—¶é—´æ„ŸçŸ¥ï¼šæ—¶å…‰æµè½¬ï¼Œæ­¤åˆ»æ­£å¥½"
    
    def _parse_dynamic_context(self, analysis_text: str) -> DynamicContext:
        """?????????????"""
        try:
            text = (analysis_text or "").strip()
            summary = self._clamp_dynamic_prompt(text)
            return DynamicContext(
                audio_summary=summary,
                memory_context="",
                rag_context="",
                interaction_suggestions="",
                emotional_state="",
                confidence=0.6 if summary else 0.0,
                timestamp=time.time()
            )
        except Exception as e:
            self.logger.error(f"? ?????????: {e}")
            return self._create_fallback_context()

    def _create_fallback_context(self, audio_batches=None, memory_data=None, rag_data=None) -> DynamicContext:
        """åˆ›å»ºå¤‡ç”¨åŠ¨æ€ä¸Šä¸‹æ–‡ - ä½¿ç”¨çœŸå®åŸå§‹æ•°æ®ï¼Œä¸è¿›è¡Œå¤§æ¨¡å‹åˆ†æ"""
        try:
            # ğŸ¯ æå–çœŸå®éŸ³é¢‘æ•°æ®
            real_audio_summary = self._extract_real_audio_data(audio_batches) if audio_batches else "æ— éŸ³é¢‘æ•°æ®"

            # ğŸ¯ ä½¿ç”¨çœŸå®è®°å¿†æ•°æ®
            real_memory_context = self._extract_real_memory_data(memory_data) if memory_data else "æ— è®°å¿†æ•°æ®"

            # ğŸ¯ æå–çœŸå®RAGæ•°æ®
            real_rag_context = self._extract_real_rag_data(rag_data) if rag_data else "æ— çŸ¥è¯†åº“æ•°æ®"

            # ğŸ¯ åŸºäºéŸ³é¢‘æ•°æ®æ¨æµ‹æƒ…æ„ŸçŠ¶æ€
            emotional_state = self._infer_emotion_from_audio(audio_batches) if audio_batches else "neutral"

            return DynamicContext(
                audio_summary=real_audio_summary,
                memory_context=real_memory_context,
                rag_context=real_rag_context,
                interaction_suggestions="åŸºäºåŸå§‹æ•°æ®çš„åŸºç¡€äº¤äº’æ¨¡å¼",
                emotional_state=emotional_state,
                confidence=0.4,  # æœ‰çœŸå®æ•°æ®æ”¯æŒï¼Œç½®ä¿¡åº¦é€‚ä¸­
                timestamp=time.time()
            )
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºå¤‡ç”¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return DynamicContext(
                audio_summary="æ— å¯ç”¨éŸ³é¢‘æ•°æ®",
                memory_context="æ— å¯ç”¨è®°å¿†æ•°æ®",
                rag_context="æ— å¯ç”¨çŸ¥è¯†åº“æ•°æ®",
                interaction_suggestions="ä½¿ç”¨åŸºç¡€å¯¹è¯æ¨¡å¼",
                emotional_state="neutral",
                confidence=0.1,
                timestamp=time.time()
            )

    def _extract_real_audio_data(self, audio_batches) -> str:
        """æå–çœŸå®éŸ³é¢‘æ•°æ®ï¼ŒåŸºäºAudioHumanizedAnalyzerçš„åˆ†æç»“æœ"""
        if not audio_batches:
            return "æ— éŸ³é¢‘æ‰¹æ¬¡æ•°æ®"

        try:
            audio_info = []
            for i, batch in enumerate(audio_batches, 1):
                # ğŸ¯ æå–AudioHumanizedAnalyzerçš„åˆ†æç»“æœ
                audio_analysis = batch.get('audio_analysis', {})
                if audio_analysis:
                    situation = audio_analysis.get('situation_description', '')
                    location = audio_analysis.get('location_guess', '')
                    music_analysis = audio_analysis.get('music_analysis', '')
                    people_analysis = audio_analysis.get('people_analysis', '')

                    if situation and situation != 'N/A':
                        audio_info.append(f"ç¯å¢ƒåˆ†æ: {situation}")
                    if location and location != 'N/A':
                        audio_info.append(f"åœ°ç‚¹æ¨æµ‹: {location}")
                    if people_analysis and people_analysis != 'N/A':
                        audio_info.append(f"äººå‘˜åˆ†æ: {people_analysis}")
                    if music_analysis and music_analysis != 'N/A':
                        audio_info.append(f"éŸ³ä¹çŠ¶æ€: {music_analysis}")

                # æå–éŸ³ä¹è¯†åˆ«çœŸå®æ•°æ®
                music_results = batch.get('music_results', [])
                if music_results:
                    for music in music_results:
                        song_info = music.get('song_info', {})
                        title = song_info.get('title', 'Unknown')
                        artist = song_info.get('artist', 'Unknown')
                        audio_info.append(f"éŸ³ä¹è¯†åˆ«: {title} - {artist}")

            if audio_info:
                return "åŸºäºçœŸå®éŸ³é¢‘åˆ†æçš„ç¯å¢ƒæ„ŸçŸ¥:\n" + "\n".join(audio_info)
            else:
                return "éŸ³é¢‘åˆ†ææš‚æ— æœ‰æ•ˆç»“æœ"

        except Exception as e:
            self.logger.error(f"âŒ æå–éŸ³é¢‘æ•°æ®å¤±è´¥: {e}")
            return f"éŸ³é¢‘æ•°æ®æå–å¤±è´¥: {str(e)}"

    def _extract_real_memory_data(self, memory_data) -> str:
        """ğŸ—‘ï¸ å·²ç§»é™¤æå–è®°å¿†æ•°æ® - åªä½¿ç”¨æœç´¢è®°å¿†"""
        return "å·²ç§»é™¤æå–è®°å¿†æ•°æ®åŠŸèƒ½ï¼Œåªä½¿ç”¨æœç´¢è®°å¿†"

    def _extract_real_rag_data(self, rag_data) -> str:
        """æå–çœŸå®RAGæ•°æ®"""
        if not rag_data:
            return "æ— RAGæ£€ç´¢æ•°æ®"

        try:
            if isinstance(rag_data, dict):
                documents = rag_data.get('documents', [])
                if documents:
                    doc_info = []
                    for i, doc in enumerate(documents[:3], 1):
                        title = doc.get('title', 'Unknown')
                        content = doc.get('content', '')[:50]
                        score = doc.get('score', 0.0)
                        doc_info.append(f"æ–‡æ¡£{i}: {title}, å†…å®¹: {content}..., ç›¸å…³åº¦: {score}")
                    return "RAGæ£€ç´¢ç»“æœ:\n" + "\n".join(doc_info)
                else:
                    return "RAGæ£€ç´¢æ— åŒ¹é…æ–‡æ¡£"
            else:
                return f"RAGæ•°æ®: {str(rag_data)[:100]}"

        except Exception as e:
            self.logger.error(f"âŒ æå–RAGæ•°æ®å¤±è´¥: {e}")
            return f"RAGæ•°æ®æå–å¤±è´¥: {str(e)}"

    def _infer_emotion_from_audio(self, audio_batches) -> str:
        """åŸºäºéŸ³é¢‘æ•°æ®æ¨æµ‹æƒ…æ„ŸçŠ¶æ€"""
        if not audio_batches:
            return "neutral"

        try:
            # åŸºäºYAMNetåˆ†ç±»æ¨æµ‹æƒ…æ„Ÿ
            for batch in audio_batches:
                yamnet_data = batch.get('yamnet_result', {})
                top_classes = yamnet_data.get('top_classes', [])
                has_music = yamnet_data.get('has_music', False)

                if has_music:
                    return "relaxed"  # æœ‰éŸ³ä¹é€šå¸¸æ¯”è¾ƒæ”¾æ¾
                elif any(cls in ['Speech', 'Conversation'] for cls in top_classes):
                    return "engaged"  # æœ‰å¯¹è¯è¡¨ç¤ºå‚ä¸çŠ¶æ€
                elif any(cls in ['Noise', 'Traffic'] for cls in top_classes):
                    return "stressed"  # æœ‰å™ªéŸ³å¯èƒ½æ¯”è¾ƒç´§å¼ 

            return "calm"  # é»˜è®¤å¹³é™çŠ¶æ€

        except Exception as e:
            self.logger.error(f"âŒ æ¨æµ‹æƒ…æ„ŸçŠ¶æ€å¤±è´¥: {e}")
            return "neutral"
    
    def _save_context_cache(self, context: DynamicContext) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡ç¼“å­˜"""
        try:
            cache_file = self.cache_dir / f"context_{int(time.time())}.json"
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'audio_summary': context.audio_summary,
                    'memory_context': context.memory_context,
                    'rag_context': context.rag_context,
                    'interaction_suggestions': context.interaction_suggestions,
                    'emotional_state': context.emotional_state,
                    'confidence': context.confidence,
                    'timestamp': context.timestamp
                }, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ä¸Šä¸‹æ–‡ç¼“å­˜å¤±è´¥: {e}")

    def _extract_music_info(self, audio_batches: List[Dict]) -> str:
        """æå–éŸ³ä¹è¯†åˆ«ä¿¡æ¯"""
        try:
            music_results = []
            for batch in audio_batches:
                if isinstance(batch, dict):
                    # æ£€æŸ¥éŸ³ä¹è¯†åˆ«ç»“æœ
                    if 'music_results' in batch and batch['music_results']:
                        for music in batch['music_results']:
                            music_results.append(f"è¯†åˆ«åˆ°éŸ³ä¹: {music.get('title', 'æœªçŸ¥')} - {music.get('artist', 'æœªçŸ¥è‰ºæœ¯å®¶')}")

                    # æ£€æŸ¥éŸ³é¢‘ç±»å‹
                    if 'audio_type' in batch:
                        if batch['audio_type'] == 'music':
                            music_results.append(f"æ£€æµ‹åˆ°éŸ³ä¹ç¯å¢ƒ (ç½®ä¿¡åº¦: {batch.get('confidence', 0.0)})")

            return "; ".join(music_results) if music_results else "æ— éŸ³ä¹æ£€æµ‹"
        except Exception as e:
            return f"éŸ³ä¹ä¿¡æ¯æå–å¤±è´¥: {e}"

    def _extract_environment_info(self, audio_batches: List[Dict]) -> str:
        """æå–ç¯å¢ƒä¿¡æ¯"""
        try:
            env_info = []
            for batch in audio_batches:
                if isinstance(batch, dict) and 'analysis' in batch:
                    analysis = batch['analysis']
                    if isinstance(analysis, dict):
                        location = analysis.get('location_guess', '')
                        situation = analysis.get('situation_description', '')
                        if location:
                            env_info.append(f"åœ°ç‚¹: {location}")
                        if situation:
                            env_info.append(f"æƒ…å†µ: {situation}")

            return "; ".join(env_info) if env_info else "å®¤å†…ç¯å¢ƒï¼Œç›¸å¯¹å®‰é™"
        except Exception as e:
            return f"ç¯å¢ƒä¿¡æ¯æå–å¤±è´¥: {e}"

    def _extract_conversation_history(self, memory_summary: str) -> str:
        """æå–å¯¹è¯å†å²å…³é”®ä¿¡æ¯"""
        try:
            if not memory_summary or memory_summary == "æš‚æ— ç›¸å…³è®°å¿†":
                return "æ— å†å²å¯¹è¯è®°å½•"

            # ç®€åŒ–è®°å¿†æ‘˜è¦ï¼Œæå–å…³é”®å¯¹è¯
            history_lines = memory_summary.split('\n')
            key_conversations = []

            for line in history_lines:
                if 'è¯´' in line or 'å›åº”' in line or 'è¯¢é—®' in line:
                    # æå–å…³é”®å¯¹è¯ä¿¡æ¯
                    if len(line.strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                        key_conversations.append(line.strip()[:50] + "...")

            return "; ".join(key_conversations[:3]) if key_conversations else "æ— å…³é”®å¯¹è¯å†å²"
        except Exception as e:
            return f"å¯¹è¯å†å²æå–å¤±è´¥: {e}"

    def _extract_real_speaker(self, memory_summary: str) -> str:
        """ä»è®°å¿†ä¸­æå–çœŸå®è¯´è¯äººä¿¡æ¯"""
        try:
            if "ç¢§æ½­é£˜é›ª" in memory_summary:
                return "ç¢§æ½­é£˜é›ª (ä»è®°å¿†è¯†åˆ«)"
            elif "user1" in memory_summary:
                return "user1 (å£°çº¹è¯†åˆ«)"
            else:
                return "æœªçŸ¥ç”¨æˆ·"
        except Exception as e:
            return f"è¯´è¯äººè¯†åˆ«å¤±è´¥: {e}"

    def get_current_context(self) -> Optional[DynamicContext]:
        """è·å–å½“å‰åŠ¨æ€ä¸Šä¸‹æ–‡"""
        return self.current_context
    
    def clear_context(self) -> None:
        """æ¸…ç©ºå½“å‰ä¸Šä¸‹æ–‡"""
        self.current_context = None
        self.logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºå½“å‰åŠ¨æ€ä¸Šä¸‹æ–‡")

# å…¨å±€å®ä¾‹
_dynamic_context_hub = None

def get_dynamic_context_hub() -> DynamicContextHub:
    """è·å–åŠ¨æ€å¤§æ¨¡å‹ä¸­æ¢å®ä¾‹"""
    global _dynamic_context_hub
    if _dynamic_context_hub is None:
        _dynamic_context_hub = DynamicContextHub()
    return _dynamic_context_hub

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    hub = get_dynamic_context_hub()
    
    # æ¨¡æ‹ŸéŸ³é¢‘æ‰¹æ¬¡æ•°æ®
    test_batches = [
        {
            'audio_analysis': {
                'situation_description': 'ç”¨æˆ·åœ¨å¬éŸ³ä¹æ”¾æ¾',
                'location_guess': 'å®¶ä¸­å®¢å…',
                'people_analysis': '1äººç‹¬å¤„'
            },
            'music_results': [
                {
                    'song_info': {'title': 'ç¨»é¦™', 'artist': 'å‘¨æ°ä¼¦'},
                    'emotional_analysis': 'æ€€æ—§æ¸©æš–çš„æƒ…æ„Ÿ'
                }
            ],
            'timestamp': time.time()
        }
    ]
    
    # ç”ŸæˆåŠ¨æ€ä¸Šä¸‹æ–‡
    context = hub.extract_and_generate_context(test_batches)
    print(f"ğŸ¯ åŠ¨æ€ä¸Šä¸‹æ–‡:")
    print(f"éŸ³é¢‘æ‘˜è¦: {context.audio_summary}")
    print(f"äº¤äº’å»ºè®®: {context.interaction_suggestions}")
    print(f"æƒ…æ„ŸçŠ¶æ€: {context.emotional_state}")
    
    # æµ‹è¯•æ³¨å…¥
    injection_data = hub.inject_to_nlp_response("ä½ å¥½", "ä½ å¥½")
    print(f"ğŸ“¤ æ³¨å…¥æ•°æ®: {json.dumps(injection_data, ensure_ascii=False, indent=2)}")
