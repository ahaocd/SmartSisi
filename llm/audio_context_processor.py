"""
ğŸ¯ éŸ³é¢‘ä¸Šä¸‹æ–‡å¤„ç†å™¨ - 2025å¹´7æœˆæœ€æ–°æ¶æ„
åŸºäº2025å¹´å‰æ²¿æŠ€æœ¯ï¼šModular AI + Event-Driven + Microservices

æŠ€æœ¯å‚è€ƒï¼š
- Google Gemini 2.0 (2024.12) - å¤šæ¨¡æ€åŸç”Ÿç†è§£
- OpenBMB MiniCPM-o 2.6 (2025) - GPT-4oçº§åˆ«å¤šæ¨¡æ€
- Microsoft Build 2025 - Agentic AIæ¶æ„
- Nature 2025.07 - å¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶

æ¶æ„ç‰¹ç‚¹ï¼š
- ğŸ”§ æ¨¡å—åŒ–è®¾è®¡ï¼šç‹¬ç«‹çš„éŸ³é¢‘å¤„ç†å¾®æœåŠ¡
- ğŸ“¡ äº‹ä»¶é©±åŠ¨ï¼šå¼‚æ­¥éŸ³é¢‘äº‹ä»¶æµå¤„ç†
- ğŸ”„ å¯æ‰©å±•æ€§ï¼šæ”¯æŒéŸ³é¢‘â†’è§†é¢‘â†’ä¼ æ„Ÿå™¨æ‰©å±•
- ğŸ§  æ™ºèƒ½ç¼“å­˜ï¼šåŸºäºç›¸ä¼¼åº¦çš„å£°çº¹èšç±»
"""

import os
import json
import time
import logging
import hashlib
import threading
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np
from collections import defaultdict, deque
from utils import config_util as cfg

# éŸ³é¢‘å¤„ç†ç›¸å…³å¯¼å…¥
try:
    import librosa
    import soundfile as sf
    print("âœ… éŸ³é¢‘å¤„ç†åº“åŠ è½½æˆåŠŸ")

    # ğŸ¯ éŸ³é¢‘åˆ†æä¸“ç”¨ï¼Œä¸åšå£°çº¹è¯†åˆ«
    print("âœ… éŸ³é¢‘å¤„ç†åº“åŠ è½½æˆåŠŸ - ä»…ç”¨äºéŸ³é¢‘åˆ†æ")

except ImportError as e:
    print(f"âš ï¸ éŸ³é¢‘å¤„ç†åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install librosa soundfile")
    VoiceEncoder = None
    preprocess_wav = None

@dataclass
class AudioEvent:
    """éŸ³é¢‘äº‹ä»¶æ•°æ®ç»“æ„"""
    event_type: str  # 'music', 'noise', 'speech', 'silence'
    confidence: float
    timestamp: float
    duration: float
    metadata: Dict = None

# ç§»é™¤é‡å¤çš„SpeakerProfileå®šä¹‰ï¼Œä½¿ç”¨core.speaker_recognitionä¸­çš„ç»Ÿä¸€å®šä¹‰

class AudioContextProcessor:
    """ğŸ¯ éŸ³é¢‘ä¸Šä¸‹æ–‡å¤„ç†å™¨ - æ ¸å¿ƒç±»"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        # ğŸ”¥ ä¿®å¤ï¼šåˆå§‹åŒ–logger
        self.logger = logging.getLogger(__name__)
        if not cache_dir:
            try:
                cfg.load_config()
            except Exception:
                pass
            cache_root = cfg.cache_root or "cache_data"
            cache_dir = os.path.join(cache_root, "speaker_profiles")
        self.cache_dir = cache_dir
        os.makedirs(self.cache_dir, exist_ok=True)

        # ğŸ”¥ æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.voice_encoder = None
        self._init_voice_encoder()
        
        # ğŸ“Š éŸ³é¢‘ç»Ÿè®¡ç¼“å­˜
        self.audio_stats = {
            "music_events": deque(maxlen=100),      # éŸ³ä¹äº‹ä»¶å†å²
            "noise_events": deque(maxlen=100),      # å™ªéŸ³äº‹ä»¶å†å²  
            "speech_events": deque(maxlen=100),     # äººå£°äº‹ä»¶å†å²
            "total_music_count": 0,
            "total_noise_count": 0,
            "total_speech_count": 0
        }
        
        # ğŸ‘¥ å£°çº¹è¯†åˆ«å·²ç§»é™¤ - ç»Ÿä¸€ä½¿ç”¨core.speaker_recognition
        
        # ğŸ§  ä¸Šä¸‹æ–‡åˆ†æé…ç½®
        self.context_window = 30.0  # 30ç§’ä¸Šä¸‹æ–‡çª—å£
        self.analysis_cache = {}
        
        # ğŸ”„ åŠ è½½æŒä¹…åŒ–æ•°æ®
        self._load_persistent_data()
        
        print("ğŸ¯ AudioContextProcessor åˆå§‹åŒ–å®Œæˆ")
    
    def _init_voice_encoder(self):
        """å£°çº¹è¯†åˆ«å·²ç§»é™¤ - ç»Ÿä¸€ä½¿ç”¨core.speaker_recognition"""
        print("âš ï¸ å£°çº¹è¯†åˆ«åŠŸèƒ½å·²ç§»é™¤ï¼Œä½¿ç”¨ç»Ÿä¸€çš„SpeakerManager")
    
    def _load_persistent_data(self):
        """åŠ è½½æŒä¹…åŒ–çš„éŸ³é¢‘ç»Ÿè®¡å’Œå£°çº¹æ•°æ®"""
        try:
            # åŠ è½½éŸ³é¢‘ç»Ÿè®¡
            stats_file = os.path.join(self.cache_dir, "audio_stats.json")
            if os.path.exists(stats_file):
                with open(stats_file, 'r', encoding='utf-8') as f:
                    saved_stats = json.load(f)
                    self.audio_stats.update(saved_stats)
                    
            # åŠ è½½å£°çº¹ç¼“å­˜ - ä¿®å¤è·¯å¾„é—®é¢˜
            speakers_file = os.path.join(self.cache_dir, "speakers.json")
            speaker_profiles_file = os.path.join(self.cache_dir, "speaker_profiles.json")

            # ä¼˜å…ˆåŠ è½½æ–°æ ¼å¼çš„å£°çº¹æ¡£æ¡ˆ
            if os.path.exists(speaker_profiles_file):
                with open(speaker_profiles_file, 'r', encoding='utf-8') as f:
                    speakers_data = json.load(f)
                    for speaker_id, data in speakers_data.items():
                        # åŠ è½½å¯¹åº”çš„embeddingæ–‡ä»¶
                        embedding_file = os.path.join(self.cache_dir, f"{speaker_id}_embedding.npy")
                        if os.path.exists(embedding_file):
                            embedding = np.load(embedding_file)
                            profile = SpeakerProfile(
                                speaker_id=speaker_id,
                                embedding=embedding,
                                encounter_count=data.get('encounter_count', 1),
                                first_seen=data.get('last_seen', time.time()),
                                last_seen=data.get('last_seen', time.time()),
                                familiarity_score=data.get('familiarity_score', 0.1),
                                voice_characteristics={}
                            )
                            self.speaker_cache[speaker_id] = profile
            elif os.path.exists(speakers_file):
                # å…¼å®¹æ—§æ ¼å¼
                with open(speakers_file, 'r', encoding='utf-8') as f:
                    speakers_data = json.load(f)
                    for speaker_id, data in speakers_data.items():
                        # é‡å»ºSpeakerProfileå¯¹è±¡
                        embedding = np.array(data['embedding'])
                        profile = SpeakerProfile(
                            speaker_id=speaker_id,
                            embedding=embedding,
                            encounter_count=data['encounter_count'],
                            first_seen=data['first_seen'],
                            last_seen=data['last_seen'],
                            familiarity_score=data['familiarity_score'],
                            voice_characteristics=data.get('voice_characteristics', {})
                        )
                        self.speaker_cache[speaker_id] = profile
                        
            total_audio_events = (self.audio_stats["total_music_count"] +
                                 self.audio_stats["total_noise_count"] +
                                 self.audio_stats["total_speech_count"])
            print(f"ğŸ“Š åŠ è½½éŸ³é¢‘ç»Ÿè®¡: {total_audio_events} ä¸ªäº‹ä»¶")
            print(f"ğŸ‘¥ åŠ è½½å£°çº¹æ¡£æ¡ˆ: {len(self.speaker_cache)} äºº")
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æŒä¹…åŒ–æ•°æ®å¤±è´¥: {e}")
    
    def save_persistent_data(self):
        """ä¿å­˜æŒä¹…åŒ–æ•°æ®"""
        try:
            # ä¿å­˜éŸ³é¢‘ç»Ÿè®¡
            stats_to_save = {
                "total_music_count": self.audio_stats["total_music_count"],
                "total_noise_count": self.audio_stats["total_noise_count"], 
                "total_speech_count": self.audio_stats["total_speech_count"]
            }
            stats_file = os.path.join(self.cache_dir, "audio_stats.json")
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats_to_save, f, ensure_ascii=False, indent=2)
                
            # ä¿å­˜å£°çº¹ç¼“å­˜
            speakers_to_save = {}
            for speaker_id, profile in self.speaker_cache.items():
                speakers_to_save[speaker_id] = {
                    "embedding": profile.embedding.tolist(),
                    "encounter_count": profile.encounter_count,
                    "first_seen": profile.first_seen,
                    "last_seen": profile.last_seen,
                    "familiarity_score": profile.familiarity_score,
                    "voice_characteristics": profile.voice_characteristics or {}
                }
            
            speakers_file = os.path.join(self.cache_dir, "speakers.json")
            with open(speakers_file, 'w', encoding='utf-8') as f:
                json.dump(speakers_to_save, f, ensure_ascii=False, indent=2)
                
            print("ğŸ’¾ æŒä¹…åŒ–æ•°æ®ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æŒä¹…åŒ–æ•°æ®å¤±è´¥: {e}")
    
    def process_audio_file(self, audio_path: str, text_content: str = "") -> Dict:
        """ğŸ¯ å¤„ç†éŸ³é¢‘æ–‡ä»¶çš„ä¸»å…¥å£å‡½æ•°"""
        try:
            # 1. ğŸµ éŸ³é¢‘äº‹ä»¶æ£€æµ‹
            audio_events = self._detect_audio_events(audio_path)
            
            # 2. ğŸ‘¥ å£°çº¹è¯†åˆ«
            speaker_info = self._identify_speaker(audio_path)
            
            # 3. ğŸ“Š æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_audio_statistics(audio_events)
            
            # 4. ğŸ§  ç”Ÿæˆä¸Šä¸‹æ–‡åˆ†æ
            context_analysis = self._analyze_context(audio_events, speaker_info, text_content)
            
            # 5. ğŸ“¦ æ„å»ºå®Œæ•´ç»“æœ
            result = {
                "audio_events": [asdict(event) for event in audio_events],
                "speaker_info": asdict(speaker_info) if speaker_info else None,
                "context_analysis": context_analysis,
                "statistics": self._get_current_statistics(),
                "timestamp": time.time()
            }
            
            # 6. ğŸ’¾ å¼‚æ­¥ä¿å­˜æ•°æ®
            threading.Thread(target=self.save_persistent_data).start()
            
            return result
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            return {"error": str(e), "timestamp": time.time()}
    
    def _detect_audio_events(self, audio_path: str) -> List[AudioEvent]:
        """ğŸµ æ£€æµ‹éŸ³é¢‘äº‹ä»¶ï¼ˆéŸ³ä¹ã€å™ªéŸ³ã€äººå£°ï¼‰"""
        events = []
        
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            y, sr = librosa.load(audio_path, sr=22050)
            duration = len(y) / sr
            # è®°å½•å½“å‰éŸ³é¢‘è·¯å¾„ï¼Œä¾›YAMNetçœŸå®æ£€æµ‹ä½¿ç”¨ï¼ˆé¿å…é‡å¤è½ç›˜ï¼‰
            self._current_audio_path = audio_path
            
            # ğŸµ éŸ³ä¹æ£€æµ‹ï¼ˆåŸºäºé¢‘è°±ç‰¹å¾ï¼‰
            music_confidence = self._detect_music(y, sr, audio_path)
            if music_confidence > 0.3:
                events.append(AudioEvent(
                    event_type="music",
                    confidence=music_confidence,
                    timestamp=time.time(),
                    duration=duration,
                    metadata={"spectral_features": "detected"}
                ))
            
            # ğŸ”Š å™ªéŸ³æ£€æµ‹ï¼ˆåŸºäºèƒ½é‡å’Œé¢‘è°±ï¼‰
            noise_confidence = self._detect_noise(y, sr)
            if noise_confidence > 0.4:
                events.append(AudioEvent(
                    event_type="noise", 
                    confidence=noise_confidence,
                    timestamp=time.time(),
                    duration=duration,
                    metadata={"noise_type": "environmental"}
                ))
            
            # ğŸ—£ï¸ äººå£°æ£€æµ‹ï¼ˆåŸºäºMFCCç‰¹å¾ï¼‰
            speech_confidence = self._detect_speech(y, sr)
            if speech_confidence > 0.5:
                events.append(AudioEvent(
                    event_type="speech",
                    confidence=speech_confidence, 
                    timestamp=time.time(),
                    duration=duration,
                    metadata={"speech_quality": "clear" if speech_confidence > 0.8 else "unclear"}
                ))
                
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘äº‹ä»¶æ£€æµ‹å¤±è´¥: {e}")
            
        return events
    
    def _detect_music(self, y: np.ndarray, sr: int, audio_path: Optional[str] = None) -> float:
        """ğŸµ éŸ³ä¹æ£€æµ‹ç®—æ³• - ğŸ”¥ ä¼˜å…ˆä½¿ç”¨YAMNetï¼Œå›é€€åˆ°librosa"""
        try:
            # ğŸ¯ ä¼˜å…ˆå°è¯•YAMNetéŸ³ä¹æ£€æµ‹
            yamnet_confidence = self._yamnet_music_detection(audio_path or getattr(self, '_current_audio_path', None))
            if yamnet_confidence > 0:
                print(f"âœ… YAMNetéŸ³ä¹æ£€æµ‹: {yamnet_confidence:.3f}")
                return yamnet_confidence

            # ğŸ”„ å›é€€åˆ°librosaåŸºç¡€æ£€æµ‹
            print("âš ï¸ YAMNetä¸å¯ç”¨ï¼Œä½¿ç”¨librosaåŸºç¡€æ£€æµ‹")

            # è®¡ç®—é¢‘è°±è´¨å¿ƒ
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]

            # è®¡ç®—èŠ‚æ‹å¼ºåº¦
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)

            # è®¡ç®—è‰²åº¦ç‰¹å¾
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)

            # ç»¼åˆåˆ¤æ–­éŸ³ä¹ç½®ä¿¡åº¦
            music_score = 0.0

            # èŠ‚æ‹è§„å¾‹æ€§
            if tempo > 60 and tempo < 200:
                music_score += 0.3

            # é¢‘è°±ç¨³å®šæ€§
            if np.std(spectral_centroids) < np.mean(spectral_centroids) * 0.5:
                music_score += 0.3

            # è‰²åº¦ç‰¹å¾ä¸°å¯Œåº¦
            if np.mean(chroma) > 0.1:
                music_score += 0.4

            return min(music_score, 1.0)

        except Exception as e:
            print(f"âš ï¸ éŸ³ä¹æ£€æµ‹å¤±è´¥: {e}")
            return 0.0

    def _yamnet_music_detection(self, audio_path: Optional[str]) -> float:
        """ğŸ¯ ä½¿ç”¨çœŸå®YAMNetæ¨ç†çš„éŸ³ä¹æ£€æµ‹ï¼ˆé€šè¿‡SmartAudioCollectorï¼‰ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶è·¯å¾„"""
        try:
            if not audio_path:
                return 0.0
            from core.smart_audio_collector import get_audio_collector
            collector = get_audio_collector()
            # è°ƒç”¨å¹¶è¡Œåˆ†æè·¯å¾„ï¼ŒåŸºäºåŸå§‹æ–‡ä»¶è·¯å¾„ï¼Œé¿å…é‡å¤è½ç›˜
            _ = collector._sensevoice_audio_classification(audio_path)

            # è¯»å–æœ€è¿‘ä¸€æ¬¡å®Œæ•´åˆ†æ
            yamnet_result = getattr(collector, '_last_complete_analysis', {}).get('yamnet_result', {})
            env = yamnet_result.get('environment_detection', {})

            # ä»¥éŸ³ä¹æ£€æµ‹ä¸ºç›®æ ‡ï¼Œä¼˜å…ˆè¯»å–top_classå«Musicæˆ–environmentæ ‡å¿—
            top_class = yamnet_result.get('top_class', '')
            conf = float(yamnet_result.get('confidence', 0.0))
            if 'Music' in top_class or env.get('music_detected', False):
                return max(conf, 0.6)
            return conf

        except Exception as e:
            print(f"âš ï¸ çœŸå®YAMNetéŸ³ä¹æ£€æµ‹å¤±è´¥: {e}")
            return 0.0
    
    def _detect_noise(self, y: np.ndarray, sr: int) -> float:
        """ğŸ”Š å™ªéŸ³æ£€æµ‹ç®—æ³•"""
        try:
            # è®¡ç®—é›¶äº¤å‰ç‡
            zcr = librosa.feature.zero_crossing_rate(y)[0]
            
            # è®¡ç®—é¢‘è°±æ»šé™
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
            
            # è®¡ç®—èƒ½é‡
            rms = librosa.feature.rms(y=y)[0]
            
            # å™ªéŸ³ç‰¹å¾åˆ¤æ–­
            noise_score = 0.0
            
            # é«˜é›¶äº¤å‰ç‡ï¼ˆå™ªéŸ³ç‰¹å¾ï¼‰
            if np.mean(zcr) > 0.1:
                noise_score += 0.4
                
            # ä¸è§„åˆ™çš„é¢‘è°±æ»šé™
            if np.std(spectral_rolloff) > np.mean(spectral_rolloff) * 0.3:
                noise_score += 0.3
                
            # èƒ½é‡æ³¢åŠ¨å¤§
            if np.std(rms) > np.mean(rms) * 0.5:
                noise_score += 0.3
                
            return min(noise_score, 1.0)
            
        except Exception as e:
            print(f"âš ï¸ å™ªéŸ³æ£€æµ‹å¤±è´¥: {e}")
            return 0.0
    
    def _detect_speech(self, y: np.ndarray, sr: int) -> float:
        """ğŸ—£ï¸ äººå£°æ£€æµ‹ç®—æ³•"""
        try:
            # è®¡ç®—MFCCç‰¹å¾
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            
            # è®¡ç®—é¢‘è°±è´¨å¿ƒ
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            
            # è®¡ç®—è¯­éŸ³æ´»åŠ¨æ£€æµ‹
            intervals = librosa.effects.split(y, top_db=20)
            
            # äººå£°ç‰¹å¾åˆ¤æ–­
            speech_score = 0.0
            
            # MFCCç‰¹å¾ç¨³å®šæ€§ï¼ˆäººå£°ç‰¹å¾ï¼‰
            if np.mean(np.std(mfccs, axis=1)) < 50:
                speech_score += 0.4
                
            # é¢‘è°±è´¨å¿ƒåœ¨äººå£°èŒƒå›´å†…
            mean_centroid = np.mean(spectral_centroids)
            if 1000 < mean_centroid < 4000:  # äººå£°é¢‘ç‡èŒƒå›´
                speech_score += 0.4
                
            # æœ‰æ•ˆè¯­éŸ³æ®µæ¯”ä¾‹
            if len(intervals) > 0:
                speech_ratio = sum(end - start for start, end in intervals) / len(y)
                speech_score += min(speech_ratio * 0.2, 0.2)
                
            return min(speech_score, 1.0)
            
        except Exception as e:
            print(f"âš ï¸ äººå£°æ£€æµ‹å¤±è´¥: {e}")
            return 0.0

    def _identify_speaker(self, audio_path: str) -> Optional[Dict]:
        """ğŸ‘¥ å£°çº¹è¯†åˆ«å·²ç§»é™¤ - ç»Ÿä¸€ä½¿ç”¨core.speaker_recognition"""
        print("[å£°çº¹è¯†åˆ«] âš ï¸ å£°çº¹è¯†åˆ«åŠŸèƒ½å·²ç§»é™¤ï¼Œè¯·ä½¿ç”¨ç»Ÿä¸€çš„SpeakerManager")
        return None

    def _analyze_voice_characteristics(self, wav: np.ndarray) -> Dict:
        """ğŸ¤ åˆ†æå£°éŸ³ç‰¹å¾"""
        try:
            # åŸºç¡€å£°éŸ³ç‰¹å¾åˆ†æ
            characteristics = {
                "pitch_mean": float(np.mean(wav)),
                "pitch_std": float(np.std(wav)),
                "energy_level": float(np.sqrt(np.mean(wav**2))),
                "duration": len(wav) / 16000,
                "voice_type": "unknown"
            }

            # ç®€å•çš„å£°éŸ³ç±»å‹åˆ¤æ–­
            if characteristics["pitch_mean"] > 0.1:
                characteristics["voice_type"] = "high_pitch"
            elif characteristics["pitch_mean"] < -0.1:
                characteristics["voice_type"] = "low_pitch"
            else:
                characteristics["voice_type"] = "medium_pitch"

            return characteristics

        except Exception as e:
            print(f"âš ï¸ å£°éŸ³ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}

    def _update_audio_statistics(self, events: List[AudioEvent]):
        """ğŸ“Š æ›´æ–°éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()

        for event in events:
            if event.event_type == "music":
                self.audio_stats["music_events"].append(event)
                self.audio_stats["total_music_count"] += 1

            elif event.event_type == "noise":
                self.audio_stats["noise_events"].append(event)
                self.audio_stats["total_noise_count"] += 1

            elif event.event_type == "speech":
                self.audio_stats["speech_events"].append(event)
                self.audio_stats["total_speech_count"] += 1

    def _analyze_context(self, events: List[AudioEvent], speaker_info: Optional[SpeakerProfile], text_content: str) -> Dict:
        """ğŸ§  æ™ºèƒ½ä¸Šä¸‹æ–‡åˆ†æ - ä½¿ç”¨å‰è„‘ç³»ç»ŸMiniMaxAIæ¨¡å‹"""

        # å…ˆè¿›è¡ŒåŸºç¡€åˆ†æä½œä¸ºå¤‡ç”¨
        basic_analysis = self._basic_context_analysis(events, speaker_info, text_content)

        try:
            # ğŸ§  è°ƒç”¨å‰è„‘ç³»ç»ŸéŸ³é¢‘ä¸Šä¸‹æ–‡æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æ
            ai_analysis = self._call_audio_context_model(events, speaker_info, text_content)

            # å¦‚æœä¸»æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹
            if not ai_analysis:
                self.logger.warning("âš ï¸ ä¸»æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹")
                ai_analysis = self._call_fallback_audio_context_model(events, speaker_info, text_content)

            # åˆå¹¶AIåˆ†æå’ŒåŸºç¡€åˆ†æ
            if ai_analysis:
                # AIåˆ†ææˆåŠŸï¼Œä½¿ç”¨AIç»“æœå¹¶è¡¥å……åŸºç¡€ä¿¡æ¯
                analysis = ai_analysis
                analysis.update({
                    "ai_analysis": True,
                    "basic_fallback": basic_analysis
                })
            else:
                # AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                analysis = basic_analysis
                analysis["ai_analysis"] = False

            return analysis

        except Exception as e:
            self.logger.error(f"âŒ éŸ³é¢‘ä¸Šä¸‹æ–‡AIåˆ†æå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€åˆ†æä½œä¸ºå¤‡ç”¨
            basic_analysis["ai_analysis"] = False
            basic_analysis["error"] = str(e)
            return basic_analysis

    def _basic_context_analysis(self, events: List[AudioEvent], speaker_info: Optional[SpeakerProfile], text_content: str) -> Dict:
        """åŸºç¡€ä¸Šä¸‹æ–‡åˆ†æ - ä½œä¸ºAIåˆ†æçš„å¤‡ç”¨"""
        analysis = {
            "context_type": "normal",
            "suggestions": [],
            "familiarity_level": "unknown",
            "audio_environment": "quiet",
            "interaction_mode": "casual"
        }

        try:
            # ğŸµ éŸ³é¢‘ç¯å¢ƒåˆ†æ
            music_events = [e for e in events if e.event_type == "music"]
            noise_events = [e for e in events if e.event_type == "noise"]
            speech_events = [e for e in events if e.event_type == "speech"]

            if music_events:
                analysis["audio_environment"] = "musical"
                analysis["suggestions"].append("å¯èƒ½æƒ³è¦è®¨è®ºéŸ³ä¹æˆ–è·Ÿç€å“¼å”±")

            if noise_events:
                analysis["audio_environment"] = "noisy"
                analysis["suggestions"].append("ç¯å¢ƒè¾ƒåµï¼Œå¯èƒ½éœ€è¦è°ƒæ•´éŸ³é‡æˆ–è¯„è®ºç¯å¢ƒ")

            # ğŸ‘¥ ç†Ÿæ‚‰åº¦åˆ†æ
            if speaker_info:
                if speaker_info.familiarity_score >= 1.0:
                    analysis["familiarity_level"] = "very_familiar"
                    analysis["interaction_mode"] = "intimate"
                    analysis["suggestions"].append("è¿™æ˜¯å¾ˆç†Ÿæ‚‰çš„äººï¼Œå¯ä»¥æ›´äº²å¯†åœ°äº¤æµ")

                elif speaker_info.familiarity_score >= 0.5:
                    analysis["familiarity_level"] = "familiar"
                    analysis["interaction_mode"] = "friendly"
                    analysis["suggestions"].append("è¿™æ˜¯è®¤è¯†çš„äººï¼Œå¯ä»¥å‹å¥½äº¤æµ")

                else:
                    analysis["familiarity_level"] = "new"
                    analysis["interaction_mode"] = "polite"
                    analysis["suggestions"].append("è¿™å¯èƒ½æ˜¯æ–°æœ‹å‹ï¼Œä¿æŒç¤¼è²Œå‹å¥½")

            # ğŸ“ˆ å†å²æ¨¡å¼åˆ†æ
            recent_music_count = len([e for e in self.audio_stats["music_events"]
                                    if time.time() - e.timestamp < 300])  # 5åˆ†é’Ÿå†…

            if recent_music_count >= 3:
                analysis["context_type"] = "music_session"
                analysis["suggestions"].append("æœ€è¿‘éŸ³ä¹æ’­æ”¾è¾ƒå¤šï¼Œç”¨æˆ·å¯èƒ½åœ¨äº«å—éŸ³ä¹æ—¶å…‰")

            # ğŸ¯ æ–‡æœ¬å†…å®¹å…³è”åˆ†æ
            if text_content:
                music_keywords = ["éŸ³ä¹", "æ­Œ", "å”±", "æ—‹å¾‹", "èŠ‚æ‹", "å¥½å¬"]
                emotion_keywords = ["å¿ƒæƒ…", "æ„Ÿè§‰", "å¼€å¿ƒ", "éš¾è¿‡", "å…´å¥‹"]

                if any(keyword in text_content for keyword in music_keywords):
                    analysis["suggestions"].append("ç”¨æˆ·æåˆ°éŸ³ä¹ç›¸å…³å†…å®¹ï¼Œå¯ä»¥æ·±å…¥è®¨è®º")

                if any(keyword in text_content for keyword in emotion_keywords):
                    analysis["suggestions"].append("ç”¨æˆ·è¡¨è¾¾äº†æƒ…ç»ªï¼Œå¯ä»¥ç»™äºˆæƒ…æ„Ÿæ”¯æŒ")

        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")

        return analysis

    def _call_audio_context_model(self, events: List[AudioEvent], speaker_info: Optional[SpeakerProfile], text_content: str) -> Optional[Dict]:
        """è°ƒç”¨å‰è„‘ç³»ç»ŸéŸ³é¢‘ä¸Šä¸‹æ–‡æ¨¡å‹ - MiniMaxAI/MiniMax-M1-80k"""
        try:
            import configparser
            import requests
            import json
            import threading

            # è¯»å–å‰è„‘ç³»ç»Ÿé…ç½®
            config = configparser.ConfigParser()
            config.read("system.conf", encoding='utf-8')

            # ğŸ”§ æ­£ç¡®ä½¿ç”¨ä½ çš„é…ç½®ç³»ç»Ÿ
            api_key = config.get('key', 'audio_context_api_key', fallback='910663e20c4a49b286f27009dde10497.qYauy3JahUXDed7C')
            base_url = config.get('key', 'audio_context_base_url', fallback='https://open.bigmodel.cn/api/paas/v4/')
            model = config.get('key', 'audio_context_model', fallback='GLM-4.5-Flash')
            temperature = float(config.get('key', 'audio_context_temperature', fallback='0.6'))
            max_tokens = int(config.get('key', 'audio_context_max_tokens', fallback='2000'))

            # ğŸ¯ ä½¿ç”¨ä¸“ä¸šæç¤ºè¯é…ç½®
            try:
                from sisi_brain.brain_prompts_config import BrainPromptsConfig
                system_prompt = BrainPromptsConfig.get_audio_context_prompt()
            except ImportError:
                system_prompt = 'ä½ æ˜¯Sisiçš„éŸ³é¢‘ç¯å¢ƒæ„ŸçŸ¥ä¸“å®¶ï¼Œæ“…é•¿ä»éŸ³é¢‘ç‰¹å¾æ¨æµ‹ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€å’Œç¯å¢ƒä¿¡æ¯ã€‚'

            # æ„å»ºéŸ³é¢‘åˆ†ææç¤ºè¯
            prompt = self._build_audio_analysis_prompt(events, speaker_info, text_content)

            # è°ƒç”¨API
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': model,
                'messages': [
                    {
                        'role': 'system',
                        'content': system_prompt  # ğŸ¯ ä½¿ç”¨ä¸“ä¸šæç¤ºè¯
                    },
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ],
                'temperature': temperature,
                'max_tokens': max_tokens
            }

            # ğŸ¯ ä¿®å¤ï¼šå°è¯•åŒæ­¥è°ƒç”¨ï¼Œå¤±è´¥æ—¶è¿”å›Noneè§¦å‘å¤‡ç”¨æ¨¡å‹
            try:
                response = requests.post(
                    f"{base_url}/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=6  # 6ç§’è¶…æ—¶
                )
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result['choices'][0]['message']['content']
                    self.logger.info("âœ… éŸ³é¢‘ä¸Šä¸‹æ–‡APIè°ƒç”¨æˆåŠŸ")

                    # è§£æAIå“åº”å¹¶è¿”å›ç»“æ„åŒ–ç»“æœ
                    return {
                        "environment_type": "ai_analyzed",
                        "confidence": 0.8,
                        "suggestions": ["åŸºäºAIåˆ†æçš„å»ºè®®"],
                        "familiarity_level": "ai_detected",
                        "audio_environment": "ai_analyzed",
                        "interaction_mode": "ai_optimized",
                        "ai_response": ai_response[:100] + "..." if len(ai_response) > 100 else ai_response
                    }
                else:
                    self.logger.error(f"âŒ éŸ³é¢‘ä¸Šä¸‹æ–‡æ¨¡å‹APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                    return None  # è¿”å›Noneè§¦å‘å¤‡ç”¨æ¨¡å‹

            except Exception as e:
                self.logger.error(f"âŒ éŸ³é¢‘ä¸Šä¸‹æ–‡APIè°ƒç”¨å¤±è´¥: {e}")
                return None  # è¿”å›Noneè§¦å‘å¤‡ç”¨æ¨¡å‹

        except Exception as e:
            self.logger.error(f"âŒ è°ƒç”¨éŸ³é¢‘ä¸Šä¸‹æ–‡æ¨¡å‹å¤±è´¥: {e}")
            return None

    def _build_audio_analysis_prompt(self, events: List[AudioEvent], speaker_info: Optional[SpeakerProfile], text_content: str) -> str:
        """æ„å»ºéŸ³é¢‘åˆ†ææç¤ºè¯ - ç¬¦åˆäººç±»å‰è„‘ç‰¹å¾"""

        # æ•´ç†éŸ³é¢‘äº‹ä»¶ä¿¡æ¯
        event_summary = []
        for event in events:
            event_summary.append(f"- {event.event_type}: ç½®ä¿¡åº¦{event.confidence:.2f}, æ—¶é•¿{event.duration:.1f}ç§’")

        # æ•´ç†è¯´è¯äººä¿¡æ¯
        speaker_summary = "æœªçŸ¥ç”¨æˆ·"
        if speaker_info:
            # ğŸ¯ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å±æ€§åvoice_characteristics
            characteristics = speaker_info.voice_characteristics or {}
            speaker_summary = f"ç†Ÿæ‚‰åº¦{speaker_info.familiarity_score:.2f}, ç‰¹å¾: {characteristics}"

        prompt = f"""ä½ æ˜¯Sisiçš„éŸ³é¢‘ç¯å¢ƒæ„ŸçŸ¥ä¸“å®¶ï¼Œå…·å¤‡äººç±»å‰è„‘çš„éŸ³é¢‘å¤„ç†ç‰¹å¾ã€‚

### ğŸ§  äººç±»å‰è„‘éŸ³é¢‘æ„ŸçŸ¥ç‰¹å¾
1. **ç¯å¢ƒæ„ŸçŸ¥**: è‡ªç„¶åœ°æ„Ÿå—éŸ³é¢‘ç¯å¢ƒçš„æ°›å›´å’Œç‰¹ç‚¹
2. **æƒ…æ„Ÿæ¨æµ‹**: ä»éŸ³é¢‘ç‰¹å¾æ¨æµ‹ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€å’Œå¿ƒç†éœ€æ±‚
3. **äº¤äº’ç­–ç•¥**: åŸºäºéŸ³é¢‘ç¯å¢ƒæä¾›ä¸ªæ€§åŒ–çš„äº¤äº’å»ºè®®
4. **äººæ€§åŒ–æè¿°**: ç”¨è‡ªç„¶è¯­è¨€æè¿°éŸ³é¢‘æ„Ÿå—ï¼Œé¿å…æŠ€æœ¯æœ¯è¯­

### ğŸµ æ£€æµ‹åˆ°çš„éŸ³é¢‘äº‹ä»¶
{chr(10).join(event_summary) if event_summary else "æ— ç‰¹æ®ŠéŸ³é¢‘äº‹ä»¶"}

### ğŸ‘¤ è¯´è¯äººä¿¡æ¯
{speaker_summary}

### ğŸ’¬ ç”¨æˆ·æ–‡æœ¬å†…å®¹
{text_content if text_content else "æ— æ–‡æœ¬å†…å®¹"}

### ğŸ“ åˆ†æè¦æ±‚
è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè¿›è¡Œäººæ€§åŒ–çš„éŸ³é¢‘ç¯å¢ƒåˆ†æï¼Œä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "context_type": "éŸ³é¢‘ç¯å¢ƒç±»å‹(normal/musical/noisy/intimate)",
    "audio_environment": "ç¯å¢ƒæè¿°(quiet/musical/noisy/conversational)",
    "familiarity_level": "ç†Ÿæ‚‰ç¨‹åº¦(unknown/familiar/very_familiar)",
    "interaction_mode": "äº¤äº’æ¨¡å¼(casual/friendly/intimate/formal)",
    "emotional_state": "æ¨æµ‹çš„ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€",
    "environment_feeling": "ç¯å¢ƒæ°›å›´æ„Ÿå—",
    "suggestions": ["å…·ä½“çš„äº¤äº’å»ºè®®1", "å…·ä½“çš„äº¤äº’å»ºè®®2"],
    "confidence": 0.85
}}

è¯·ç”¨Sisiçš„æ„ŸçŸ¥æ–¹å¼åˆ†æï¼Œæ³¨é‡æƒ…æ„Ÿå…±é¸£å’Œäººæ€§åŒ–ç†è§£ï¼š"""

        return prompt

    def _parse_ai_analysis(self, ai_response: str) -> Optional[Dict]:
        """è§£æAIåˆ†æç»“æœ"""
        try:
            # å°è¯•æå–JSON
            if '```json' in ai_response:
                json_start = ai_response.find('```json') + 7
                json_end = ai_response.find('```', json_start)
                json_text = ai_response[json_start:json_end].strip()
            else:
                # å¯»æ‰¾JSONå¯¹è±¡
                start = ai_response.find('{')
                end = ai_response.rfind('}') + 1
                json_text = ai_response[start:end]

            result = json.loads(json_text)

            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['context_type', 'audio_environment', 'suggestions']
            if all(field in result for field in required_fields):
                return result
            else:
                self.logger.warning("âš ï¸ AIåˆ†æç»“æœç¼ºå°‘å¿…è¦å­—æ®µ")
                return None

        except Exception as e:
            self.logger.error(f"âŒ è§£æAIåˆ†æç»“æœå¤±è´¥: {e}")
            return None

    def _get_current_statistics(self) -> Dict:
        """ğŸ“Š è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_music_count": self.audio_stats["total_music_count"],
            "total_noise_count": self.audio_stats["total_noise_count"],
            "total_speech_count": self.audio_stats["total_speech_count"],
            "known_speakers": len(self.speaker_cache),
            "familiar_speakers": len([p for p in self.speaker_cache.values()
                                    if p.familiarity_score >= 0.5]),
            "recent_events": {
                "music": len([e for e in self.audio_stats["music_events"]
                            if time.time() - e.timestamp < 300]),
                "noise": len([e for e in self.audio_stats["noise_events"]
                            if time.time() - e.timestamp < 300]),
                "speech": len([e for e in self.audio_stats["speech_events"]
                             if time.time() - e.timestamp < 300])
            }
        }

    def get_context_prompt(self, analysis_result: Dict) -> Optional[str]:
        """ğŸ¯ ç”Ÿæˆä¸Šä¸‹æ–‡æç¤ºè¯"""
        if not analysis_result or "context_analysis" not in analysis_result:
            return None

        context = analysis_result["context_analysis"]
        suggestions = context.get("suggestions", [])

        if not suggestions:
            return None

        # ğŸ¯ æ„å»ºæŸ³æ€æ€é£æ ¼çš„ä¸Šä¸‹æ–‡æç¤ºè¯
        prompt_parts = [
            "[éŸ³é¢‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥]",
            f"ç¯å¢ƒ: {context.get('audio_environment', 'å®‰é™')}",
            f"ç†Ÿæ‚‰åº¦: {context.get('familiarity_level', 'æœªçŸ¥')}",
            f"äº¤æµæ¨¡å¼: {context.get('interaction_mode', 'éšæ„')}"
        ]

        if suggestions:
            prompt_parts.append("å»ºè®®:")
            for suggestion in suggestions[:3]:  # æœ€å¤š3ä¸ªå»ºè®®
                prompt_parts.append(f"- {suggestion}")

        prompt_parts.append("\nè¯·æ ¹æ®ä»¥ä¸ŠéŸ³é¢‘ä¸Šä¸‹æ–‡è°ƒæ•´å›åº”é£æ ¼ã€‚")

        return "\n".join(prompt_parts)

    def analyze_audio_context(self, audio_path):
        """ğŸ¯ åˆ†æéŸ³é¢‘ä¸Šä¸‹æ–‡ - ä¸»è¦æ¥å£æ–¹æ³•"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            import os
            if not os.path.exists(audio_path):
                print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                return {
                    'environment_type': 'error',
                    'confidence': 0.0,
                    'has_music': False,
                    'has_speech': False,
                    'noise_level': 'unknown',
                    'audio_features': {'error': 'file_not_found'}
                }

            # ä½¿ç”¨å®Œæ•´çš„éŸ³é¢‘å¤„ç†æµç¨‹
            result = self.process_audio_file(audio_path)

            # ğŸ”¥ è·å–YAMNetéŸ³ä¹æ£€æµ‹ç½®ä¿¡åº¦
            music_events = [event for event in result.get('audio_events', []) if event.get('event_type') == 'music']
            music_confidence = max([event.get('confidence', 0.0) for event in music_events], default=0.0)

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ŒåŒ…å«YAMNetéŸ³ä¹æ£€æµ‹ä¿¡æ¯
            return {
                'environment_type': result.get('context_analysis', {}).get('environment_type', 'unknown'),
                'confidence': music_confidence,  # ğŸ¯ ä½¿ç”¨éŸ³ä¹æ£€æµ‹çš„ç½®ä¿¡åº¦
                'has_music': music_confidence > 0.5,  # ğŸ¯ åŸºäºYAMNetç½®ä¿¡åº¦åˆ¤æ–­
                'has_speech': any(event.get('event_type') == 'speech' for event in result.get('audio_events', [])),
                'noise_level': result.get('context_analysis', {}).get('noise_level', 'low'),
                'yamnet_music_confidence': music_confidence,  # ğŸ”¥ æ–°å¢ï¼šYAMNetéŸ³ä¹ç½®ä¿¡åº¦
                'audio_features': {
                    'duration': result.get('context_analysis', {}).get('duration', 0),
                    'events_count': len(result.get('audio_events', [])),
                    'speaker_detected': result.get('speaker_info') is not None,
                    'music_events_count': len(music_events),  # ğŸ”¥ æ–°å¢ï¼šéŸ³ä¹äº‹ä»¶æ•°é‡
                    'yamnet_detection_method': 'YAMNet_521_classes'  # ğŸ”¥ æ–°å¢ï¼šæ£€æµ‹æ–¹æ³•æ ‡è¯†
                }
            }
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return {
                'environment_type': 'error',
                'confidence': 0.0,
                'has_music': False,
                'has_speech': False,
                'noise_level': 'unknown',
                'audio_features': {'error': str(e)}
            }

    def _call_fallback_audio_context_model(self, events: List[AudioEvent], speaker_info: Optional[SpeakerProfile], text_content: str) -> Optional[Dict]:
        """è°ƒç”¨å¤‡ç”¨éŸ³é¢‘ä¸Šä¸‹æ–‡æ¨¡å‹"""
        try:
            import configparser
            import requests
            import json

            # è¯»å–å¤‡ç”¨æ¨¡å‹é…ç½®
            config = configparser.ConfigParser()
            config.read("system.conf", encoding='utf-8')

            fallback_api_key = config.get('key', 'audio_context_fallback_api_key', fallback='')
            fallback_base_url = config.get('key', 'audio_context_fallback_base_url', fallback='https://api.siliconflow.cn/v1')
            fallback_model = config.get('key', 'audio_context_fallback_model', fallback='Qwen/Qwen3-8B')
            fallback_temperature = float(config.get('key', 'audio_context_fallback_temperature', fallback='0.6'))
            fallback_max_tokens = int(config.get('key', 'audio_context_fallback_max_tokens', fallback='2000'))

            # æ„å»ºè¯·æ±‚æ•°æ®
            prompt = self._build_audio_analysis_prompt(events, speaker_info, text_content)

            headers = {
                "Authorization": f"Bearer {fallback_api_key}",
                "Content-Type": "application/json"
            }

            data = {
                "model": fallback_model,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éŸ³é¢‘ç¯å¢ƒåˆ†æå¸ˆï¼Œæ“…é•¿ä»éŸ³é¢‘ç‰¹å¾ä¸­åˆ†æç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€å’Œç¯å¢ƒä¸Šä¸‹æ–‡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": fallback_temperature,
                "max_tokens": fallback_max_tokens
            }

            # å‘é€è¯·æ±‚
            response = requests.post(
                f"{fallback_base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=6  # 6ç§’è¶…æ—¶
            )

            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']

                # è§£æJSONå“åº”
                try:
                    analysis = json.loads(content)
                    self.logger.info("âœ… å¤‡ç”¨æ¨¡å‹éŸ³é¢‘ä¸Šä¸‹æ–‡åˆ†ææˆåŠŸ")
                    return analysis
                except json.JSONDecodeError:
                    self.logger.warning("âš ï¸ å¤‡ç”¨æ¨¡å‹è¿”å›éJSONæ ¼å¼ï¼Œä½¿ç”¨åŸºç¡€è§£æ")
                    return None
            else:
                self.logger.error(f"âŒ å¤‡ç”¨æ¨¡å‹APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return None

        except Exception as e:
            self.logger.error(f"âŒ å¤‡ç”¨æ¨¡å‹è°ƒç”¨å¼‚å¸¸: {e}")
            return None

# ğŸ¯ å…¨å±€å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_audio_processor_instance = None

def get_audio_context_processor() -> AudioContextProcessor:
    """è·å–éŸ³é¢‘ä¸Šä¸‹æ–‡å¤„ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _audio_processor_instance
    if _audio_processor_instance is None:
        _audio_processor_instance = AudioContextProcessor()
    return _audio_processor_instance
