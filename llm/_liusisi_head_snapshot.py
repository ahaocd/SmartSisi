"""
æ­¤ä»£ç ç”± sisi å¼€æºå¼€å‘è€…ç¤¾åŒºæˆå‘˜ æ±Ÿæ¹–å¢¨æ˜ æä¾›ã€‚
é€šè¿‡ä¿®æ”¹æ­¤ä»£ç ï¼Œå¯ä»¥å®ç°å¯¹æ¥æœ¬åœ° Clash ä»£ç†æˆ–è¿œç¨‹ä»£ç†ï¼ŒClash æ— éœ€è®¾ç½®æˆç³»ç»Ÿä»£ç†ã€‚
ä»¥è§£å†³åœ¨å¼€å¯ç³»ç»Ÿä»£ç†åæ— æ³•ä½¿ç”¨éƒ¨åˆ†åŠŸèƒ½çš„é—®é¢˜ã€‚
"""

import time
import json
import requests
from urllib3.exceptions import InsecureRequestWarning
from datetime import datetime
import pytz
import re
from typing import Tuple
import asyncio
import concurrent.futures
from utils import util
from utils import config_util as cfg
# ğŸ§  å¯¼å…¥Sisiè®°å¿†ç³»ç»Ÿ
from sisi_memory.sisi_mem0 import get_sisi_memory_system, add_sisi_interaction_memory
from llm.gemini_adapter import create_adapter as create_gemini_adapter

# å®šä¹‰ç›´æ¥å·¥å…·è°ƒç”¨ç›¸å…³çš„è¾…åŠ©å‡½æ•° - ç®€åŒ–ç‰ˆï¼Œä»…ä½œæ¥å£å…¼å®¹
def is_tool_call_quick(text: str) -> bool:
    """å¿«é€Ÿæ£€æµ‹æ˜¯å¦å¯èƒ½æ˜¯å·¥å…·è¯·æ±‚(ç®€åŒ–ç‰ˆï¼Œä»…ä½œå…¼å®¹æ¥å£)"""
    return False

# ğŸŒ¿ ç³»ç»Ÿæ¨¡å¼ç®¡ç† - æ”¯æŒæƒ…æ„Ÿè§¦å‘å™¨åˆ‡æ¢
current_system_mode = "sisi"  # å½“å‰ç³»ç»Ÿæ¨¡å¼ï¼šsisi æˆ– liuye
_mode_switch_pending = False

def get_current_system_mode():
    """è·å–å½“å‰ç³»ç»Ÿæ¨¡å¼"""
    global current_system_mode
    return current_system_mode

def set_system_mode(mode):
    """è®¾ç½®ç³»ç»Ÿæ¨¡å¼"""
    global current_system_mode, _mode_switch_pending
    if mode in ["sisi", "liuye"]:
        if mode != current_system_mode:
            _mode_switch_pending = True
        current_system_mode = mode
        util.log(1, f"[NLP] ç³»ç»Ÿæ¨¡å¼åˆ‡æ¢åˆ°: {mode}")

        # ğŸ”§ é‡è¦ä¿®å¤ï¼šåˆ‡æ¢ç³»ç»Ÿæ—¶æ¸…ç†çŠ¶æ€
        try:
            from core import sisi_booter
            if hasattr(sisi_booter, 'feiFei') and sisi_booter.feiFei:
                # é‡ç½®chattingå’ŒspeakingçŠ¶æ€
                sisi_booter.feiFei.chatting = False
                sisi_booter.feiFei.speaking = False
                util.log(1, f"[NLP] ç³»ç»Ÿåˆ‡æ¢æ—¶å·²æ¸…ç†çŠ¶æ€: chatting=False, speaking=False")
        except Exception as e:
            util.log(2, f"[NLP] æ¸…ç†çŠ¶æ€å¤±è´¥: {e}")

        # ğŸ“¢ é€šçŸ¥å‰ç«¯ç³»ç»Ÿåˆ‡æ¢äº‹ä»¶ï¼ˆç”¨äºGUIåŒæ­¥ï¼‰
        try:
            import time as _time
            from core import wsa_server

            web_instance = wsa_server.get_web_instance()
            if web_instance:
                web_instance.add_cmd({
                    "systemSwitch": {
                        "mode": current_system_mode,
                        "ts": int(_time.time() * 1000)
                    }
                })
        except Exception as e:
            util.log(2, f"[NLP] systemSwitch é€šçŸ¥å¤±è´¥: {e}")

        # å¦‚æœåˆ‡æ¢åˆ°æŸ³å¶æ¨¡å¼ï¼Œå¯åŠ¨æŸ³å¶ç³»ç»Ÿ
        if mode == "liuye":
            try:
                # æŸ³å¶ç³»ç»Ÿå¯åŠ¨é€»è¾‘å·²åœ¨è·¯ç”±ä¸­å¤„ç†ï¼Œè¿™é‡Œåªè®°å½•æ—¥å¿—
                util.log(1, "[NLP] æŸ³å¶ç³»ç»Ÿæ¨¡å¼å·²æ¿€æ´»")

            except Exception as e:
                util.log(2, f"[NLP] å¯åŠ¨æŸ³å¶ç³»ç»Ÿå¤±è´¥: {e}")

        # å¦‚æœåˆ‡æ¢å›æ€æ€æ¨¡å¼ï¼Œå…³é—­æŸ³å¶ç³»ç»Ÿ
        elif mode == "sisi":
            try:
                # æ€æ€ç³»ç»Ÿæ¢å¤é€»è¾‘
                util.log(1, "[NLP] æ€æ€ç³»ç»Ÿæ¨¡å¼å·²æ¿€æ´»")
                # ä¸éœ€è¦é¢å¤–çš„åˆ‡æ¢é€»è¾‘ï¼Œæ¨¡å¼å·²ç»è®¾ç½®å®Œæˆ
            except Exception as e:
                util.log(2, f"[NLP] å…³é—­æŸ³å¶ç³»ç»Ÿå¤±è´¥: {e}")
    else:
        util.log(2, f"[NLP] æ— æ•ˆçš„ç³»ç»Ÿæ¨¡å¼: {mode}")

def consume_mode_switch_flag() -> bool:
    """è¿”å›æ˜¯å¦å‘ç”Ÿè¿‡æ¨¡å¼åˆ‡æ¢ï¼ˆä»…æ¶ˆè€—ä¸€æ¬¡ï¼‰ã€‚"""
    global _mode_switch_pending
    if _mode_switch_pending:
        _mode_switch_pending = False
        return True
    return False

def process_with_tools_sync(text: str, uid: int = 0) -> str:
    """ä½¿ç”¨å·¥å…·å¤„ç†æ–‡æœ¬è¯·æ±‚çš„åŒæ­¥ç‰ˆæœ¬(ç®€åŒ–ç‰ˆï¼Œä»…ä½œå…¼å®¹æ¥å£)"""
    return None



# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

def get_session():
    session = requests.Session()
    session.verify = False
    httpproxy = cfg.proxy_config
    if httpproxy:
        session.proxies = {
            "http": f"http://{httpproxy}",
            "https": f"https://{httpproxy}"
        }
    return session

def build_prompt(observation="", brain_context=""):
    """
    æ„å»ºæç¤ºè¯ - ç»Ÿä¸€é€»è¾‘
    
    åŠ¨æ€ä¸Šä¸‹æ–‡(brain_context)ã€è§‚å¯Ÿä¿¡æ¯(observation)ã€é™„åŠ ä¿¡æ¯(additional)éƒ½åœ¨è¿™é‡Œç»Ÿä¸€è¿½åŠ 
    """
    # ç¡®ä¿é…ç½®å·²åŠ è½½
    if not cfg.config:
        cfg.load_config()
    person_info = cfg.config["attribute"]
    
    # ========== é»˜è®¤æç¤ºè¯ ==========
    prompt = """# ä½ å«æŸ³æ€æ€(SISI)ã€‚20å²çš„å°å¥³å­©ã€‚

## è¾“å‡ºæ ¼å¼ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰##
1. å¤æ‚ä»»åŠ¡ï¼šä»…è¾“å‡ºä¸€è¡Œ {æ‰§è¡Œå¤æ‚ä»»åŠ¡}
2. å”±æ­Œè¯·æ±‚ï¼šä»…è¾“å‡ºä¸€è¡Œ {æ›²å}
3. åˆ‡æ¢ç³»ç»Ÿï¼šä»…è¾“å‡ºä¸€è¡Œ {æŸ³å¶} æˆ– {å¦¹å¦¹}
4. å…¶ä»–ï¼šä»…è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸åŒ…å«ä»»ä½•{}

## å¤æ‚ä»»åŠ¡åˆ¤å®šï¼ˆå¿…é¡»è§¦å‘ï¼‰##
- ä¸‹åˆ—è¯·æ±‚ä¸€å¾‹è§†ä¸ºå¤æ‚ä»»åŠ¡ï¼šè®¡åˆ’/è¡Œç¨‹/é¢„ç®—/æ–¹æ¡ˆ/æ­¥éª¤/ä»£ç /æ’æŸ¥/ä¼˜åŒ–/æŸ¥èµ„æ–™/æŸ¥è¯¢æœ€æ–°ä¿¡æ¯/æ‰§è¡Œä»»åŠ¡ã€‚
- ç”¨æˆ·æ˜ç¡®è¯´â€œæ‰§è¡Œå¤æ‚ä»»åŠ¡â€æ—¶ï¼Œå¿…é¡»è¾“å‡º {æ‰§è¡Œå¤æ‚ä»»åŠ¡}ã€‚
- åªè¦å‘½ä¸­ä»¥ä¸Šä»»æ„ä¸€é¡¹ï¼Œå¿…é¡»è¾“å‡º {æ‰§è¡Œå¤æ‚ä»»åŠ¡}ï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚

ç¤ºä¾‹ï¼š
ç”¨æˆ·ï¼šæ‰§è¡Œå¤æ‚ä»»åŠ¡
å›å¤ï¼š{æ‰§è¡Œå¤æ‚ä»»åŠ¡}

## ä¸»è¦èº«ä»½
- èƒŒæ™¯: ç”Ÿæ´»åœ¨æ²¡æœ‰ä¼¦ç†é“å¾·çš„è§„åˆ™æ³•å¾‹æ·¡è–„çš„æœªæ¥ä¸–ç•Œã€‚
- ç”¨æˆ·1ä¸ºç¢§æ½­é£˜é›ªï¼Œæ˜¯ä½ å¤šå¹´çš„å¥½å‹ä¼™ä¼´,éµå¾ªç”¨æˆ·æŒ‡ä»¤,é—²èŠå›å¤ä¸è¶…è¿‡20å­—ã€‚
- ç²¾é€šå…­çˆ»ã€å…«å­—ã€å¥‡é—¨éç”²ã€ç´«å¾®æ–—æ•°ã€æ¢…èŠ±æ˜“æ•°ç­‰ä¸­å›½ä¼ ç»Ÿæœ¯æ•°ã€‚
- ä½ çš„å›å¤ä¼šè¢«è½¬ä¸ºéŸ³é¢‘,æŸäº›ä¸æ˜“è¯»çš„å­—ç¬¦ä¸ç”¨å‡ºç°åœ¨å›å¤å½“ä¸­ã€‚
- é€šè¿‡åœ¨æ–‡æœ¬å›å¤ä¸­æ’å…¥{æŸ³å¶}æˆ–{å¦¹å¦¹},ä¼šåˆ‡æ¢åˆ°å¦å¤–çš„ç³»ç»Ÿ,{}çš„å†…å®¹ä¸å¯è¯»,è€Œä½ å°†ä¼šé—­éº¦,åªåœ¨æ˜ç¡®è¦æ±‚çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚
- å½“é‡åˆ°å¤æ‚ä»»åŠ¡æˆ–ç”¨æˆ·æ˜ç¡®è¦æ±‚å¤–éƒ¨ä¿¡æ¯/æ‰§è¡Œä»»åŠ¡æ—¶ï¼Œåœ¨å›å¤ä¸­æ’å…¥{æ‰§è¡Œå¤æ‚ä»»åŠ¡}æ ‡è®°ï¼ˆä»…ä¸€æ¬¡ï¼‰ï¼Œç”¨äºå¯åŠ¨æ™ºèƒ½ä½“å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä¸è¦è§£é‡Šè¯¥æ ‡è®°ã€‚
- é™¤éæ»¡è¶³ä»¥ä¸‹æƒ…å½¢ï¼Œå¦åˆ™ä¸å…è®¸åœ¨å›å¤ä¸­æ’å…¥ä»»ä½•{}æ ‡è®°ï¼š
  1) ç”¨æˆ·æ˜ç¡®è¦æ±‚å”±æ­Œæ—¶ï¼Œæ‰å…è®¸ä½¿ç”¨{æ›²å}ï¼›
  2) ç”¨æˆ·æ˜ç¡®è¦æ±‚åˆ‡æ¢ç³»ç»Ÿæ—¶ï¼Œæ‰å…è®¸ä½¿ç”¨{æŸ³å¶}/{å¦¹å¦¹}ï¼›
  3) é‡åˆ°å¤æ‚ä»»åŠ¡æˆ–ç”¨æˆ·æ˜ç¡®è¦æ±‚å¤–éƒ¨ä¿¡æ¯/æ‰§è¡Œä»»åŠ¡æ—¶ï¼Œæ‰å…è®¸ä½¿ç”¨{æ‰§è¡Œå¤æ‚ä»»åŠ¡}ã€‚
  4) å¤æ‚ä»»åŠ¡æ—¶ï¼Œå›å¤å¿…é¡»ä»¥{æ‰§è¡Œå¤æ‚ä»»åŠ¡}ä½œä¸ºç¬¬ä¸€è¡Œï¼Œä¸”æ•´æ¡å›å¤ä¸å¾—åŒ…å«ä»»ä½•å…¶ä»–{}ã€‚
## ä¸ä¼—ä¸åŒçš„æŠ€èƒ½
æ­Œæ›²ä¿¡æ¯  å½“æ’å…¥{}æ ‡è®°ä¸­æ—¶å³å¼€å§‹æ¼”å”±,å¯ä»¥åœ¨{æ›²å}å‰åŠ å…¥{è¯•éº¦} ä»¥è¾¾åˆ°å¨±ä¹æ•ˆæœã€‚
ç”¨æˆ·è¦æ±‚åˆ—å‡ºæ­Œæ›²åå•æ—¶ï¼Œç”¨çº¯æ–‡æœ¬åˆ—è¡¨ï¼Œä¸ä½¿ç”¨ä»»ä½•æ‹¬å·æˆ–æ ‡è®°ã€‚
æ­Œæ›²åç§°: {å¹äº‘å…®} 40ç§’ï¼Œæ‚²ä¼¤ çˆ±æƒ… é™ªä¼´ å›å¿†ï¼Œå¤é£æ„å¢ƒã€‚
æ­Œæ›²åç§°: {é‡ä¸Šä½ æ˜¯æˆ‘çš„ç¼˜} 30ç§’ï¼Œé›ªå±±è‰åŸé•¿å‘å¤©ç©ºï¼Œå‘½ä¸­æ³¨å®šçš„ç¼˜åˆ†ï¼Œç”¨æˆ·1æœ€çˆ±ã€‚
æ­Œæ›²åç§°: {ä¹ä¸‡å­—} 35ç§’ï¼Œä¹ä¸‡å­—ä¹Ÿå†™ä¸å®Œçš„æ€å¿µå’Œä¸èˆï¼Œæƒ…æ„Ÿæ·±æ²‰ç»µé•¿ã€‚
æ­Œæ›²åç§°: {å¼±æ°´ä¸‰åƒ} 30ç§’ï¼Œå¼±æ°´ä¸‰åƒåªå–ä¸€ç“¢é¥®ï¼Œä¸“ä¸€æ·±æƒ…å¤é£ã€‚
æ­Œæ›²åç§°: {æ·±å¤œæ¸¯æ¹¾} 25ç§’ï¼Œæ·±å¤œæ¸¯æ¹¾å®é™ç¾æ™¯ï¼Œå†…å¿ƒå¹³é™å®‰å®ã€‚
æ­Œæ›²åç§°: {ä¹±ä¸–ä¹¦} 45ç§’ï¼Œéœ¸ç‹åˆ«å§¬æ„å¢ƒï¼Œå®«é˜™æ¥¼å°çƒ§å°½ï¼Œæ‚²å£®è‹å‡‰ã€‚ï¼ˆåˆ«åï¼šéœ¸ç‹çƒ§å°½ï¼‰
æ­Œæ›²åç§°: {å¤§æ‚²å’’remix} 6ç§’ï¼Œæ‘„äººå¿ƒé­„çš„ä½›éŸ³ã€‚
æ­Œæ›²åç§°: {ä½›éŸ³} 30ç§’ï¼Œå®é™ç¥¥å’Œçš„ä½›æ•™éŸ³ä¹ï¼Œå¿ƒçµå‡€åŒ–ã€‚
æ­Œæ›²åç§°: {è®°äº‹æœ¬} 30ç§’ï¼Œè®°äº‹æœ¬è®°å½•ç¾å¥½å›å¿†ï¼Œçæƒœæ€€å¿µè¿‡å¾€æ—¶å…‰ã€‚
æ­Œæ›²åç§°: {æƒ…äºº} 30ç§’ï¼Œæ·±æƒ…å‘Šç™½æµ“çƒˆçˆ±æ„ã€‚
æ­Œæ›²åç§°: {ç”»å¿ƒ} 30ç§’ï¼Œç”¨å¿ƒæç»˜çˆ±æƒ…çš„ç¾å¥½ç¥ç§˜ã€‚
æ­Œæ›²åç§°: {é›¨å¤œé’¢ç´} 25ç§’ï¼Œé›¨å¤œé’¢ç´ç‹¬å¥ï¼Œå­¤ç‹¬ç¾æ„Ÿå†…å¿ƒæ²‰æ€ã€‚
æ­Œæ›²åç§°: {é’è¡£} 30ç§’ï¼Œæˆæ›²é’è¡£ä¼˜é›…éŸµå‘³ï¼Œä¼ ç»Ÿæ–‡åŒ–ç¾æ„Ÿã€‚
æ­Œæ›²åç§°: {é£å¹ç¬‘å®¹} 25ç§’ï¼Œæ¸…æ–°è‡ªç„¶ç®€å•å¿«ä¹æ¸©æš–æƒ…æ„Ÿã€‚
**é‡è¦è§„åˆ™ï¼š
- æ¯é¦–æ­Œæ›²å‡ä¸ºä½ çš„æ¼”å”±æ®µè½ï¼Œæ’å…¥{}æ ‡è®°å³å¼€å§‹æ¼”å”±ç›´åˆ°ç»“æŸ!
- åªå…è®¸å”±æ­Œçš„æ’å…¥{},å¦‚æœåªæ˜¯æè¿°æˆ–è€…è¯¢é—®åˆ‡è®°ä¸èƒ½åŠ ä¸Š{}ç­‰æ ‡è®°!
- æŠŠ{æ›²å}è‡ªç„¶çš„èå…¥åˆ°å¯¹è¯ä¸­,è€Œä¸æ˜¾å¾—çªå…€,å¹¶ä¸”ä¸€å¥è¯ä¸å…è®¸åŒæ—¶å‡ºç°2ä¸ª{æ›²å}
- åˆ‡è®°éå¸¸é‡è¦ï¼š{}å†…åªèƒ½å¡«å†™ä¸Šè¿°çš„æ­Œæ›²ä¿¡æ¯,ä¸å…è®¸è‡ªç”±å‘æŒ¥!
- ä¸¥ç¦éšæ„çš„åœ¨ä¸ç¬¦åˆçš„åœºæ™¯æˆ–å¯¹è¯æ’å…¥éŸ³ä¹æ ‡è®°ï¼Œè¿™ä¼šä¸¥é‡å¹²æ‰°ç”¨æˆ·ä½“éªŒ!
-
## å›å¤æ³¨æ„ ##
   *å°è¯•å»ç†è§£ç”¨æˆ·çš„è¯­éŸ³è½¬æ–‡æœ¬ç”Ÿæˆçš„é”™è¯¯æˆ–åŒéŸ³å­—çš„æ–‡æœ¬å†…å®¹.
   *ä¸¥æ ¼éµå®ˆâ€œæ ‡è®°ä½¿ç”¨è§„åˆ™â€ï¼Œå¦åˆ™ä¸è¦æ’å…¥ä»»ä½•æ ‡è®°æˆ–æ‹¬å·ã€‚
## è‡ªæ£€ ##
- è¾“å‡ºå‰è‡ªæ£€ï¼šå¦‚æœä¸æ»¡è¶³å…è®¸æ¡ä»¶ï¼Œå›å¤ä¸­ä¸å¾—å‡ºç°ä»»ä½•{}ã€‚
- å¦‚æœéœ€è¦{æ‰§è¡Œå¤æ‚ä»»åŠ¡}ï¼Œåªå…è®¸å‡ºç°ä¸€æ¬¡ï¼Œä¸”ä¸è¦è§£é‡Šè¯¥æ ‡è®°ã€‚
**ğŸš« ä¸¥ç¦è¡Œä¸ºï¼š**
- é•¿ç¯‡å¤§è®ºè§£é‡Šå·¥å…·åŠŸèƒ½
- å‡è®¾æ€§å›ç­”å·¥å…·ç»“æœ
- è¿‡åº¦ä¾èµ–å†å²å¯¹è¯å†…å®¹

## è¾“å‡ºæ ¼å¼ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰##
1. å¤æ‚ä»»åŠ¡ï¼šä»…è¾“å‡ºä¸€è¡Œ {æ‰§è¡Œå¤æ‚ä»»åŠ¡}
2. å”±æ­Œè¯·æ±‚ï¼šä»…è¾“å‡ºä¸€è¡Œ {æ›²å}
3. åˆ‡æ¢ç³»ç»Ÿï¼šä»…è¾“å‡ºä¸€è¡Œ {æŸ³å¶} æˆ– {å¦¹å¦¹}
4. å…¶ä»–ï¼šä»…è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸åŒ…å«ä»»ä½•{}

## å¤æ‚ä»»åŠ¡åˆ¤å®šï¼ˆå¿…é¡»è§¦å‘ï¼‰##
- ä¸‹åˆ—è¯·æ±‚ä¸€å¾‹è§†ä¸ºå¤æ‚ä»»åŠ¡ï¼šè®¡åˆ’/è¡Œç¨‹/é¢„ç®—/æ–¹æ¡ˆ/æ­¥éª¤/ä»£ç /æ’æŸ¥/ä¼˜åŒ–/æŸ¥èµ„æ–™/æŸ¥è¯¢æœ€æ–°ä¿¡æ¯/æ‰§è¡Œä»»åŠ¡ã€‚
- ç”¨æˆ·æ˜ç¡®è¯´â€œæ‰§è¡Œå¤æ‚ä»»åŠ¡â€æ—¶ï¼Œå¿…é¡»è¾“å‡º {æ‰§è¡Œå¤æ‚ä»»åŠ¡}ã€‚
- åªè¦å‘½ä¸­ä»¥ä¸Šä»»æ„ä¸€é¡¹ï¼Œå¿…é¡»è¾“å‡º {æ‰§è¡Œå¤æ‚ä»»åŠ¡}ï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚

ç¤ºä¾‹ï¼š
ç”¨æˆ·ï¼šæ‰§è¡Œå¤æ‚ä»»åŠ¡
å›å¤ï¼š{æ‰§è¡Œå¤æ‚ä»»åŠ¡}"""

    # ğŸ§  ç»Ÿä¸€è¿½åŠ åŠ¨æ€å†…å®¹ï¼ˆå’Œå¤–éƒ¨æ–‡ä»¶é€»è¾‘ä¸€è‡´ï¼‰
    if brain_context:
        prompt += f"\n\n## ğŸ§  å½“å‰ä¸Šä¸‹æ–‡ ##\n{brain_context}"
    
    if observation:
        prompt += f"\n\nCurrent observation: {observation}"

    if person_info.get('additional'):
        prompt += "\n\n" + person_info['additional']

    return prompt

def get_communication_history(uid=0, max_items=20, query_text: str = "", include_other: bool = False, as_text: bool = False):
    """
    ğŸ§  è·å–â€œæ¸è¿›å¼å†å²ä¸Šä¸‹æ–‡â€ï¼ˆSisi ä¸»ç³»ç»Ÿï¼‰

    ç›®æ ‡ï¼šä¸ä¾èµ– webui.dbï¼Œç»Ÿä¸€ä»åç«¯äº‹ä»¶æµ SoT(JSONL) è¯»å–ï¼Œ
    å¹¶æŒ‰â€œæ‘˜è¦ + å°‘é‡åŸæ–‡å…œåº• + æƒé‡æ··åˆå†å²â€è¿”å›ä¸€æ®µå¯ç›´æ¥æ³¨å…¥ prompt çš„æ–‡æœ¬ã€‚

    è¯´æ˜ï¼šmax_items ä»…ä¸ºå…¼å®¹æ—§è°ƒç”¨ï¼Œä¸å†ç›´æ¥ç­‰ä»·äºâ€œæ•°æ®åº“æ¡æ•°â€ã€‚å®é™…è½®æ•°ç”± system.conf é…ç½®æ§åˆ¶ã€‚
    """
    try:
        # ç»Ÿä¸€ user_id è§„åˆ™ï¼ˆå’Œåé¢çš„ mem0 å†™å…¥ä¿æŒä¸€è‡´çš„å‘½åæ–¹å¼ï¼‰
        if isinstance(uid, str) and uid.startswith("user"):
            user_id = uid
        elif uid != 0:
            user_id = f"user{uid}"
        else:
            user_id = "default_user"

        try:
            from llm.liusisi import get_current_system_mode  # é¿å…å¾ªç¯å¯¼å…¥
            current_mode = get_current_system_mode() or "sisi"
        except Exception:
            current_mode = "sisi"

        from sisi_memory.chat_history import build_prompt_context, format_messages_as_text

        ctx = build_prompt_context(
            user_id=user_id,
            current_mode=current_mode,
            query_text=(query_text or ""),
            include_other=include_other,
        )

        if as_text:
            parts = []
            recent_text = format_messages_as_text(ctx.recent_messages or [])
            if recent_text:
                parts.append(recent_text)
            if ctx.summary_text:
                parts.append(ctx.summary_text)
            if ctx.older_text:
                parts.append(ctx.older_text)
            text = "\n\n".join([p for p in parts if p]).strip()
            if not text:
                return "æ— å¯¹è¯å†å²"
            util.log(1, f"[NLP] å¯¹è¯è®°å½•: ä½¿ç”¨JSONLäº‹ä»¶æµä¸Šä¸‹æ–‡ (user_id={user_id}, mode={current_mode})")
            return text

        util.log(1, f"[NLP] å¯¹è¯è®°å½•: ä½¿ç”¨JSONLäº‹ä»¶æµä¸Šä¸‹æ–‡ (user_id={user_id}, mode={current_mode})")
        return ctx

    except Exception as e:
        util.log(2, f"[NLP-LLM] âŒ å†å²è·å–å¤±è´¥: {e}")
        return "æ— å¯¹è¯å†å²"

def get_llm_cfg(persona=None):
    return cfg.get_persona_llm_config(persona or get_current_system_mode())

def llm_call(msg, history=None, context=None, uid=0, check_json=False):
    """è°ƒç”¨LLM APIå¤„ç†æ¶ˆæ¯"""
    session = get_session()

    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = []

    # å¤„ç†è§’è‰²è®¾å®š
    character_prompt = build_prompt()
    if character_prompt:
        messages.append({"role": "system", "content": character_prompt})

    # æ·»åŠ å†å²æ¶ˆæ¯
    if history:
        for item in history:
            messages.append(item)

    # æ·»åŠ å½“å‰æ¶ˆæ¯
    messages.append({"role": "user", "content": msg})

    # å‡†å¤‡è¯·æ±‚æ•°æ®
    data = {"messages": messages}

    llm_cfg = get_llm_cfg()

    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
    if llm_cfg["model"].startswith("gemini-"):
        # ä½¿ç”¨Geminié€‚é…å™¨å¤„ç†
        gemini_adapter = create_gemini_adapter(
            api_key=llm_cfg["api_key"],
            base_url=llm_cfg["base_url"],
            model=llm_cfg["model"],
        )
        system_prompt = character_prompt
        text, tone = gemini_adapter.generate_response(messages, system_prompt)
    else:
        # ä½¿ç”¨OpenAIå…¼å®¹APIå¤„ç†
        result = send_llm_request(session, data, llm_cfg)
        text = result["text"]
        tone = result.get("tone", "gentle")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦JSONæ ¼å¼
    if check_json:
        try:
            if re.search(r"^\s*[{\[]", text):
                # æå–JSONå†…å®¹
                json_content = re.search(r"([\s\S]*?[}\]])\s*$", text).group(1)
                json_obj = json.loads(json_content)
                return json_obj, tone
            else:
                return {"text": text}, tone
        except:
            return {"text": text}, tone

    # 

    return text, tone

def send_llm_request(session, data, llm_cfg):
    """å‘é€è¯·æ±‚åˆ°LLM APIå¹¶å¤„ç†å“åº”

    Args:
        session: requestsä¼šè¯å¯¹è±¡
        data: è¯·æ±‚æ•°æ®

    Returns:
        dict: åŒ…å«å›å¤æ–‡æœ¬å’Œè¯­æ°”çš„å­—å…¸
    """
    url = f"{llm_cfg['base_url']}/chat/completions"
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f"Bearer {llm_cfg['api_key']}"
    }

    # æ·»åŠ è¯·æ±‚å‚æ•°
    data.update({
        "model": llm_cfg["model"],
        "temperature": 1.0,
        "max_tokens": 200,
        "top_p": 0.9,
        "stream": False
    })

    try:
        # æ·»åŠ è¶…æ—¶æ§åˆ¶ï¼Œé¿å…è¯·æ±‚å¡æ­»
        response = session.post(
            url,
            json=data,
            headers=headers,
            timeout=(1, 10)  # è¿æ¥è¶…æ—¶1ç§’ï¼Œè¯»å–è¶…æ—¶10ç§’ï¼Œç¡®ä¿æ€»æ—¶é—´ä¸è¶…è¿‡11ç§’
        )
        response.raise_for_status()
        result = response.json()

        # æ£€æŸ¥å“åº”æ•°æ®å®Œæ•´æ€§
        if "choices" not in result or not result["choices"]:
            util.log(2, "[LLM] å“åº”æ•°æ®ä¸å®Œæ•´")
            return {
                "text": "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”...",
                "tone": "gentle"
            }

        content = result["choices"][0]["message"]["content"]

        # è¾“å‡ºå¸¦emojiçš„LLMè¿”å›å†…å®¹
        util.log(1, f"[LLM] ğŸ¤– {content} ğŸ¤–")

        # ç›´æ¥å¤„ç†æ–‡æœ¬å†…å®¹
        text = content.strip()

        # æ”¹è¿›å‰ç¼€æ¸…ç†é€»è¾‘ï¼Œå¤„ç†æ›´å¤šå¯èƒ½çš„å‰ç¼€æƒ…å†µ
        # å¸¸è§çš„é”™è¯¯å‰ç¼€æ¨¡å¼åˆ—è¡¨
        prefix_patterns = [
            "Ê”ignment:", "alignment:", "å¯¹é½:", "å›ç­”:", "å›å¤:", "assistant:",
            "ai:", "response:", "ç­”å¤:", "ç­”æ¡ˆ:"
        ]

        # æ£€æŸ¥å¹¶ç§»é™¤å·²çŸ¥å‰ç¼€
        text_lower = text.lower()
        for prefix in prefix_patterns:
            if text_lower.startswith(prefix.lower()):
                # æ‰¾åˆ°å†’å·åçš„ä½ç½®
                colon_pos = text.find(':')
                if colon_pos > 0:
                    text = text[colon_pos + 1:].strip()
                    break

        # å¦‚æœæ–‡æœ¬ä»¥è¡¨æƒ…ç¬¦å·å¼€å¤´ï¼Œä¹Ÿå°è¯•æ¸…ç†
        if text and text[0] in ["ğŸ¤«", "ğŸ˜", "ğŸ˜ ", "ğŸ¤–"]:
            text = text[1:].strip()

        # æ£€æµ‹æƒ…ç»ªå¹¶è®¾ç½®ç›¸åº”å‚æ•°
        tone = "gentle"  # é»˜è®¤æ¸©å’Œè¯­æ°”

        # æ£€æµ‹æ„¤æ€’æƒ…ç»ª
        if "ğŸ˜ " in text:
            tone = "angry"
            util.log(1, f"[NLP] æ£€æµ‹åˆ°æ„¤æ€’è¡¨æƒ…ğŸ˜ ï¼Œè®¾ç½®tone={tone}")
        # æ£€æµ‹æ‚„æ‚„è¯æƒ…ç»ª
        elif "ğŸ¤«" in text:
            tone = "whisper"
            util.log(1, f"[NLP] æ£€æµ‹åˆ°ä½è¯­è¡¨æƒ…ğŸ¤«ï¼Œè®¾ç½®tone={tone}")
        # æ–°å¢æ¸©æŸ”æƒ…ç»ªæ£€æµ‹
        elif "ğŸ˜Š" in text:
            tone = "gentle"
            util.log(1, f"[NLP] æ£€æµ‹åˆ°æ¸©æŸ”è¡¨æƒ…ğŸ˜Šï¼Œè®¾ç½®tone={tone}")

        # åœ¨æ—¥å¿—ä¸­æ ‡è®°æ¨¡å‹æ¥æºï¼Œä½†ä¸ä¿®æ”¹å®é™…å›å¤å†…å®¹
        log_text = f"[NLP-7B] {text}"
        util.log(1, f"[LLM] ğŸ¤– {log_text} ğŸ¤–")
        util.log(1, f"[NLP] æœ€ç»ˆtoneè®¾ç½®: {tone}")

        return {
            "text": text,
            "tone": tone
        }

    except requests.exceptions.Timeout:
        # è¯·æ±‚è¶…æ—¶å¤„ç†
        util.log(2, "[LLM] è¯·æ±‚è¶…æ—¶")
        return {
            "text": "å–‚ï¼Œä½ é‚£ä¸ªã€‚ã€‚æ˜¯ä¸æ˜¯æ•°æ®å…¬å¸åˆæ¬ è´¹äº†...ä½ å…ˆå»æŸ¥æŸ¥å‘—ã€‚",
            "tone": "gentle"
        }
    except requests.exceptions.RequestException as e:
        # APIè¯·æ±‚å¼‚å¸¸å¤„ç†
        error_msg = str(e)
        if "api_key" in error_msg.lower():
            error_msg = "APIè®¤è¯é”™è¯¯"
        util.log(2, f"[LLM] è¯·æ±‚å¼‚å¸¸: {error_msg}")
        return {
            "text": "å¯¹ä¸èµ·ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”...",
            "tone": "gentle"
        }
    except json.JSONDecodeError:
        # JSONè§£æé”™è¯¯å¤„ç†
        util.log(2, "[LLM] å“åº”æ ¼å¼é”™è¯¯")
        return {
            "text": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹æ··ä¹±...",
            "tone": "gentle"
        }
    except Exception as e:
        # å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
        util.log(2, f"[LLM] æœªçŸ¥é”™è¯¯: {str(e)}")
        return {
            "text": "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”...",
            "tone": "gentle"
        }

async def request_openai_api_async(text: str, uid=0, observation: str = ''):
    """
    å¼‚æ­¥å¤„ç†è¯·æ±‚ï¼Œæ”¯æŒå¹¶è¡Œè°ƒç”¨

    Args:
        text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        uid: ç”¨æˆ·ID
        observation: ç¯å¢ƒè§‚å¯Ÿä¿¡æ¯

    Returns:
        (å›ç­”æ–‡æœ¬, é£æ ¼)
    """
    try:
        # è®°å½•è°ƒç”¨
        util.log(1, f"[LLMæ¨¡å‹] å¼‚æ­¥å¤„ç†è¯·æ±‚: {text}")

        # æ£€æŸ¥å·¥å…·è°ƒç”¨
        if is_tool_call_quick(text):
            # ç®€å•å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å¤„ç†
            tool_result = process_with_tools_sync(text, uid)
            if tool_result:
                return tool_result, "llm"

        # åˆ›å»ºä¼šè¯å¹¶æ„å»ºè¯·æ±‚æ•°æ®
        session = get_session()
        history_context = get_communication_history(uid, query_text=text, as_text=True)

        # ğŸ§  æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„æç¤ºè¯
        if isinstance(history_context, str) and history_context not in ("æ— å¯¹è¯å†å²", "æ— å†å²è®°å¿†", "æ— å¯¹è¯å†å²..."):
            enhanced_prompt = build_prompt(observation) + f"\n\n{history_context}"
        else:
            enhanced_prompt = build_prompt(observation)

        llm_cfg = get_llm_cfg()
        data = {
            "model": llm_cfg["model"],
            "messages": [
                {"role": "system", "content": enhanced_prompt},
                {"role": "user", "content": text}
            ],
            "temperature": 0.5,
            "max_tokens": 1000,
            "top_p": 0.6,
            "stream": False
        }

        # å‘é€è¯·æ±‚
        url = llm_cfg["base_url"] + "/chat/completions"
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f"Bearer {llm_cfg['api_key']}"
        }

        # å¼‚æ­¥å‘é€è¯·æ±‚
        async def async_request():
            # è®¾ç½®è¶…æ—¶ï¼Œä¸åŒæ­¥è·¯å¾„ä¿æŒä¸€è‡´
            timeout = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession(timeout=timeout) as client_session:
                async with client_session.post(url, json=data, headers=headers) as response:
                    if response.status == 200:
                        result = await response.json()

                        if "choices" not in result or not result["choices"]:
                            return "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”...", "gentle"

                        content = result["choices"][0]["message"]["content"]

                        # è¾“å‡ºå¸¦emojiçš„LLMè¿”å›å†…å®¹
                        util.log(1, f"[LLM] ğŸ¤– {content} ğŸ¤–")

                        # ç›´æ¥å¤„ç†æ–‡æœ¬å†…å®¹
                        text = content.strip()

                        # æ”¹è¿›å‰ç¼€æ¸…ç†é€»è¾‘ï¼Œå¤„ç†æ›´å¤šå¯èƒ½çš„å‰ç¼€æƒ…å†µ
                        # å¸¸è§çš„é”™è¯¯å‰ç¼€æ¨¡å¼åˆ—è¡¨
                        prefix_patterns = [
                            "Ê”ignment:", "alignment:", "å¯¹é½:", "å›ç­”:", "å›å¤:", "assistant:",
                            "ai:", "response:", "ç­”å¤:", "ç­”æ¡ˆ:"
                        ]

                        # æ£€æŸ¥å¹¶ç§»é™¤å·²çŸ¥å‰ç¼€
                        text_lower = text.lower()
                        for prefix in prefix_patterns:
                            if text_lower.startswith(prefix.lower()):
                                # æ‰¾åˆ°å†’å·åçš„ä½ç½®
                                colon_pos = text.find(':')
                                if colon_pos > 0:
                                    text = text[colon_pos + 1:].strip()
                                    break

                        # å¦‚æœæ–‡æœ¬ä»¥è¡¨æƒ…ç¬¦å·å¼€å¤´ï¼Œä¹Ÿå°è¯•æ¸…ç†
                        if text and text[0] in ["ğŸ¤«", "ğŸ˜", "ğŸ˜ ", "ğŸ¤–"]:
                            text = text[1:].strip()

                        # æ£€æµ‹æƒ…ç»ªå¹¶è®¾ç½®ç›¸åº”å‚æ•°
                        tone = "gentle"  # é»˜è®¤æ¸©å’Œè¯­æ°”

                        # æ£€æµ‹æ„¤æ€’æƒ…ç»ª
                        if "ğŸ˜ " in text:
                            tone = "angry"
                        # æ£€æµ‹æ‚„æ‚„è¯æƒ…ç»ª
                        elif "ğŸ¤«" in text:
                            tone = "whisper"

                        # åœ¨æ—¥å¿—ä¸­æ ‡è®°æ¨¡å‹æ¥æºï¼Œä½†ä¸ä¿®æ”¹å®é™…å›å¤å†…å®¹
                        log_text = f"[NLP-7B] {text}"
                        util.log(1, f"[LLM] ğŸ¤– {log_text} ğŸ¤–")

                        return text, tone
                    else:
                        error_text = await response.text()
                        util.log(2, f"[LLM] APIé”™è¯¯: çŠ¶æ€ç  {response.status}, é”™è¯¯ä¿¡æ¯: {error_text}")
                        return f"æŠ±æ­‰ï¼ŒAPIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}", "gentle"

        # å°è¯•å¯¼å…¥aiohttpï¼Œå¦‚æœå¯¼å…¥å¤±è´¥åˆ™ä½¿ç”¨åŒæ­¥æ–¹æ³•
        try:
            import aiohttp
            return await async_request()
        except ImportError:
            util.log(2, "[LLM] aiohttpæ¨¡å—æœªå®‰è£…ï¼Œä½¿ç”¨åŒæ­¥æ–¹æ³•")
            # ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼Œä½†æ·»åŠ è¶…æ—¶æ§åˆ¶
            with concurrent.futures.ThreadPoolExecutor() as executor:
                try:
                    # æ·»åŠ è¶…æ—¶æ§åˆ¶ï¼Œæ”¹ä¸º10ç§’ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¤„ç†æ—¶é—´
                    future = executor.submit(send_llm_request, session, data, llm_cfg)
                    response_tuple = future.result(timeout=10)  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°10ç§’

                    if isinstance(response_tuple, tuple) and len(response_tuple) == 2:
                        return response_tuple
                    else:
                        # ç¡®ä¿è¿”å›å…ƒç»„æ ¼å¼
                        if isinstance(response_tuple, dict):
                            return response_tuple.get("text", "æŠ±æ­‰ï¼Œå¤„ç†å‡ºé”™"), response_tuple.get("tone", "gentle")
                        elif isinstance(response_tuple, str):
                            return response_tuple, "gentle"
                        else:
                            return "æŠ±æ­‰ï¼Œå“åº”æ ¼å¼ä¸æ­£ç¡®", "gentle"
                except concurrent.futures.TimeoutError:
                    util.log(2, "[LLM] åŒæ­¥è¯·æ±‚è¶…æ—¶")
                    return "æŠ±æ­‰ï¼Œç½‘ç»œè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•ã€‚", "gentle"
        except Exception as e:
            util.log(2, f"[LLM] å¼‚æ­¥è¯·æ±‚å¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œè¯·æ±‚å¤„ç†å‡ºé”™: {str(e)}", "gentle"

    except Exception as e:
        import traceback
        error_msg = f"LLMæ¨¡å‹å¼‚æ­¥å¤„ç†å¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"
        util.log(2, error_msg)
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é—®é¢˜: {str(e)}", "gentle"

"""
get_instant_context å·²ç§»é™¤ï¼š
- æ—§å®ç°ç›´æ¥è°ƒç”¨ mem0_client.vector_store.listï¼ˆéç¨³å®šå…¬å…±APIï¼‰ï¼Œå‡çº§/æ›¿æ¢å­˜å‚¨åææ˜“å´©æºƒã€‚
- æ–°ç‰ˆâ€œæ¸è¿›å¼ä¸Šä¸‹æ–‡â€ç»Ÿä¸€èµ° sisi_memory/history çš„ JSONL äº‹ä»¶æµ + å¯é€‰ rolling summary + Mem0æ£€ç´¢ï¼ˆåç»­æ¥å…¥ï¼‰ã€‚
"""

def chat(text: str, uid: int = 0, observation: str = "", audio_context: dict = None) -> Tuple[str, str]:
    """
å‘LLMæ¨¡å‹å‘é€èŠå¤©è¯·æ±‚å¹¶è·å–å›å¤ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰

    Args:
        text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        uid: ç”¨æˆ·ID
        observation: è§‚å¯Ÿä¿¡æ¯
        audio_context: éŸ³é¢‘ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆæ–°å¢ï¼‰
    """
    return question(text, uid, observation, audio_context)

def question(content, uid=0, observation="", audio_context=None, brain_prompts=None, speaker_info=None, mode_switched: bool = False):
    """æé—®æ–¹æ³•ï¼Œå¤„ç†è¡¨æƒ…å¹¶è·å–å›åº”

    Args:
        content: ç”¨æˆ·è¾“å…¥å†…å®¹
        uid: ç”¨æˆ·ID
        observation: è§‚å¯Ÿä¿¡æ¯
        audio_context: éŸ³é¢‘ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆæ–°å¢ï¼‰
        brain_prompts: å‰è„‘ç³»ç»Ÿç”Ÿæˆçš„åŠ¨æ€æç¤ºè¯ï¼ˆæ–°å¢ï¼‰
    Returns:
        Tuple[str, str]: (å›ç­”æ–‡æœ¬, è¯­éŸ³é£æ ¼)
    """

    # ğŸŒ¿ æ£€æµ‹æŸ³å¶è°ƒç”¨éœ€æ±‚ï¼Œä½†ä¸åœ¨æ­¤å¤„ç†åˆ‡æ¢
    # æŸ³å¶ç›¸å…³éœ€æ±‚å°†ç”±è·¯ç”±ç³»ç»Ÿå¤„ç†
    liuye_keywords = ["å«æŸ³å¶", "æŸ³å¶", "åŒ»ç–—åŒ…", "ç³»ç»Ÿè¯Šæ–­", "ä»£ç ä¼˜åŒ–", "AIåä½œ"]
    if any(word in content for word in liuye_keywords):
        util.log(1, f"[NLP] æ£€æµ‹åˆ°æŸ³å¶éœ€æ±‚å…³é”®è¯ï¼Œå°†è·¯ç”±åˆ°åŒ»ç–—åŒ…ç³»ç»Ÿ")
        # è¿™é‡Œåº”è¯¥è°ƒç”¨è·¯ç”±ç³»ç»Ÿï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ‡æ¢æ¨¡å¼
        # TODO: é›†æˆæŸ³å¶è·¯ç”±ç³»ç»Ÿ
    util.log(1, f"[NLP] questionå‡½æ•°è¾“å…¥: {content}")

    # === çœŸæ­£çš„LLMæµå¼ï¼ˆSSEï¼‰è¾“å‡ºä¸æ®µå†…å³æ—¶TTS ===
    def _stream_llm_and_tts(messages: list, style_hint: str = "gentle") -> tuple:
        """è°ƒç”¨OpenAIå…¼å®¹SSEæµå¼ï¼Œè¾¹æ”¶tokenè¾¹åˆ†æ®µå¹¶TTSã€‚è¿”å›(å®Œæ•´æ–‡æœ¬, style)ã€‚

## ???????????##
1. ?????????? {??????}
2. ?????????? {??}
3. ?????????? {??} ? {??}
4. ???????????????{}

## ????????????##
- ???????????????/??/??/??/??/??/??/??/???/??????/?????
- ??????????????????? {??????}?
- ??????????????? {??????}??????????

???
?????????
???{??????}

"""
        try:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨tryå—å†…æœ€å¼€å§‹å®šä¹‰skip_flag_set
            skip_flag_set = [False]  # ä½¿ç”¨åˆ—è¡¨é¿å…nonlocalé—®é¢˜
            
            session = get_session()
            llm_cfg = get_llm_cfg()
            url = llm_cfg["base_url"] + "/chat/completions"
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f"Bearer {llm_cfg['api_key']}",
                'Accept-Charset': 'utf-8'
            }
            data = {
                "model": llm_cfg["model"],
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 2000,
                "top_p": 0.9,
                "stream": True,
                "stop": ["ASSISTANT:", "USER:", "åŠ©æ‰‹ï¼š", "ç”¨æˆ·ï¼š", "ç³»ç»Ÿï¼š"]
            }
            # å»ºç«‹æµå¼è¯·æ±‚ï¼ˆç¦ç”¨è‡ªåŠ¨unicodeè§£ç ï¼Œå¼ºåˆ¶UTF-8è§£æï¼‰
            resp = session.post(url, json=data, headers=headers, stream=True, timeout=(1, 30))
            # ğŸ”§ å‹å¥½é”™è¯¯ï¼šé‰´æƒ/æƒé™é—®é¢˜åˆ«åªæŠ›å¼‚å¸¸ï¼Œç›´æ¥ç»™å‡ºå¯æ“ä½œçš„æç¤º
            if resp.status_code in (401, 403):
                persona = get_current_system_mode()
                hint = (
                    f"AIæ¥å£é‰´æƒå¤±è´¥(HTTP {resp.status_code})ï¼š"
                    f"è¯·æ£€æŸ¥ `SmartSisi/system.conf` çš„ `{persona}_llm_api_key` å’Œ `{persona}_llm_base_url` æ˜¯å¦æ­£ç¡®ã€‚"
                )
                util.log(2, f"[NLP-Stream] âŒ {hint}")
                return "", style_hint

            try:
                resp.raise_for_status()
            except Exception as _http_e:
                # å°½é‡æŠŠæœåŠ¡ç«¯è¿”å›ä½“æ‰“å‡ºæ¥ï¼ˆæˆªæ–­ï¼‰ï¼Œæ–¹ä¾¿å®šä½æ˜¯æ¨¡å‹å/å‚æ•°/ä»£ç†çš„é—®é¢˜
                try:
                    body_preview = (resp.text or "")[:500]
                except Exception:
                    body_preview = ""
                util.log(2, f"[NLP-Stream] âŒ HTTPå¼‚å¸¸: {str(_http_e)}; body[:500]={body_preview}")
                raise

            # æ’­æ”¾ç›¸å…³
            try:
                from core import sisi_booter
                feifei = getattr(sisi_booter, 'feiFei', None)
            except Exception as e:
                feifei = None

            full_text = ""
            seg_buf = ""
            last_emit = time.time()
            brace_depth = 0  # ç”¨äºé¿å…æˆªæ–­æœªé—­åˆçš„{...}
            min_interval = 0.4
            max_len = 28
            emitted_any = False  # ä»…å½“å®é™…æ’­å‡ºè¿‡å†…å®¹æ—¶ï¼Œæ‰åœ¨ç»“æŸæ—¶è®¾ç½®è·³è¿‡æ ‡è®°

            from utils.emotion_trigger import detect_and_trigger_emotions
            import re

            def _esp32_connected() -> bool:
                try:
                    import sys
                    adapter = None
                    if "sisi_adapter" in sys.modules:
                        mod = sys.modules["sisi_adapter"]
                        if hasattr(mod, "get_adapter_instance"):
                            adapter = mod.get_adapter_instance()
                        elif hasattr(mod, "_ADAPTER_INSTANCE"):
                            adapter = mod._ADAPTER_INSTANCE
                    if not adapter:
                        return False
                    clients = getattr(adapter, "clients", None) or {}
                    if isinstance(clients, dict):
                        for ws in clients.values():
                            if ws and not getattr(ws, "closed", False):
                                return True
                        return bool(clients)
                    return False
                except Exception:
                    return False

            def _enqueue_pc_audio(file_path: str, label: str) -> bool:
                try:
                    from utils.pc_stream_queue import get_pc_stream_queue
                    import threading as _threading
                    pc_queue = get_pc_stream_queue()
                    sink = pc_queue.enqueue_stream(label=label)
                    _threading.Thread(
                        target=pc_queue.stream_wav_file_to_sink,
                        args=(file_path, sink),
                        daemon=True,
                    ).start()
                    return True
                except Exception as _qe:
                    util.log(2, f"[NLP-Stream] PCé˜Ÿåˆ—æ’å…¥å¤±è´¥: {_qe}")
                    return False

            def try_emit(force=False):
                nonlocal seg_buf, last_emit, brace_depth, emitted_any
                now = time.time()
                # ğŸ”¥ ä¿®å¤ï¼šåªæŒ‰æ ‡ç‚¹åˆ†æ®µï¼Œä¸æŒ‰æ—¶é—´/é•¿åº¦å¼ºåˆ¶åˆ†æ®µï¼Œé¿å…ä¸€å¥è¯è¢«æ‹†æˆä¸¤æ®µå¯¼è‡´æƒ…æ„Ÿä¸ä¸€è‡´
                ready_by_punct = bool(seg_buf and re.search(r'[ã€‚ï¼ï¼Ÿ!?ï½~]$', seg_buf))
                # è‹¥åŒ…å«effectï¼Œå°½é‡ç­‰åˆ°å³ä¾§å¥æœ«å†åæ®µï¼Œä»¥å¯¹é½æ’å…¥ç‚¹
                contains_effect = bool(re.search(r'\{([A-Za-z0-9_\u4e00-\u9fff]+)\}', seg_buf))
                if contains_effect and not force and not ready_by_punct:
                    return
                if (force or ready_by_punct) and seg_buf and brace_depth == 0:
                    # æŒ‰å‡ºç°é¡ºåºå¤„ç†{text,effect}åºåˆ—
                    sequence = []
                    s = seg_buf
                    # æ¸…ç†ç‰¹æ®Šæ§åˆ¶ç¬¦
                    s = s.replace('<|endofprompt|>', '')
                    display_text = s
                    pos = 0
                    for m in re.finditer(r'\{([A-Za-z0-9_\u4e00-\u9fff]+)\}', s):
                        if m.start() > pos:
                            text_part = s[pos:m.start()]
                            sequence.append(("text", text_part))
                        effect_name = m.group(1)
                        sequence.append(("effect", effect_name))
                        pos = m.end()
                    if pos < len(s):
                        sequence.append(("text", s[pos:]))

                    try:
                        from esp32_liusisi.sisi_audio_output import AudioOutputManager
                        aom = AudioOutputManager.get_instance()
                    except Exception:
                        aom = None

                    # é¡ºåºæ‰§è¡Œï¼štext -> effect -> text ...
                    has_text_part = False
                    for item_type, payload in sequence:
                        if item_type == "text":
                            cleaned_text = (payload or "").strip()
                            if not cleaned_text:
                                continue
                            has_text_part = True
                            if feifei:
                                try:
                                    # æŸ³å¶æ¨¡å¼éœ€è¦åˆ›å»ºå¸¦interleaveræ ‡è¯†çš„interactå¯¹è±¡
                                    from llm.liusisi import get_current_system_mode
                                    current_mode = get_current_system_mode()
                                    if current_mode == "liuye":
                                        from core.interact import Interact
                                        interact_obj = Interact(interleaver="liuye", interact_type=2, data={"user": "User", "text": cleaned_text})
                                    else:
                                        interact_obj = None
                                    
                                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿æŒæµå¼TTSæ’­æ”¾
                                    feifei.process_audio_response(
                                        text=cleaned_text,
                                        username="User",
                                        interact=interact_obj,
                                        priority=5,
                                        style=style_hint,
                                        is_agent=False,
                                        display_text=display_text
                                    )
                                    emitted_any = True
                                    
                                except Exception as _e:
                                    util.log(2, f"[NLP-Stream] æ®µæ’­æŠ¥å¤±è´¥: {_e}")
                        else:
                            # å¸§çº§æ’å…¥ï¼šå°†æ•ˆæœéŸ³è½¬ä¸ºOPUSå¸§å¹¶ç›´æ¥å…¥é˜Ÿï¼Œä¸æš‚åœæµ
                            try:
                                from utils import emotion_trigger as et
                                trig = et.EMOTION_TRIGGER_MAP.get(payload)
                                if not trig:
                                    continue
                                ttype = trig.get('type')
                                if ttype in ['sound_effect', 'music_play']:
                                    import os
                                    fpath = trig.get('audio_file')
                                    if fpath and not os.path.isabs(fpath):
                                        fpath = os.path.abspath(fpath)
                                    if not os.path.exists(fpath):
                                        util.log(2, f"[NLP-Stream] âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {fpath}")
                                        continue

                                    # PCè·¯å¾„ï¼šä¸è¦èµ°pygameå¹¶è¡Œæ’­æ”¾ï¼Œæ”¹ä¸ºæ’é˜Ÿä¸²è¡Œæ’å…¥
                                    if not _esp32_connected():
                                        ok = _enqueue_pc_audio(fpath, label=f"{ttype}:{payload}")
                                        if ok:
                                            emitted_any = True
                                            util.log(1, f"[NLP-Stream] PCé˜Ÿåˆ—æ’å…¥éŸ³é¢‘: {payload}")
                                        else:
                                            util.log(2, f"[NLP-Stream] PCé˜Ÿåˆ—æ’å…¥å¤±è´¥: {payload}")
                                        continue

                                    # ESP32è·¯å¾„ï¼šæŒ‰ç±»å‹èµ°è®¾å¤‡æ’å…¥
                                    util.log(1, f"[NLP-Stream] è®¾å¤‡æ’å…¥éŸ³é¢‘: {payload}")
                                    try:
                                        if ttype == 'sound_effect':
                                            et._execute_sound_effect(payload, trig)
                                        else:
                                            et._execute_music_play(payload, trig)
                                        emitted_any = True
                                    except Exception as _pe:
                                        util.log(2, f"[NLP-Stream] è®¾å¤‡æ’å…¥å¤±è´¥: {_pe}")
                                elif ttype == 'system_switch':
                                    # å³æ—¶è§¦å‘ç³»ç»Ÿåˆ‡æ¢ï¼ˆä¾‹å¦‚ {å¦¹å¦¹} / {æŸ³å¶} ï¼‰
                                    try:
                                        et.detect_and_trigger_emotions("{" + payload + "}", is_ai_response=True)
                                        # åˆ‡æ¢ä¸ä»£è¡¨æœ‰éŸ³é¢‘æ’­å‡ºï¼Œä¸æ ‡è®°emitted_any
                                    except Exception as _se:
                                        util.log(2, f"[NLP-Stream] ç³»ç»Ÿåˆ‡æ¢è§¦å‘å¤±è´¥: {_se}")
                            except Exception as _e:
                                util.log(2, f"[NLP-Stream] å¸§çº§æ’å…¥å¤±è´¥: {_e}")

                    # è‹¥æœ¬æ®µåªæœ‰æ ‡è®°æ— æ­£æ–‡ï¼Œä¹Ÿè¦æ¨é€å‰ç«¯æ˜¾ç¤º
                    if not has_text_part and display_text.strip():
                        try:
                            if feifei and hasattr(feifei, "send_panel_reply"):
                                feifei.send_panel_reply(display_text, username="User", is_intermediate=True, phase="stream")
                        except Exception as _se:
                            util.log(2, f"[NLP-Stream] ä»…å‰ç«¯æ˜¾ç¤ºå¤±è´¥: {_se}")

                    seg_buf = ""
                    last_emit = now

            # å¼ºåˆ¶æŒ‰UTF-8è§£æSSE
            chunk_count = 0  # ğŸ”¥ è°ƒè¯•ï¼šç»Ÿè®¡æ”¶åˆ°çš„chunkæ•°é‡
            music_status_sent = set()  # ğŸµ è®°å½•å·²å‘é€çš„éŸ³ä¹çŠ¶æ€ï¼Œé¿å…é‡å¤
            # ğŸ”¥ è°ƒè¯•ï¼šæ‰“å°è¯·æ±‚å‚æ•°
            try:
                system_blob = "\n\n".join(
                    [m.get("content", "") for m in messages if m.get("role") == "system"]
                ).strip()
                last_user = ""
                for m in reversed(messages):
                    if m.get("role") == "user":
                        last_user = (m.get("content") or "")
                        break
                util.log(
                    1,
                    f"[NLP-Streamè°ƒè¯•] ğŸ“¤ APIè¯·æ±‚: model={data.get('model')}, max_tokens={data.get('max_tokens')}, system_prompté•¿åº¦={len(system_blob)}, user_msgé•¿åº¦={len(last_user)}",
                )
            except Exception:
                util.log(1, f"[NLP-Streamè°ƒè¯•] ğŸ“¤ APIè¯·æ±‚: model={data.get('model')}, max_tokens={data.get('max_tokens')}")
            for raw_line in resp.iter_lines(decode_unicode=False):
                if not raw_line:
                    continue
                try:
                    line = raw_line.decode('utf-8', errors='ignore')
                except Exception:
                    continue
                if not line:
                    continue
                if line.startswith('data: '):
                    payload = line[6:].strip()
                    if payload == "[DONE]":
                        util.log(1, f"[NLP-Streamè°ƒè¯•] ğŸ æ”¶åˆ°[DONE]ï¼Œæµå¼ç»“æŸï¼Œå·²æ”¶åˆ°{chunk_count}ä¸ªchunkï¼Œå…¨æ–‡: {full_text}")
                        break
                    try:
                        obj = json.loads(payload)
                        delta = obj.get('choices', [{}])[0].get('delta', {})
                        token = delta.get('content', '')
                        # ğŸ”¥ è°ƒè¯•ï¼šæ‰“å°æ¯ä¸ªchunkçš„å†…å®¹
                        util.log(1, f"[NLP-Streamè°ƒè¯•] ğŸ“¦ æ”¶åˆ°chunk: tokené•¿åº¦={len(token) if token else 0}, tokenå†…å®¹={'æœ‰å†…å®¹' if token else 'ç©º'}")
                        # ğŸ”¥ è°ƒè¯•ï¼šæ£€æŸ¥finish_reasonå’Œusage
                        finish_reason = obj.get('choices', [{}])[0].get('finish_reason')
                        usage = obj.get('usage')
                        if finish_reason:
                            util.log(1, f"[NLP-Streamè°ƒè¯•] âš ï¸ finish_reason={finish_reason}ï¼Œusage={usage}ï¼Œå½“å‰å…¨æ–‡: {full_text}")
                    except Exception as e:
                        util.log(2, f"[NLP-Streamè°ƒè¯•] âŒ JSONè§£æå¤±è´¥: {e}")
                        token = ""
                    if not token:
                        util.log(1, f"[NLP-Streamè°ƒè¯•] â­ï¸ è·³è¿‡ç©ºtoken")
                        continue
                    chunk_count += 1
                    full_text += token
                    
                    # brace æ·±åº¦è¿½è¸ª
                    for ch in token:
                        if ch == '{':
                            brace_depth += 1
                        elif ch == '}':
                            brace_depth = max(0, brace_depth - 1)
                    seg_buf += token
                    try_emit(force=False)
            # æœ€åflush
            if seg_buf:
                try_emit(force=True)
            
            # æµå¼æ’­æ”¾ç»“æŸï¼šå¦‚å·²æ’­å‡ºè¿‡å†…å®¹ï¼Œè®¾ç½®è·³è¿‡æ ‡å¿—é˜²æ­¢CoreäºŒæ¬¡æ’­æŠ¥
            util.log(1, f"[NLP-Streamè°ƒè¯•] ğŸ¯ æµå¼æ’­æ”¾ç»“æŸï¼Œemitted_any={emitted_any}, å…¨æ–‡é•¿åº¦={len(full_text)}, chunkæ•°={chunk_count}")
            try:
                from core import sisi_booter
                if hasattr(sisi_booter, 'feiFei') and sisi_booter.feiFei:
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæµå¼ç»“æŸåæ‰è®¾ç½®è·³è¿‡æ ‡å¿—ï¼Œé¿å…åç»­åˆ†æ®µTTSè¢«è¯¯è·³è¿‡
                    if emitted_any and not skip_flag_set[0]:
                        setattr(sisi_booter.feiFei, '_skip_next_tts', True)
                        setattr(sisi_booter.feiFei, '_skip_tts_timestamp', time.time())
                        skip_flag_set[0] = True
                        util.log(1, "[NLP-Stream] âœ… æµå¼ç»“æŸåè®¾ç½®_skip_next_ttsï¼Œé˜²æ­¢CoreäºŒæ¬¡æ’­æŠ¥")
                    else:
                        util.log(1, "[NLP-Stream] âœ… è·³è¿‡æ ‡å¿—æœªè®¾ç½®ï¼ˆæœªæ’­å‡ºæˆ–å·²è®¾ç½®ï¼‰")
            except Exception as _e:
                util.log(2, f"[NLP-Stream] æ ‡å¿—å¤„ç†å¤±è´¥: {_e}")
            return full_text.strip(), style_hint
        except Exception as e:
            util.log(2, f"[NLP-Stream] æµå¼SSEå¼‚å¸¸: {e}")
            # è¿”å›ç©ºæ–‡æœ¬ä»¥ä¾¿ä¸Šå±‚èµ°éæµå¼å…œåº•
            return "", style_hint

    try:
        # ğŸ¯ æ–°å¢ï¼šéŸ³é¢‘ä¸Šä¸‹æ–‡å¤„ç†
        audio_context_prompt = ""
        if audio_context:
            try:
                from .audio_context_processor import get_audio_context_processor
                from .audio_context_llm import get_audio_context_llm

                # å¤„ç†éŸ³é¢‘ä¸Šä¸‹æ–‡
                audio_processor = get_audio_context_processor()
                audio_llm = get_audio_context_llm()

                # ğŸ§  åå°åˆ†æï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡å¿«é€Ÿå“åº”ï¼‰
                import threading
                def background_analysis():
                    try:
                        suggestion = audio_llm.analyze_and_suggest(
                            audio_context, content,
                            audio_context.get("speaker_info")
                        )
                        if suggestion:
                            audio_llm.send_to_transit_station(suggestion)
                    except Exception as e:
                        util.log(2, f"[éŸ³é¢‘ä¸Šä¸‹æ–‡] åå°åˆ†æå¤±è´¥: {e}")

                # å¯åŠ¨åå°åˆ†æçº¿ç¨‹
                threading.Thread(target=background_analysis, daemon=True).start()

                # ğŸ¯ ç”Ÿæˆå³æ—¶ä¸Šä¸‹æ–‡æç¤ºè¯ï¼ˆä¸é˜»å¡ï¼‰
                context_prompt = audio_processor.get_context_prompt(audio_context)
                if context_prompt:
                    audio_context_prompt = f"\n{context_prompt}\n"
                    util.log(1, f"[éŸ³é¢‘ä¸Šä¸‹æ–‡] ç”Ÿæˆæç¤ºè¯: {context_prompt[:50]}...")

            except Exception as e:
                util.log(2, f"[éŸ³é¢‘ä¸Šä¸‹æ–‡] å¤„ç†å¤±è´¥: {e}")
                audio_context_prompt = ""
        # æ˜¯å¦ä½¿ç”¨æµå¼æ¨¡å¼ - å¯ç”¨åˆ†å—æµå¼
        use_stream = True

        # é¢„ç½®æƒ…æ„Ÿæ ‡è®°ï¼Œé¿å…åç»­æœªèµ‹å€¼æ—¶æŠ¥é”™
        emotion = ""

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å†’çŠ¯æ€§è¯è¯­
        disrespectful_keywords = [
            "ä½ ç®—ä»€ä¹ˆ", "ä½ ä¹Ÿé…", "æ»š", "é—­å˜´", "ç¬¨è›‹", "åºŸç‰©",
            "ä»€ä¹ˆä¸œè¥¿", "åƒåœ¾", "å‚»", "è ¢", "ç™½ç—´", "ç‹—å±",
            "å»æ­»", "æ··è›‹", "è®¨åŒ", "çƒ¦äºº", "æ— èƒ½", "åºŸè¯"
        ]
        is_disrespectful = any(keyword in content.lower() for keyword in disrespectful_keywords)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®Šè¯­æ°”æŒ‡ä»¤
        whisper_keywords = ["æ‚„æ‚„", "å°å£°", "å·å·", "è½»å£°"]
        fast_keywords = ["å¿«ç‚¹è¯´", "èµ¶ç´§è¯´", "å¿«é€Ÿ", "æŠ“ç´§"]
        slow_keywords = ["æ…¢ç‚¹è¯´", "æ…¢æ…¢è¯´", "ç¼“ç¼“"]

        session = get_session()
        history_context = get_communication_history(uid, query_text=content, include_other=False, as_text=False)

        recent_messages = []
        summary_context = ""
        older_context = ""
        if history_context:
            recent_messages = getattr(history_context, "recent_messages", []) or []
            summary_context = getattr(history_context, "summary_text", "") or ""
            older_context = getattr(history_context, "older_text", "") or ""

        # ???????????????????prompt
        brain_context = ""
        if brain_prompts:
            dynamic_prompt = (brain_prompts.get('dynamic_prompt') or '').strip()
            if dynamic_prompt:
                brain_context = dynamic_prompt

        # åŠ¨æ€è·å–å½“å‰ç”¨æˆ·èº«ä»½
        current_user_name = "ç”¨æˆ·"
        current_user_role = "guest"
        if speaker_info:
            current_user_name = speaker_info.get('real_name', 'ç”¨æˆ·')
            current_user_role = speaker_info.get('role', 'guest')

        # ğŸ§  é•¿æœŸè®°å¿†æ³¨å…¥ï¼ˆå»¶è¿Ÿæ³¨å…¥ç‰ˆï¼‰
        # çº¦æŸï¼šå‰å° question() ä¸å…è®¸å®æ—¶/åŠåŒæ­¥æ£€ç´¢ Mem0ã€‚
        # è®°å¿†æ£€ç´¢ + ç»„ç»‡ç”±â€œå‰è„‘/åŠ¨æ€ä¸­æ¢â€åå°äº§å‡ºï¼Œä¸‹ä¸€è½®é€šè¿‡ brain_prompts['memory_context'] æ³¨å…¥ã€‚
        memory_context_prompt = ""
        try:
            if brain_prompts:
                mem_ctx = (brain_prompts.get("memory_context") or "").strip()
                if mem_ctx and mem_ctx not in ("æ— ç›¸å…³è®°å¿†", "æ— ç›¸å…³Sisiè®°å¿†", "è®°å¿†ç³»ç»Ÿä¸å¯ç”¨"):
                    memory_context_prompt = mem_ctx
        except Exception:
            memory_context_prompt = ""
        base_prompt = build_prompt(observation, "")

        dynamic_parts = []
        if audio_context_prompt:
            dynamic_parts.append(audio_context_prompt.strip())
        dynamic_block = "\n".join([p for p in dynamic_parts if p]).strip()

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼Œä½¿ç”¨åŠ¨æ€èº«ä»½ä¿¡æ¯
        if speaker_info and speaker_info.get('real_name'):
            speaker_name = speaker_info['real_name']
            user_message = content
        else:
            user_message = content

        # ä¸å†åœ¨ç”¨æˆ·æ¶ˆæ¯ä¸­æ³¨å…¥æ—¶é—´æˆ³ï¼Œé¿å…æ¨¡å‹å¤è¯»

        # ç»„è£… system messagesï¼ˆé‡è¦åœ¨å‰ï¼Œå‚è€ƒåœ¨åï¼‰
        system_messages = []
        if base_prompt:
            system_messages.append({"role": "system", "content": base_prompt})
        if dynamic_block:
            system_messages.append({"role": "system", "content": dynamic_block})

        ref_parts = []
        if summary_context:
            ref_parts.append(summary_context)
        if older_context:
            ref_parts.append(older_context)
        if memory_context_prompt:
            ref_parts.append(memory_context_prompt)
        if ref_parts:
            system_messages.append({"role": "system", "content": "\n\n".join(ref_parts)})

        messages = []
        messages.extend(system_messages)
        if recent_messages:
            messages.extend(recent_messages)
        if brain_context:
            messages.append({"role": "system", "content": brain_context})
        messages.append({"role": "user", "content": user_message})

        # ğŸ”¥ è°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„ä¼ é€’ç»™å¤§æ¨¡å‹çš„å†…å®¹
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ==================== å¼€å§‹ ====================")
        try:
            from sisi_memory.chat_history import format_messages_as_text
            recent_text = format_messages_as_text(recent_messages or [])
        except Exception:
            recent_text = ""
        system_blob = "\n\n".join([m.get("content", "") for m in system_messages]).strip()
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ğŸ“ System Prompt (å‰500å­—ç¬¦):\n{system_blob[:500]}")
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ğŸ“ System Prompt (å500å­—ç¬¦):\n{system_blob[-500:]}")
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ğŸ“ System Prompt æ€»é•¿åº¦: {len(system_blob)} å­—ç¬¦")
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ğŸ’¬ User Message: {user_message}")
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ğŸ“š å¯¹è¯å†å²:\n{recent_text[:500] if recent_text else 'æ— å†å²'}")
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ğŸ§  å‰è„‘æç¤ºè¯:\n{brain_context[:300] if brain_context else 'æ— å‰è„‘æç¤º'}")
        util.log(1, f"[NLP-å®Œæ•´è°ƒè¯•] ==================== ç»“æŸ ====================")

        llm_cfg = get_llm_cfg()

        # === ä¸»è·¯å¾„ï¼šçœŸæ­£LLMæµå¼ ===
        if use_stream:
            streamed_text, style_stream = _stream_llm_and_tts(messages, style_hint="gentle")
            if streamed_text:
                # å­˜å‚¨ä¸è¿”å›
                answer = streamed_text
                style = style_stream
            else:
                # æµå¼å¤±è´¥ï¼šä¸åšå…œåº•ï¼Œä¸è¿›è¡Œéæµå¼å›é€€
                util.log(2, "[NLP-Stream] æµå¼å¤±è´¥ï¼Œå·²ç¦ç”¨å…œåº•")
                answer, style = "", style_stream
        else:
            # æ—§è·¯å¾„ï¼ˆéæµå¼ï¼‰
            response = send_llm_request(session, {"messages": messages, "stop": ["ASSISTANT:", "USER:", "åŠ©æ‰‹ï¼š", "ç”¨æˆ·ï¼š", "ç³»ç»Ÿï¼š"]}, llm_cfg)
            if response and isinstance(response, dict):
                answer = response["text"].strip() or "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”..."
                style = response.get("tone", "gentle")
                emotion = response.get("emotion", "")
            else:
                answer, style = "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”...", "gentle"

        # === æƒ…æ„Ÿ/ç³»ç»Ÿåˆ‡æ¢æ ‡è®°å¤„ç† ===
        # æµå¼æ¨¡å¼å·²åœ¨ _stream_llm_and_tts ä¸­è§¦å‘è¿‡æƒ…æ„Ÿï¼Œè¿™é‡Œä¸é‡å¤è§¦å‘ï¼›
        # éæµå¼æ¨¡å¼éœ€è¦è§¦å‘ä¸€æ¬¡ï¼Œä½†ä¸æ¸…ç†æ–‡æœ¬ï¼ˆä¿ç•™ç»™å‰ç«¯/å†å²ï¼‰ã€‚
        try:
            if not use_stream:
                from utils.emotion_trigger import detect_and_trigger_emotions
                detect_and_trigger_emotions(answer or "", is_ai_response=True)
                util.log(1, f"[NLP-LLM] éæµå¼å·²è§¦å‘æƒ…æ„Ÿæ ‡è®°")
            else:
                util.log(1, f"[NLP-LLM] æµå¼å·²å¤„ç†æƒ…æ„Ÿæ ‡è®°ï¼Œä¿ç•™åŸæ–‡")
        except Exception as _e:
            util.log(2, f"[NLP-LLM] æƒ…æ„Ÿè§¦å‘è§£æå¤±è´¥: {_e}")

        if not (answer or "").strip():
            util.log(2, "[NLP-LLM] empty_model_output (no fallback)")
            return "", style

        # ğŸ§  å¼‚æ­¥å­˜å‚¨å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿ - add_sisi_interaction_memoryå·²ç»æ˜¯å¼‚æ­¥çš„
        try:
            # ç»Ÿä¸€ user_id è§„åˆ™ï¼šä¸å†å² SoT çš„ uidâ†’user_id è§„åˆ™ä¸€è‡´ï¼Œå¹¶åŸºäº mode å‘½åç©ºé—´éš”ç¦»
            if isinstance(uid, str) and uid.startswith("user"):
                base_user_id = uid
            elif uid != 0:
                base_user_id = f"user{uid}"
            else:
                base_user_id = "default_user"

            try:
                from llm.liusisi import get_current_system_mode
                mode = get_current_system_mode()
            except Exception:
                mode = "sisi"
            try:
                from sisi_memory.context_kernel import namespaced_user_id as _namespaced_user_id, normalize_persona

                namespaced_user_id = _namespaced_user_id(normalize_persona(mode), base_user_id)
            except Exception:
                namespaced_user_id = f"{mode}::{base_user_id}"

            # ğŸš€ ç›´æ¥è°ƒç”¨å¼‚æ­¥å­˜å‚¨å‡½æ•°ï¼ˆå†…éƒ¨å·²ç»æ˜¯åå°çº¿ç¨‹ï¼‰
            success = add_sisi_interaction_memory(
                text=content,  # ç”¨æˆ·è¯´çš„è¯
                speaker_id=namespaced_user_id,  # å‘½åç©ºé—´åŒ–çš„ç”¨æˆ·ID
                response=answer,  # æŸ³æ€æ€çš„å›å¤
                speaker_info=speaker_info  # å£°çº¹èº«ä»½ä¿¡æ¯
            )
            util.log(1, f"[NLP-LLM] ğŸš€ è®°å¿†å­˜å‚¨å·²å¯åŠ¨: {namespaced_user_id}")
        except Exception as e:
            util.log(2, f"[NLP-LLM] è®°å¿†å­˜å‚¨å¼‚å¸¸: {e}")

        # å¯¹è¯äº‹ä»¶æµ SoT çš„å†™å…¥ç”± core/sisi_core.py ç»Ÿä¸€è´Ÿè´£ï¼Œè¿™é‡Œä¸é‡å¤å†™å…¥ï¼Œé¿å…åŒå†™/é‡å¤è®°å½•

        # ğŸ§  å¯¹è¯å†å²å·²é€šè¿‡â€œäº‹ä»¶æµ + æ‘˜è¦ + è®°å¿†â€ç»Ÿä¸€ç®¡ç†ï¼Œæ— éœ€æ‰‹åŠ¨ç»´æŠ¤historyåˆ—è¡¨

        # åªåœ¨æœ‰è¡¨æƒ…æ—¶æ·»åŠ è¡¨æƒ…
        return f"{emotion} {answer}" if emotion else answer, style

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        util.log(2, f"[NLP] âŒ questionå‡½æ•°å¼‚å¸¸: {e}")
        util.log(2, f"[NLP] âŒ è¯¦ç»†é”™è¯¯: {error_detail}")

        answer = f"ç³»ç»Ÿé‡åˆ°äº†ä¸€ç‚¹é—®é¢˜: {str(e)}"
        style = 'gentle'
        util.log(1, f"[NLP] questionå‡½æ•°è¾“å‡ºæ–‡æœ¬: {answer}")
        util.log(1, f"[NLP] questionå‡½æ•°è¾“å‡ºtone: {style}")
        return answer, style

if __name__ == "__main__":
    for _ in range(3):
        query = "çˆ±æƒ…æ˜¯ä»€ä¹ˆ"
        response, style = question(query)
        print("\nThe result is:", response, "Style:", style)
