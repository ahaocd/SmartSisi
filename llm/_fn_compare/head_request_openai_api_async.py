async def request_openai_api_async(text: str, uid=0, observation: str = ''):
    """
    å¼‚æ­¥å¤„ç†è¯·æ±‚ï¼Œæ”¯æŒå¹¶è¡Œè°ƒç”¨

    Args:
        text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        uid: ç”¨æˆ·ID
        observation: ç¯å¢ƒè§‚å¯Ÿä¿¡æ¯

    Returns:
        (å›ç­”æ–‡æœ¬, é£æ ¼)
    """
    try:
        # è®°å½•è°ƒç”¨
        util.log(1, f"[LLMæ¨¡å‹] å¼‚æ­¥å¤„ç†è¯·æ±‚: {text}")

        # æ£€æŸ¥å·¥å…·è°ƒç”¨
        if is_tool_call_quick(text):
            # ç®€å•å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å¤„ç†
            tool_result = process_with_tools_sync(text, uid)
            if tool_result:
                return tool_result, "llm"

        # åˆ›å»ºä¼šè¯å¹¶æ„å»ºè¯·æ±‚æ•°æ®
        session = get_session()
        history_context = get_communication_history(uid, query_text=text, as_text=True)

        # ğŸ§  æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„æç¤ºè¯
        if isinstance(history_context, str) and history_context not in ("æ— å¯¹è¯å†å²", "æ— å†å²è®°å¿†", "æ— å¯¹è¯å†å²..."):
            enhanced_prompt = build_prompt(observation) + f"\n\n{history_context}"
        else:
            enhanced_prompt = build_prompt(observation)

        llm_cfg = get_llm_cfg()
        data = {
            "model": llm_cfg["model"],
            "messages": [
                {"role": "system", "content": enhanced_prompt},
                {"role": "user", "content": text}
            ],
            "temperature": 0.5,
            "max_tokens": 1000,
            "top_p": 0.6,
            "stream": False
        }

        # å‘é€è¯·æ±‚
        url = llm_cfg["base_url"] + "/chat/completions"
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f"Bearer {llm_cfg['api_key']}"
        }

        # å¼‚æ­¥å‘é€è¯·æ±‚
        async def async_request():
            # è®¾ç½®è¶…æ—¶ï¼Œä¸åŒæ­¥è·¯å¾„ä¿æŒä¸€è‡´
            timeout = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession(timeout=timeout) as client_session:
                async with client_session.post(url, json=data, headers=headers) as response:
                    if response.status == 200:
                        result = await response.json()

                        if "choices" not in result or not result["choices"]:
                            return "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”...", "gentle"

                        content = result["choices"][0]["message"]["content"]

                        # è¾“å‡ºå¸¦emojiçš„LLMè¿”å›å†…å®¹
                        util.log(1, f"[LLM] ğŸ¤– {content} ğŸ¤–")

                        # ç›´æ¥å¤„ç†æ–‡æœ¬å†…å®¹
                        text = content.strip()

                        # æ”¹è¿›å‰ç¼€æ¸…ç†é€»è¾‘ï¼Œå¤„ç†æ›´å¤šå¯èƒ½çš„å‰ç¼€æƒ…å†µ
                        # å¸¸è§çš„é”™è¯¯å‰ç¼€æ¨¡å¼åˆ—è¡¨
                        prefix_patterns = [
                            "Ê”ignment:", "alignment:", "å¯¹é½:", "å›ç­”:", "å›å¤:", "assistant:",
                            "ai:", "response:", "ç­”å¤:", "ç­”æ¡ˆ:"
                        ]

                        # æ£€æŸ¥å¹¶ç§»é™¤å·²çŸ¥å‰ç¼€
                        text_lower = text.lower()
                        for prefix in prefix_patterns:
                            if text_lower.startswith(prefix.lower()):
                                # æ‰¾åˆ°å†’å·åçš„ä½ç½®
                                colon_pos = text.find(':')
                                if colon_pos > 0:
                                    text = text[colon_pos + 1:].strip()
                                    break

                        # å¦‚æœæ–‡æœ¬ä»¥è¡¨æƒ…ç¬¦å·å¼€å¤´ï¼Œä¹Ÿå°è¯•æ¸…ç†
                        if text and text[0] in ["ğŸ¤«", "ğŸ˜", "ğŸ˜ ", "ğŸ¤–"]:
                            text = text[1:].strip()

                        # æ£€æµ‹æƒ…ç»ªå¹¶è®¾ç½®ç›¸åº”å‚æ•°
                        tone = "gentle"  # é»˜è®¤æ¸©å’Œè¯­æ°”

                        # æ£€æµ‹æ„¤æ€’æƒ…ç»ª
                        if "ğŸ˜ " in text:
                            tone = "angry"
                        # æ£€æµ‹æ‚„æ‚„è¯æƒ…ç»ª
                        elif "ğŸ¤«" in text:
                            tone = "whisper"

                        # åœ¨æ—¥å¿—ä¸­æ ‡è®°æ¨¡å‹æ¥æºï¼Œä½†ä¸ä¿®æ”¹å®é™…å›å¤å†…å®¹
                        log_text = f"[NLP-7B] {text}"
                        util.log(1, f"[LLM] ğŸ¤– {log_text} ğŸ¤–")

                        return text, tone
                    else:
                        error_text = await response.text()
                        util.log(2, f"[LLM] APIé”™è¯¯: çŠ¶æ€ç  {response.status}, é”™è¯¯ä¿¡æ¯: {error_text}")
                        return f"æŠ±æ­‰ï¼ŒAPIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}", "gentle"

        # å°è¯•å¯¼å…¥aiohttpï¼Œå¦‚æœå¯¼å…¥å¤±è´¥åˆ™ä½¿ç”¨åŒæ­¥æ–¹æ³•
        try:
            import aiohttp
            return await async_request()
        except ImportError:
            util.log(2, "[LLM] aiohttpæ¨¡å—æœªå®‰è£…ï¼Œä½¿ç”¨åŒæ­¥æ–¹æ³•")
            # ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼Œä½†æ·»åŠ è¶…æ—¶æ§åˆ¶
            with concurrent.futures.ThreadPoolExecutor() as executor:
                try:
                    # æ·»åŠ è¶…æ—¶æ§åˆ¶ï¼Œæ”¹ä¸º10ç§’ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¤„ç†æ—¶é—´
                    future = executor.submit(send_llm_request, session, data, llm_cfg)
                    response_tuple = future.result(timeout=10)  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°10ç§’

                    if isinstance(response_tuple, tuple) and len(response_tuple) == 2:
                        return response_tuple
                    else:
                        # ç¡®ä¿è¿”å›å…ƒç»„æ ¼å¼
                        if isinstance(response_tuple, dict):
                            return response_tuple.get("text", "æŠ±æ­‰ï¼Œå¤„ç†å‡ºé”™"), response_tuple.get("tone", "gentle")
                        elif isinstance(response_tuple, str):
                            return response_tuple, "gentle"
                        else:
                            return "æŠ±æ­‰ï¼Œå“åº”æ ¼å¼ä¸æ­£ç¡®", "gentle"
                except concurrent.futures.TimeoutError:
                    util.log(2, "[LLM] åŒæ­¥è¯·æ±‚è¶…æ—¶")
                    return "æŠ±æ­‰ï¼Œç½‘ç»œè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•ã€‚", "gentle"
        except Exception as e:
            util.log(2, f"[LLM] å¼‚æ­¥è¯·æ±‚å¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œè¯·æ±‚å¤„ç†å‡ºé”™: {str(e)}", "gentle"

    except Exception as e:
        import traceback
        error_msg = f"LLMæ¨¡å‹å¼‚æ­¥å¤„ç†å¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"
        util.log(2, error_msg)
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é—®é¢˜: {str(e)}", "gentle"
