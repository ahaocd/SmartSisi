def question(content, uid=0, observation="", audio_context=None, brain_prompts=None, speaker_info=None, mode_switched: bool = False):
    """é»æ„°æ£¶é‚è§„ç¡¶é”›å²€æ‚Šç›ã„¦å„éªæƒ°å¹é™æ §æ´–æ´?

    Args:
        content: é¢ã„¦åŸ›æˆæ’³å†é?
        uid: ç”¨æˆ·ID
        observation: è§‚å¯Ÿä¿¡æ¯
        audio_context: é—Šå……ç¬‚æ¶“å¬«æƒéç‰ˆåµ“é‚?
        brain_prompts: å‰è„‘ç³»ç»Ÿç”Ÿæˆçš„åŠ¨æ€æç¤ºè¯ï¼ˆæ–°å¢ï¼‰
    Returns:
        Tuple[str, str]: (é¥ç‚µç“Ÿé‚å›¨æ¹°, ç’‡ç†¼î—“é?
    """

    # é¦ƒå°¶ å¨´å¬«ç…¶é™æƒ°çšŸé¢ã„©æ¸¶å§¹å‚¦ç´æµ£å—•ç¬‰é¦ã„¥î˜©éå——åé¹?
    # éŒå†²å½¾é©ç¨¿å§å§¹å‚šçš¢é¢è¾«çŸ¾é¢è¾©éƒ´ç¼?
    if isinstance(content, list):
        text_parts = []
        for part in content:
            if isinstance(part, dict):
                if part.get("type") == "text":
                    text_parts.append(str(part.get("text") or "").strip())
                    continue
                text_parts.append(f"[{part.get('type') or 'part'}]")
            else:
                text_parts.append(str(part))
        content = " ".join([p for p in text_parts if p]).strip()
    elif isinstance(content, dict):
        if "text" in content:
            content = str(content.get("text") or "").strip()
        else:
            try:
                content = json.dumps(content, ensure_ascii=False)
            except Exception:
                content = str(content)
    else:
        content = str(content or "")
    liuye_keywords = ["å«æŸ³å¶", "æŸ³å¶", "åŒ»ç–—åŒ…", "ç³»ç»Ÿè¯Šæ–­", "ä»£ç ä¼˜åŒ–", "AIåä½œ"]
    if any(word in content for word in liuye_keywords):
        util.log(1, f"[NLP] æ£€æµ‹åˆ°æŸ³å¶éœ€æ±‚å…³é”®è¯ï¼Œå°†è·¯ç”±åˆ°åŒ»ç–—åŒ…ç³»ç»Ÿ")
        # æ©æ¬“å™·æ´æ—‡çšŸé¢ã„¨çª‹ç»¯è¤ç²ºé”›å±¼ç¬‰é„æ¶™å¸´é’å›¨å´²å¦¯?
        # TODO: é—†å—˜åšéŒå†²å½¾ç’ºæ—‚éƒ´ç¼?
    util.log(1, f"[NLP] questionå‡½æ•°è¾“å…¥: {content}")

    # === éªç†ºæ®‘LLMå¨´ä½¸ç´¡é”›åœ«SEé”›å¤ç·­é‘è½°ç¬Œå¨ˆé›å”´é—è™«æ¤‚TTS ===
    def _stream_llm_and_tts(messages: list, style_hint: str = "gentle") -> tuple:
        """ç’‹å†ªæ•¤OpenAIéç³SEå¨´ä½¸ç´¡é”›å²ƒç«Ÿé€ç§šokenæˆç‘°åå¨ˆé›è‹ŸTTSéŠ†å‚ç¹‘?ç€¹å±¾æš£é‚å›¨æ¹°, style)?

## ???????????##
1. ?????????? {??????}
2. ?????????? {??}
3. ?????????? {??} ? {??}
4. ???????????????{}

## ????????????##
- ???????????????/??/??/??/??/??/??/??/???/??????/?????
- ??????????????????? {??????}?
- ??????????????? {??????}??????????

???
?????????
???{??????}

"""
        try:
            # é¦ƒæ•Ÿ éæŠ½æ•­é”›æ°¬æ¹ªtryé§æ¥€å”´æ¿®å¬ªç•¾æ¶”å¡»kip_flag_set
            skip_flag_set = [False]  # ä½¿ç”¨åˆ—è¡¨é¿å…nonlocalé—®é¢˜
            
            session = get_session()
            llm_cfg = get_llm_cfg()
            url = llm_cfg["base_url"] + "/chat/completions"
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f"Bearer {llm_cfg['api_key']}",
                'Accept-Charset': 'utf-8'
            }
            data = {
                "model": llm_cfg["model"],
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 2000,
                "top_p": 0.9,
                "stream": True,
                "stop": ["ASSISTANT:", "USER:", "åŠ©æ‰‹ï¼š", "ç”¨æˆ·ï¼š", "ç³»ç»Ÿï¼š"]
            }
            # å¯¤è™¹ç›å¨´ä½¸ç´¡ç’‡é”‹çœ°é”›å ¢æ•¤é‘·î„å§©unicodeç‘™ï½‡çˆœé”›å±½å·±é’ç¦ªTF-8ç‘™ï½†ç€½?
            resp = session.post(url, json=data, headers=headers, stream=True, timeout=(1, 30))
            # é¦ƒæ•¡ é™å¬ªã‚½é–¿æ¬™ç´°é–´?é‰å†®æªºé’å¿”å§å¯®å‚šçˆ¶é”›å²€æ´¿éºãƒ§ç²°é‘å“„å½„æµ£æ»…æ®‘é»æ„®ãš
            if resp.status_code in (401, 403):
                persona = get_current_system_mode()
                hint = (
                    f"AIæ¥å£è®¤è¯å¤±è´¥(HTTP {resp.status_code})ã€‚"
                    f"è¯·æ£€æŸ¥ SmartSisi/system.conf ä¸­ {persona}_llm_api_key å’Œ {persona}_llm_base_url é…ç½®ã€‚"
                )
                util.log(2, f"[NLP-Stream] {hint}")
                return "", style_hint

            try:
                resp.raise_for_status()
            except Exception as _http_e:
                # çä»‹å™ºé¶å©ƒæ¹‡é”Â¤ç¹‘é¥ç‚°ç¶‹éµæ’³åš­é‰ãƒ¯ç´™é´î…æŸ¤é”›å±¾æŸŸæ¸šå®ç•¾æµ£å¶†æ§¸å¦¯â€³ç€·?é™å‚›æšŸ/æµ ï½‡æ‚Šé¨å‹¯æ£¶?
                try:
                    body_preview = (resp.text or "")[:500]
                except Exception:
                    body_preview = ""
                util.log(2, f"[NLP-Stream] HTTPå¼‚å¸¸: {str(_http_e)}; body[:500]={body_preview}")
                raise

            # é¾æ—‚æµ‰é?
            try:
                from core import sisi_booter
                sisi_core = getattr(sisi_booter, 'sisi_core', None) or getattr(sisi_booter, 'sisiCore', None)
            except Exception as e:
                sisi_core = None

            full_text = ""
            seg_buf = ""
            last_emit = time.time()
            brace_depth = 0  # ç”¨äºé¿å…æˆªæ–­æœªé—­åˆçš„{...}
            min_interval = 0.4
            max_len = 28
            emitted_any = False  # ä»…å½“å®é™…æ’­å‡ºè¿‡å†…å®¹æ—¶ï¼Œæ‰åœ¨ç»“æŸæ—¶è®¾ç½®è·³è¿‡æ ‡è®°

            from utils.emotion_trigger import detect_and_trigger_emotions
            import re

            def _esp32_connected() -> bool:
                try:
                    import sys
                    adapter = None
                    if "sisi_adapter" in sys.modules:
                        mod = sys.modules["sisi_adapter"]
                        if hasattr(mod, "get_adapter_instance"):
                            adapter = mod.get_adapter_instance()
                        elif hasattr(mod, "_ADAPTER_INSTANCE"):
                            adapter = mod._ADAPTER_INSTANCE
                    if not adapter:
                        return False
                    clients = getattr(adapter, "clients", None) or {}
                    if isinstance(clients, dict):
                        for ws in clients.values():
                            if ws and not getattr(ws, "closed", False):
                                return True
                        return bool(clients)
                    return False
                except Exception:
                    return False

            def _enqueue_pc_audio(file_path: str, label: str) -> bool:
                try:
                    from utils.pc_stream_queue import get_pc_stream_queue
                    import threading as _threading
                    pc_queue = get_pc_stream_queue()
                    sink = pc_queue.enqueue_stream(label=label)
                    _threading.Thread(
                        target=pc_queue.stream_wav_file_to_sink,
                        args=(file_path, sink),
                        daemon=True,
                    ).start()
                    return True
                except Exception as _qe:
                    util.log(2, f"[NLP-Stream] PCé˜Ÿåˆ—æ’å…¥å¤±è´¥: {_qe}")
                    return False

            def try_emit(force=False):
                nonlocal seg_buf, last_emit, brace_depth, emitted_any
                now = time.time()
                # é¦ƒæ•Ÿ é”›æ°¬å½§é¸å¤‹çˆ£éç‘°åå¨ˆç¢‰ç´æ¶“å¶†å¯œéƒå •æ£¿/é—€å®å®³å¯®å“„åŸ—é’å—­ç´é–¬å®å¤æ¶“â‚¬é™ãƒ¨ç˜½çšå¬«åšæ¶“ã‚†î†Œé‘·å­˜å„é°ç†¶ç¬‰?
                ready_by_punct = bool(seg_buf and re.search(r'[éŠ†å‚¦ç´’??é”ç€ª]$', seg_buf))
                # é‘»ãƒ¥å¯˜éšç¡ffecté”›å±½æ•–é–²å¿•ç“‘é’æ¿å½¸æ¸šÑƒå½éˆå——æ‚™é”›å±¼äº’ç€µå½’ç¶ˆé»æ‘å†?
                contains_effect = bool(re.search(r'\{([A-Za-z0-9_\u4e00-\u9fff]+)\}', seg_buf))
                if contains_effect and not force and not ready_by_punct:
                    return
                if (force or ready_by_punct) and seg_buf and brace_depth == 0:
                    # æŒ‰å‡ºç°é¡ºåºå¤„ç†{text,effect}åºåˆ—
                    sequence = []
                    s = seg_buf
                    # å¨“å‘¯æ‚Šé—è§„ç•©éºÑƒåŸ—?
                    s = s.replace('<|endofprompt|>', '')
                    display_text = s
                    pos = 0
                    for m in re.finditer(r'\{([A-Za-z0-9_\u4e00-\u9fff]+)\}', s):
                        if m.start() > pos:
                            text_part = s[pos:m.start()]
                            sequence.append(("text", text_part))
                        effect_name = m.group(1)
                        sequence.append(("effect", effect_name))
                        pos = m.end()
                    if pos < len(s):
                        sequence.append(("text", s[pos:]))

                    try:
                        from esp32_liusisi.sisi_audio_output import AudioOutputManager
                        aom = AudioOutputManager.get_instance()
                    except Exception:
                        aom = None

                    # é¡ºåºæ‰§è¡Œï¼štext -> effect -> text ...
                    has_text_part = False
                    for item_type, payload in sequence:
                        if item_type == "text":
                            cleaned_text = (payload or "").strip()
                            if not cleaned_text:
                                continue
                            has_text_part = True
                            if sisi_core:
                                try:
                                    # éŒå†²å½¾å¦¯â€³ç´¡ç‘•ä½¸å±å¯¤å“„ç”«interleaveréå›ªç˜‘é¨åˆ¬nteractç€µç¡…è–„
                                    from llm.liusisi import get_current_system_mode
                                    current_mode = get_current_system_mode()
                                    if current_mode == "liuye":
                                        from core.interact import Interact
                                        interact_obj = Interact(interleaver="liuye", interact_type=2, data={"user": "User", "text": cleaned_text})
                                    else:
                                        interact_obj = None
                                    
                                    # é¦ƒæ•Ÿ éæŠ½æ•­é”›æ°«ç¹šé¸ä½¹ç¥¦å¯®å»¡TSé¾?
                                    sisi_core.process_audio_response(
                                        text=cleaned_text,
                                        username="User",
                                        interact=interact_obj,
                                        priority=5,
                                        style=style_hint,
                                        is_agent=False,
                                        display_text=display_text
                                    )
                                    emitted_any = True
                                    
                                except Exception as _e:
                                    util.log(2, f"[NLP-Stream] æ®µæ’­æŠ¥å¤±è´¥: {_e}")
                        else:
                            # ç”¯Ñ…éª‡é»æ‘å†é”›æ°¬çš¢éå Ÿç‰é—Šå® æµ†æ¶“ç¯›PUSç”¯Ñƒè‹Ÿé©å­˜å¸´éãƒ©æ§¦é”›å±¼ç¬‰é†å‚šä» ?
                            try:
                                from utils import emotion_trigger as et
                                trig = et.EMOTION_TRIGGER_MAP.get(payload)
                                if not trig:
                                    continue
                                ttype = trig.get('type')
                                if ttype in ['sound_effect', 'music_play']:
                                    import os
                                    fpath = trig.get('audio_file')
                                    if fpath and not os.path.isabs(fpath):
                                        fpath = os.path.abspath(fpath)
                                    if not os.path.exists(fpath):
                                        util.log(2, f"[NLP-Stream] éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {fpath}")
                                        continue

                                    # PCç’ºæ’…ç´°æ¶“å¶ˆî›¦ç’§çšƒygameéªèˆµæŒƒé”›å±¾æ•¼æ¶“çƒ˜å¸“é—ƒç†¶è¦†ç›å±¾å½ƒ?
                                    if not _esp32_connected():
                                        ok = _enqueue_pc_audio(fpath, label=f"{ttype}:{payload}")
                                        if ok:
                                            emitted_any = True
                                            util.log(1, f"[NLP-Stream] PCé˜Ÿåˆ—æ’å…¥éŸ³é¢‘: {payload}")
                                        else:
                                            util.log(2, f"[NLP-Stream] PCé˜Ÿåˆ—æ’å…¥å¤±è´¥: {payload}")
                                        continue

                                    # ESP32ç’ºæ’…ç´°é¸å¤Œè¢«é¨å¬­è›‹æ¾¶å›¨å½ƒ?
                                    util.log(1, f"[NLP-Stream] è®¾å¤‡æ’å…¥éŸ³é¢‘: {payload}")
                                    try:
                                        if ttype == 'sound_effect':
                                            et._execute_sound_effect(payload, trig)
                                        else:
                                            et._execute_music_play(payload, trig)
                                        emitted_any = True
                                    except Exception as _pe:
                                        util.log(2, f"[NLP-Stream] è®¾å¤‡æ’å…¥å¤±è´¥: {_pe}")
                                elif ttype == 'system_switch':
                                    # é—è™«æ¤‚ç‘™ï¹€å½‚ç»¯è¤ç²ºé’å›¨å´²é”›å œç·¥?{æ¿¡ç® / {éŒå†²å½¾} ?
                                    try:
                                        et.detect_and_trigger_emotions("{" + payload + "}", is_ai_response=True)
                                        # é’å›¨å´²æ¶“å¶„å”¬ç›ã„¦æ¹é—Šè™«æ‹ é”›å±¼ç¬‰éå™€mitted_any
                                    except Exception as _se:
                                        util.log(2, f"[NLP-Stream] ç³»ç»Ÿåˆ‡æ¢è§¦å‘å¤±è´¥: {_se}")
                            except Exception as _e:
                                util.log(2, f"[NLP-Stream] å¸§çº§æ’å…¥å¤±è´¥: {_e}")

                    # é‘»ãƒ¦æ¹°å¨ˆé›å½§éˆå¤‹çˆ£ç’ç‰ˆæ£¤å§ï½†æƒé”›å±¼ç¯ƒç‘•ä½¹å¸¹é–«ä½¸å¢ ç»”?
                    if not has_text_part and display_text.strip():
                        try:
                            if sisi_core and hasattr(sisi_core, "send_panel_reply"):
                                sisi_core.send_panel_reply(display_text, username="User", is_intermediate=True, phase="stream")
                        except Exception as _se:
                            util.log(2, f"[NLP-Stream] ä»…å‰ç«¯æ˜¾ç¤ºå¤±è´¥: {_se}")

                    seg_buf = ""
                    last_emit = now

            # å¯®å“„åŸ—é¸å¡™TF-8ç‘™ï½†ç€½SSE
            chunk_count = 0  # ğŸ”¥ è°ƒè¯•ï¼šç»Ÿè®¡æ”¶åˆ°çš„chunkæ•°é‡
            music_status_sent = set()  # é¦ƒå¹ ç’æ¿ç¶å®¸æ’å½‚é–«ä½ºæ®‘é—Šå……ç®°é˜è®¹ç´é–¬å®å¤é–²?
            # é¦ƒæ•Ÿ ç’‹å†­ç˜¯é”›æ°­å¢¦é—ç‰ˆçœ°é™?
            try:
                system_blob = "\n\n".join(
                    [m.get("content", "") for m in messages if m.get("role") == "system"]
                ).strip()
                last_user = ""
                for m in reversed(messages):
                    if m.get("role") == "user":
                        last_user = (m.get("content") or "")
                        break
                util.log(
                    1,
                    f"[NLP-Streamè°ƒè¯•] ğŸ“¤ APIè¯·æ±‚: model={data.get('model')}, max_tokens={data.get('max_tokens')}, system_prompté•¿åº¦={len(system_blob)}, user_msgé•¿åº¦={len(last_user)}",
                )
            except Exception:
                util.log(1, f"[NLP-Streamè°ƒè¯•] ğŸ“¤ APIè¯·æ±‚: model={data.get('model')}, max_tokens={data.get('max_tokens')}")
            for raw_line in resp.iter_lines(decode_unicode=False):
                if not raw_line:
                    continue
                try:
                    line = raw_line.decode('utf-8', errors='ignore')
                except Exception:
                    continue
                if not line:
                    continue
                if line.startswith('data: '):
                    payload = line[6:].strip()
                    if payload == "[DONE]":
                        util.log(1, f"[NLP-Streamè°ƒè¯•] æ”¶åˆ°[DONE]ï¼Œæµå¼ç»“æŸï¼Œå·²æ”¶åˆ°{chunk_count}ä¸ªchunkï¼Œå…¨æ–‡: {full_text}")
                        break
                    try:
                        obj = json.loads(payload)
                        delta = obj.get('choices', [{}])[0].get('delta', {})
                        token = delta.get('content', '')
                        # é¦ƒæ•Ÿ ç’‹å†­ç˜¯é”›æ°­å¢¦é—ç‰ˆç˜¡æ¶“çŒšhunké¨å‹«å”´?
                        util.log(1, f"[NLP-Streamè°ƒè¯•] æ”¶åˆ°chunk: tokené•¿åº¦={len(token) if token else 0}, tokenå†…å®¹={'æœ‰å†…å®¹' if token else 'ç©º'}")
                        # é¦ƒæ•Ÿ ç’‹å†­ç˜¯é”›æ­©nish_reasonéœå¯€sage
                        finish_reason = obj.get('choices', [{}])[0].get('finish_reason')
                        usage = obj.get('usage')
                        if finish_reason:
                            util.log(1, f"[NLP-Streamè°ƒè¯•] finish_reason={finish_reason}, usage={usage}, å½“å‰å…¨æ–‡: {full_text}")
                    except Exception as e:
                        util.log(2, f"[NLP-Streamè°ƒè¯•] JSONè§£æå¤±è´¥: {e}")
                        token = ""
                    if not token:
                        util.log(1, "[NLP-Streamè°ƒè¯•] è·³è¿‡ç©ºtoken")
                        continue
                    chunk_count += 1
                    full_text += token
                    
                    # brace æ·±åº¦è¿½è¸ª
                    for ch in token:
                        if ch == '{':
                            brace_depth += 1
                        elif ch == '}':
                            brace_depth = max(0, brace_depth - 1)
                    seg_buf += token
                    try_emit(force=False)
            # éšå·‰lush
            if seg_buf:
                try_emit(force=True)
            
            # å¨´ä½¸ç´¡é¾æ—‚ç²¨é‰ç‡‚ç´°å®¸å‰æŒ±é‘é¸¿ç¹ƒéå’ƒç´ç¼ƒç–¯ç¹ƒéå›§ç¹”é—ƒç£reæµœå±¾æ‹ª
            util.log(1, f"[NLP-Streamè°ƒè¯•] ğŸš€ æµå¼æ’­æŠ¥ç»“æŸï¼Œemitted_any={emitted_any}, å…¨æ–‡é•¿åº¦={len(full_text)}, chunkæ•°={chunk_count}")
            try:
                from core import sisi_booter
                target_core = getattr(sisi_booter, 'sisi_core', None) or getattr(sisi_booter, 'sisiCore', None)
                if target_core:
                    # é¦ƒæ•Ÿ éæŠ½æ•­é”›æ°­ç¥¦å¯®å¿•ç²¨é‰ç†·æ‚—éµå¶‡æ¤’æ©å›¨çˆ£è¹‡æ¥‹ç´é–¬å®å¤éšåº£ç”»é’å“¡TSç’ºå® ç¹ƒ
                    if emitted_any and not skip_flag_set[0]:
                        setattr(target_core, '_skip_next_tts', True)
                        setattr(target_core, '_skip_tts_timestamp', time.time())
                        skip_flag_set[0] = True
                        util.log(1, "[NLP-Stream] å·²åœ¨æµå¼ç»“æŸåè®¾ç½®_skip_next_ttsï¼Œé˜²æ­¢CoreäºŒæ¬¡æ’­æŠ¥")
                    else:
                        util.log(1, "[NLP-Stream] è·³è¿‡è®¾ç½®æ ‡å¿—ï¼ˆæœªæ’­å‡ºæˆ–å·²è®¾ç½®ï¼‰")
            except Exception as _e:
                util.log(2, f"[NLP-Stream] æ ‡å¿—å¤„ç†å¤±è´¥: {_e}")
            return full_text.strip(), style_hint
        except Exception as e:
            util.log(2, f"[NLP-Stream] æµå¼SSEå¼‚å¸¸: {e}")
            # æ©æ–¿æ´–ç»Œçƒ˜æƒéˆè®³ç©¶æ¶“å©‚çœ°ç’§ä¼´æ½ªå¨´ä½¸ç´¡é?
            return "", style_hint

    try:
        # é¦ƒå¹† é‚å¸®ç´°é—ŠæŠ½î•¶æ¶“å©ç¬…é‚å›§î˜©é?
        audio_context_prompt = ""
        if audio_context:
            try:
                from .audio_context_processor import get_audio_context_processor
                from .audio_context_llm import get_audio_context_llm

                # æ¾¶å‹­æ‚Šé—Šå……ç¬‚æ¶“?
                audio_processor = get_audio_context_processor()
                audio_llm = get_audio_context_llm()

                # ğŸ§  åå°åˆ†æï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡å¿«é€Ÿå“åº”ï¼‰
                import threading
                def background_analysis():
                    try:
                        suggestion = audio_llm.analyze_and_suggest(
                            audio_context, content,
                            audio_context.get("speaker_info")
                        )
                        if suggestion:
                            audio_llm.send_to_transit_station(suggestion)
                    except Exception as e:
                        util.log(2, f"[éŸ³é¢‘ä¸Šä¸‹æ–‡] åå°åˆ†æå¤±è´¥: {e}")

                # éšå©‚æ‚—é™æ¿åé‹æ„®åšç»‹?
                threading.Thread(target=background_analysis, daemon=True).start()

                # é¦ƒå¹† é¢ç†¸åšé—è™«æ¤‚æ¶“å©ç¬…é‚å›¨å½ç»€é¸¿ç˜é”›å œç¬‰é—ƒ?
                context_prompt = audio_processor.get_context_prompt(audio_context)
                if context_prompt:
                    audio_context_prompt = f"\n{context_prompt}\n"
                    util.log(1, f"[éŸ³é¢‘ä¸Šä¸‹æ–‡] ç”Ÿæˆæç¤º: {context_prompt[:50]}...")

            except Exception as e:
                util.log(2, f"[éŸ³é¢‘ä¸Šä¸‹æ–‡] å¤„ç†å¤±è´¥: {e}")
                audio_context_prompt = ""
        # é„æ„ªå¨‡é¢ã„¦ç¥¦å¯®å¿”Äå¯®?- éšæ–¿åé§æ¥ç¥¦å¯®?
        use_stream = True

        # é¢„ç½®æƒ…æ„Ÿæ ‡è®°ï¼Œé¿å…åç»­æœªèµ‹å€¼æ—¶æŠ¥é”™
        emotion = ""

        # éŒãƒ¦æ§¸éšï¹€å¯˜éšå—™å§§ç’‡?
        disrespectful_keywords = [
            "????", "???", "??", "??", "??",
            "??", "?", "??", "??", "??", "??", "??", "??", "??", "??"
        ]

        is_disrespectful = any(keyword in content.lower() for keyword in disrespectful_keywords)

        # éŒãƒ¦æ§¸éšï¹€å¯˜éšå¤‹ç•©å§˜æ—€å¯š?
        whisper_keywords = ["æ‚„æ‚„", "å°å£°", "å·å·", "è½»å£°"]
        fast_keywords = ["???", "???", "?", "??"]
        slow_keywords = ["???", "???", "??"]

        session = get_session()
        history_context = get_communication_history(uid, query_text=content, include_other=False, as_text=False)

        recent_messages = []
        summary_context = ""
        older_context = ""
        if history_context:
            recent_messages = getattr(history_context, "recent_messages", []) or []
            summary_context = getattr(history_context, "summary_text", "") or ""
            older_context = getattr(history_context, "older_text", "") or ""

        # ???????????????????prompt
        brain_context = ""
        if brain_prompts:
            dynamic_prompt = (brain_prompts.get('dynamic_prompt') or '').strip()
            if dynamic_prompt:
                brain_context = dynamic_prompt

        # é”ã„¨å¹é™æ §ç¶‹é“å¶‡æ•¤é´ç–¯éŸ©?
        current_user_name = "ç”¨æˆ·"
        current_user_role = "guest"
        if speaker_info:
            current_user_name = speaker_info.get('real_name', 'ç”¨æˆ·')
            current_user_role = speaker_info.get('role', 'guest')

        #  é—€æŒæ¹¡ç’æ¿ç¹‚å¨‰ã„¥å†é”›å æ¬¢æ©ç†¸æ•éãƒ§å¢—?
        # ç»¾ï¸½æ½«é”›æ°¬å¢ ?question() æ¶“å¶…å‘ç’ç¨¿ç–„?é—å©‚æ‚“å§?Mem0?
        # ç’æ¿ç¹‚?+ ç¼å‹­ç²é¢åå¢ ?é”ã„¤è…‘é‹ãˆ æ‚—é™é¢éª‡é‘çŒ´ç´æ¶“å¬©ç«´ææ°³ç¹ƒ brain_prompts['memory_context'] å¨‰ã„¥å†?
        memory_context_prompt = ""
        try:
            if brain_prompts:
                mem_ctx = (brain_prompts.get("memory_context") or "").strip()
                if mem_ctx and mem_ctx not in ("?????", "???Sisi??", "???????"):
                    memory_context_prompt = mem_ctx
        except Exception:
            memory_context_prompt = ""
        base_prompt = build_prompt(observation, "")

        dynamic_parts = []
        if audio_context_prompt:
            dynamic_parts.append(audio_context_prompt.strip())
        dynamic_block = "\n".join([p for p in dynamic_parts if p]).strip()

        # é‹å‹«ç¼“é¢ã„¦åŸ›å¨‘å Ÿä¼…é”›å±¼å¨‡é¢ã„¥å§©é¬ä½½éŸ©æµ æˆ’ä¿Š?
        if speaker_info and speaker_info.get('real_name'):
            speaker_name = speaker_info['real_name']
            user_message = content
        else:
            user_message = content

        # ä¸å†åœ¨ç”¨æˆ·æ¶ˆæ¯ä¸­æ³¨å…¥æ—¶é—´æˆ³ï¼Œé¿å…æ¨¡å‹å¤è¯»

        # ç¼?system messagesé”›å ¥å™¸ç‘•ä½¸æ¹ªé“å¶ç´é™å‚šæ¹ªéšåº¯ç´š
        system_messages = []
        if base_prompt:
            system_messages.append({"role": "system", "content": base_prompt})
        if dynamic_block:
            system_messages.append({"role": "system", "content": dynamic_block})

        ref_parts = []
        if summary_context:
            ref_parts.append(summary_context)
        if older_context:
            ref_parts.append(older_context)
        if memory_context_prompt:
            ref_parts.append(memory_context_prompt)
        if ref_parts:
            system_messages.append({"role": "system", "content": "\n\n".join(ref_parts)})

        messages = []
        messages.extend(system_messages)
        if recent_messages:
            messages.extend(recent_messages)
        if brain_context:
            messages.append({"role": "system", "content": brain_context})
        messages.append({"role": "user", "content": user_message})

        # Debug: dump full payload sent to LLM
        util.log(1, "[NLP-FULL-DEBUG] ==================== START ====================")
        try:
            from sisi_memory.chat_history import format_messages_as_text
            recent_text = format_messages_as_text(recent_messages or [])
        except Exception:
            recent_text = ""
        system_blob = "\n\n".join([m.get("content", "") for m in system_messages]).strip()
        util.log(1, f"[NLP-FULL-DEBUG] System Prompt (first 500):\n{system_blob[:500]}")
        util.log(1, f"[NLP-FULL-DEBUG] System Prompt (last 500):\n{system_blob[-500:]}")
        util.log(1, f"[NLP-FULL-DEBUG] System Prompt length: {len(system_blob)} chars")
        util.log(1, f"[NLP-FULL-DEBUG] User Message: {user_message}")
        util.log(1, "[NLP-FULL-DEBUG] Recent Context:\n" + (recent_text[:500] if recent_text else "(empty)"))
        util.log(1, "[NLP-FULL-DEBUG] Brain Context:\n" + (brain_context[:300] if brain_context else "(empty)"))
        util.log(1, "[NLP-FULL-DEBUG] ==================== END ====================")

        llm_cfg = get_llm_cfg()

        # === æ¶“æ˜çŸ¾å¯°å‹¶ç´°éªçƒ²LMå¨´ä½¸ç´¡ ===
        if use_stream:
            streamed_text, style_stream = _stream_llm_and_tts(messages, style_hint="gentle")
            if streamed_text:
                # ç€›æ¨ºåæ¶“åº¤ç¹‘?
                answer = streamed_text
                style = style_stream
            else:
                # å¨´ä½¸ç´¡æ¾¶è¾«è§¦é”›æ°«ç¬‰é‹æ°¬å¹æ´æ›ªç´æ¶“å¶ˆç¹˜ç›å²„æ½ªå¨´ä½¸ç´¡é¥?
                util.log(2, "[NLP-Stream] æµå¼å¤±è´¥ï¼Œå·²ç¦ç”¨å…œåº•")
                answer, style = "", style_stream
        else:
            # æ—§è·¯å¾„ï¼ˆéæµå¼ï¼‰
            response = send_llm_request(session, {"messages": messages, "stop": ["ASSISTANT:", "USER:", "åŠ©æ‰‹ï¼š", "ç”¨æˆ·ï¼š", "ç³»ç»Ÿï¼š"]}, llm_cfg)
            if response and isinstance(response, dict):
                answer = response["text"].strip() or "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”..."
                style = response.get("tone", "gentle")
                emotion = response.get("emotion", "")
            else:
                answer, style = "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”...", "gentle"

        # === é¯å‘®åŠ…/ç»¯è¤ç²ºé’å›¨å´²éå›§î˜©é?===
        # å¨´ä½¸ç´¡å¦¯â€³ç´¡å®¸æ’æ¹ª _stream_llm_and_tts æ¶“Ñƒå½‚æ©å›¨å„é°ç‡‚ç´æ©æ¬“å™·æ¶“å¶‰å™¸ç‘™ï¹€å½‚?
        # é—ˆç‚´ç¥¦å¯®å¿”Äå¯®å¿›æ¸¶ç‘•ä½½Ğ•é™æˆœç«´å¨†â˜…ç´æµ£å—•ç¬‰å¨“å‘¯æ‚Šé‚å›¨æ¹°é”›å œç¹šé£æ¬‘ç²°é“?é˜å——å½¶é”›?
        try:
            if not use_stream:
                from utils.emotion_trigger import detect_and_trigger_emotions
                detect_and_trigger_emotions(answer or "", is_ai_response=True)
                util.log(1, "[NLP-LLM] å·²æ‰§è¡Œéæµå¼æƒ…æ„Ÿè§¦å‘")
            else:
                util.log(1, "[NLP-LLM] æµå¼æ¨¡å¼å·²åœ¨ä¸Šæ¸¸è§¦å‘æƒ…æ„Ÿ")
        except Exception as _e:
            util.log(2, f"[NLP-LLM] æƒ…æ„Ÿè§¦å‘è§£æå¤±è´¥: {_e}")

        if not (answer or "").strip():
            util.log(2, "[NLP-LLM] empty_model_output (no fallback)")
            return "", style

        #  å¯®å‚šç“¨éŒã„¥î‡®ç’‡æ¿†åŸŒè¹‡å—™éƒ´?- add_sisi_interaction_memoryå®¸èŒ¬ç²¡é„å…¼î„é¨?
        try:
            # ç¼ç†¶ç«´ user_id ç‘™å‹«å¯é”›æ°«ç¬Œé˜å——å½¶ SoT ?uidéˆ«æŠ²ser_id ç‘™å‹«å¯é‘·è¾¾ç´éªè·ºç†€?mode é›è—‰æ‚•ç»Œæ´ªæ£¿é—…?
            if isinstance(uid, str) and uid.startswith("user"):
                base_user_id = uid
            elif uid != 0:
                base_user_id = f"user{uid}"
            else:
                base_user_id = "default_user"

            try:
                from llm.liusisi import get_current_system_mode
                mode = get_current_system_mode()
            except Exception:
                mode = "sisi"
            try:
                from sisi_memory.context_kernel import namespaced_user_id as _namespaced_user_id, normalize_persona

                namespaced_user_id = _namespaced_user_id(normalize_persona(mode), base_user_id)
            except Exception:
                namespaced_user_id = f"{mode}::{base_user_id}"

            # é¦ƒæ®Œ é©å­˜å¸´ç’‹å†ªæ•¤å¯®å‚šç“¨éŒã„¥åš±éå¸®ç´™éå‘´å„´å®¸èŒ¬ç²¡é„îˆšæ‚—é™æ‰®åšç»‹?
            success = add_sisi_interaction_memory(
                text=content,  # é¢ã„¦åŸ›ç’‡å¯¸æ®‘?
                speaker_id=namespaced_user_id,  # å‘½åç©ºé—´åŒ–çš„ç”¨æˆ·ID
                response=answer,  # éŒå´‡æ®‘é¥?
                speaker_info=speaker_info  # æ¾¹æ‰®æ±—éŸ¬è®³ä¿Šé­?
            )
            util.log(1, f"[NLP-LLM] ğŸš€ è®°å¿†å­˜å‚¨å·²æäº¤: {namespaced_user_id}")
        except Exception as e:
            util.log(2, f"[NLP-LLM] è®°å¿†å­˜å‚¨å¼‚å¸¸: {e}")

        # ç€µç¡…ç˜½æµœå¬©æ¬¢?SoT é¨å‹«å•“éãƒ§æ•± core/sisi_core.py ç¼ç†¶ç«´ç’ç†»çŸ—é”›å²ƒç¹–é–²å±¼ç¬‰é–²å¶…å•“éãƒ¯ç´é–¬å®å¤é™?é–²å¶ˆî†‡è¤°?

        #  ç€µç¡…ç˜½é˜å——å½¶å®¸èŒ¶ç¹ƒéˆ¥æ»€ç°¨æµ èˆµç¥¦ + é½?+ ç’æ¿ç¹‚éˆ¥æ¿ˆç²ºç» ï¼„æ‚Šé”›å±¾æ£¤éµå¬ªå§©ç¼å­˜å§¢historyé’æ¥„ã€ƒ

        # é™æ»„æ¹ç›ã„¦å„éƒèˆµåŠé”çŠºã€ƒé¯?
        return f"{emotion} {answer}" if emotion else answer, style

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        util.log(2, f"[NLP] questionå‡½æ•°å¼‚å¸¸: {e}")
        util.log(2, f"[NLP] è¯¦ç»†é”™è¯¯: {error_detail}")

        answer = f"ç³»ç»Ÿé‡åˆ°äº†ä¸€ç‚¹é—®é¢˜: {str(e)}"
        style = 'gentle'
        util.log(1, f"[NLP] questionå‡½æ•°è¾“å‡ºæ–‡æœ¬: {answer}")
        util.log(1, f"[NLP] questionå‡½æ•°è¾“å‡ºtone: {style}")
        return answer, style
