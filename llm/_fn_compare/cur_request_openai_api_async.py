async def request_openai_api_async(text: str, uid=0, observation: str = ''):
    """
    å¯®å‚šî˜©éå—šî‡¬å§¹å‚¦ç´é€îˆ›å¯”éªæƒ°î”‘ç’‹?

    Args:
        text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        uid: ç”¨æˆ·ID
        observation: ç¯å¢ƒè§‚å¯Ÿä¿¡æ¯

    Returns:
        (å›ç­”æ–‡æœ¬, é£æ ¼)
    """
    try:
        # è®°å½•è°ƒç”¨
        util.log(1, f"[LLMæ¨¡å‹] å¼‚æ­¥å¤„ç†è¯·æ±‚: {text}")

        # éŒãƒ¥ä¼éç–¯çšŸ?
        if is_tool_call_quick(text):
            # é—æ›ä¼éç–¯çšŸé¢è‚©æ´¿éºãƒ¥î˜©é?
            tool_result = process_with_tools_sync(text, uid)
            if tool_result:
                return tool_result, "llm"

        # é’æ¶˜ç¼“æµ¼æ°³ç˜½éªèˆµç€¯å¯¤çƒ˜çœ°é?
        session = get_session()
        history_context = get_communication_history(uid, query_text=text, as_text=True)

        #  é‹å‹«ç¼“é–å‘­æƒˆé˜å——å½¶æ¶“å©ç¬…é‚å›©æ®‘é»æ„®ãš?
        if isinstance(history_context, str) and history_context.strip() and history_context not in ("æ— è¯å†å²", "æ— å†å²"):
            enhanced_prompt = build_prompt(observation) + f"\n\n{history_context}"
        else:
            enhanced_prompt = build_prompt(observation)

        llm_cfg = get_llm_cfg()
        data = {
            "model": llm_cfg["model"],
            "messages": [
                {"role": "system", "content": enhanced_prompt},
                {"role": "user", "content": text}
            ],
            "temperature": 0.5,
            "max_tokens": 1000,
            "top_p": 0.6,
            "stream": False
        }

        # é™?
        url = llm_cfg["base_url"] + "/chat/completions"
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f"Bearer {llm_cfg['api_key']}"
        }

        # å¯®å‚šå½‚?
        async def async_request():
            # ç’å‰§ç–†ç“’å‘®æ¤‚é”›å±¼ç¬Œéšå²ƒè‚ªæ·‡æ¿‡å¯”?
            timeout = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession(timeout=timeout) as client_session:
                async with client_session.post(url, json=data, headers=headers) as response:
                    if response.status == 200:
                        result = await response.json()

                        if "choices" not in result or not result["choices"]:
                            return "è®©æˆ‘æƒ³æƒ³è¯¥æ€ä¹ˆå›ç­”...", "gentle"

                        content = result["choices"][0]["message"]["content"]

                        # æˆæ’³åš­mojié¨å‡©LMæ©æ–¿æ´–é?
                        util.log(1, f"[LLM] ğŸ¤– {content} ğŸ¤–")

                        # é©å­˜å¸´æ¾¶å‹­æ‚Šé‚å›¨æ¹°é?
                        text = content.strip()

                        # æ”¹è¿›å‰ç¼€æ¸…ç†é€»è¾‘ï¼Œå¤„ç†æ›´å¤šå¯èƒ½çš„å‰ç¼€æƒ…å†µ
                        # å¸¸è§çš„é”™è¯¯å‰ç¼€æ¨¡å¼åˆ—è¡¨
                        prefix_patterns = [
                            "è•¯ignment:", "alignment:", "ç€µå½’ç¶ˆ:", "é¥ç‚µç“Ÿ:", "é¥?", "assistant:",
                            "ai:", "response:", "ç»›?", "ç»›?"
                        ]

                        # æ£€æŸ¥å¹¶ç§»é™¤å·²çŸ¥å‰ç¼€
                        text_lower = text.lower()
                        for prefix in prefix_patterns:
                            if text_lower.startswith(prefix.lower()):
                                # æ‰¾åˆ°å†’å·åçš„ä½ç½®
                                colon_pos = text.find(':')
                                if colon_pos > 0:
                                    text = text[colon_pos + 1:].strip()
                                    break

                        # æ¿¡å‚›ç‰é‚å›¨æ¹°æµ ãƒ¨ã€ƒé¯å‘­å½¿å¯®â‚¬æ¾¶è¾¾ç´æ¶”ç†·çš¾ç’‡æ›Ÿç«»?
                        if text and text[0] in ["ğŸ¤«", "ğŸ˜", "ğŸ˜ ", "ğŸ¤–"]:
                            text = text[1:].strip()

                        # å¨´å¬«å„ç¼ç¡…î†•ç¼ƒî†¾æµ‰æ´æ–¿å¼¬é?
                        tone = "gentle"  # é»˜è®¤æ¸©å’Œè¯­æ°”

                        # å¨´å¬«åŠ‹é¬æ“å„?
                        if "ğŸ˜ " in text:
                            tone = "angry"
                        # æ£€æµ‹æ‚„æ‚„è¯æƒ…ç»ª
                        elif "ğŸ¤«" in text:
                            tone = "whisper"

                        # é¦ã„¦æ£©è¹‡æ¤¾è…‘éå›¨Äé¨å¬«æ½µå©§æ„¶ç´æµ£å—•ç¬‰æ·‡î†½æ•¼ç€¹ç‚ºæª¯é¥ç‚²î˜²é?
                        log_text = f"[NLP-7B] {text}"
                        util.log(1, f"[LLM] ğŸ¤– {log_text} ğŸ¤–")

                        return text, tone
                    else:
                        error_text = await response.text()
                        util.log(2, f"[LLM] APIé”™è¯¯: çŠ¶æ€ç  {response.status}, é”™è¯¯ä¿¡æ¯: {error_text}")
                        return f"æŠ±æ­‰ï¼ŒAPIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}", "gentle"

        # çæ¿Šç˜¯ç€µç…å†aiohttpé”›å±¾ç‰éãƒ¥ã‘ç’ãƒ¥å¯æµ£è·¨æ•¤éšå±¾æŸŸå¨‰?
        try:
            import aiohttp
            return await async_request()
        except ImportError:
            util.log(2, "[LLM] aiohttpæ¨¡å—æœªå®‰è£…ï¼Œä½¿ç”¨åŒæ­¥æ–¹æ³•")
            # æµ£è·¨æ•¤éšå±¾æŸŸå¨‰æ›ªç´æµ£å—˜åŠé”çŠºç§´éƒèˆµå¸¶é’?
            with concurrent.futures.ThreadPoolExecutor() as executor:
                try:
                    # å¨£è¯²å§ç“’å‘®æ¤‚éºÑƒåŸ—é”›å±¾æ•¼?0ç»‰æ”â€˜æ·‡æ¿‡æ¹ç“’å´‡æ®‘éå—˜æ¤‚?
                    future = executor.submit(send_llm_request, session, data, llm_cfg)
                    response_tuple = future.result(timeout=10)  # æ¾§ç‚²å§ç“’å‘®æ¤‚éƒå •æ£¿?0?

                    if isinstance(response_tuple, tuple) and len(response_tuple) == 2:
                        return response_tuple
                    else:
                        # çº­èƒ¯ç¹‘é¥ç‚²å“ç¼å‹¬ç‰¸å¯®?
                        if isinstance(response_tuple, dict):
                            return response_tuple.get("text", "æ¨¡å‹è¿”å›ä¸ºç©º"), response_tuple.get("tone", "gentle")
                        elif isinstance(response_tuple, str):
                            return response_tuple, "gentle"
                        else:
                            return "æŠ±æ­‰ï¼Œå“åº”æ ¼å¼ä¸æ­£ç¡®", "gentle"
                except concurrent.futures.TimeoutError:
                    util.log(2, "[LLM] è¯·æ±‚è¶…æ—¶")
                    return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•", "gentle"
        except Exception as e:
            util.log(2, f"[LLM] å¼‚æ­¥è¯·æ±‚å¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œè¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}", "gentle"

    except Exception as e:
        import traceback
        error_msg = f"LLMæ¨¡å‹å¼‚æ­¥å¤„ç†å¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"
        util.log(2, error_msg)
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é—®é¢˜: {str(e)}", "gentle"
