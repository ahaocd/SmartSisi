"""
ä½¿ç”¨LangGraphæ¡†æ¶å®ç°å·¥å…·è°ƒç”¨å›¾
è¿™ä¸ªæ¨¡å—ä¸ç°æœ‰çš„å¿«é€Ÿå“åº”NLPç³»ç»Ÿå¹¶è¡Œå·¥ä½œï¼Œå¤„ç†éœ€è¦å·¥å…·è°ƒç”¨çš„å¤æ‚ä»»åŠ¡
"""

import os
import logging
import json
import sys
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple, Union, Annotated, TypedDict, Sequence, Callable, cast

# æ·»åŠ æœ¬åœ°langgraphé¡¹ç›®åˆ°Pythonè·¯å¾„
sys.path.append('e:/liusisi')

# LangGraphå¯¼å…¥
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# åˆ›å»ºå…¼å®¹çš„MessageStateç±» - ç”±äºå¯¼å…¥é—®é¢˜
class MessageState(TypedDict):
    """å…¼å®¹LangGraphçš„æ¶ˆæ¯çŠ¶æ€"""
    messages: List  # æ¶ˆæ¯åˆ—è¡¨
    
# ä¿®æ­£å¯¼å…¥è·¯å¾„
from llm.agent.tool_node import ToolNode

# æ·»åŠ MemorySaverå¯¼å…¥
from langgraph.checkpoint.memory import MemorySaver

# å¯¼å…¥cast_state_schemaæˆ–å®šä¹‰å…¼å®¹å‡½æ•°
def cast_state_schema(schema_type):
    """å°†ç±»å‹è½¬æ¢ä¸ºå›¾çŠ¶æ€æ¨¡å¼"""
    # å®é™…ä¸Šï¼Œè¿™åº”è¯¥ä½¿ç”¨LangGraphçš„cast_toå‡½æ•°
    # ä½†æˆ‘ä»¬è¿™é‡Œæä¾›ä¸€ä¸ªç®€å•å®ç°ä½œä¸ºå…¼å®¹å±‚
    return schema_type

# è‡ªå®šä¹‰tools_conditionå‡½æ•°
def tools_condition(state):
    """
    æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
    
    Args:
        state: AgentçŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯åˆ—è¡¨
    
    Returns:
        bool: å¦‚æœæœ‰å·¥å…·è°ƒç”¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    # è·å–æ¶ˆæ¯åˆ—è¡¨
    messages = state.get("messages", [])
    if not messages:
        return False
    
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦ä¸ºAIæ¶ˆæ¯ä¸”åŒ…å«å·¥å…·è°ƒç”¨
    last_message = messages[-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return True
    
    return False

# æ·»åŠ ç¼ºå¤±çš„å‡½æ•°
def should_continue(state):
    """å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œå·¥å…·è°ƒç”¨æˆ–ç»“æŸå¯¹è¯"""
    # ç®€åŒ–çš„å†³ç­–é€»è¾‘
    messages = state.get("messages", [])
    if not messages:
        return "end"
    
    last_message = messages[-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"

def get_tool_calling_prompt(config, tool_descs):
    """åˆ›å»ºå·¥å…·è°ƒç”¨æç¤º"""
    system_prompt = config.get("system_prompt", """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚
å½“éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œè¯·ä½¿ç”¨æä¾›çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚

é‡è¦æç¤ºï¼šå¦‚æœéœ€è¦åŒæ—¶ä½¿ç”¨å¤šä¸ªå·¥å…·ï¼Œè¯·ä¸€æ¬¡æ€§åˆ—å‡ºæ‰€æœ‰éœ€è¦çš„å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼ï¼š
```json
[
  {"action": "å·¥å…·åç§°1", "action_input": "è¾“å…¥å‚æ•°1"},
  {"action": "å·¥å…·åç§°2", "action_input": "è¾“å…¥å‚æ•°2"}
]
```

å•ä¸ªå·¥å…·è°ƒç”¨è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
```json
{"action": "å·¥å…·åç§°", "action_input": "è¾“å…¥å‚æ•°"}
```

å¯ç”¨å·¥å…·åŒ…æ‹¬:
{tool_descriptions}
""").format(tool_descriptions="\n".join(tool_descs))
    
    return system_prompt

def create_runnable(llm, prompt):
    """åˆ›å»ºå¯æ‰§è¡Œçš„èŠ‚ç‚¹"""
    def runnable(state):
        """LLMèŠ‚ç‚¹å¤„ç†å‡½æ•°"""
        # è·å–æ¶ˆæ¯å†å²
        messages = state.get("messages", [])
        
        # è®°å½•è°ƒç”¨ä¿¡æ¯
        print(f"[ToolCallingGraph] LLMèŠ‚ç‚¹æ”¶åˆ° {len(messages)} æ¡æ¶ˆæ¯")
        
        # è°ƒç”¨llmå¤„ç†é€»è¾‘
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„LLM
            # ç®€åŒ–å®ç°ï¼Œç›´æ¥è¿”å›ä¸€ä¸ªAIMessage
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content="æˆ‘æ˜¯Assistantï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ")
            
            # å°†AIæ¶ˆæ¯æ·»åŠ åˆ°å†å²ä¸­
            messages.append(ai_message)
            return {"messages": messages}
        except Exception as e:
            print(f"[ToolCallingGraph] LLMè°ƒç”¨å‡ºé”™: {str(e)}")
            # è¿”å›é”™è¯¯æ¶ˆæ¯
            from langchain_core.messages import AIMessage
            error_message = AIMessage(content=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
            messages.append(error_message)
            return {"messages": messages}
    
    return runnable

# LangChainå¯¼å…¥
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage, FunctionMessage, ToolMessage
from langchain_core.tools import BaseTool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser

# å¯¼å…¥SmartSisié¡¹ç›®ç°æœ‰å·¥å…·
from llm.agent.tools.Weather import Weather
from llm.agent.tools.TimeQuery import TimeQuery
from llm.agent.tools.ToRemind import ToRemind
from llm.agent.tools.QueryTimerDB import QueryTimerDB
from llm.agent.tools.DeleteTimer import DeleteTimer

# å¯¼å…¥é¡¹ç›®å·¥å…·å‡½æ•°
from utils import util
from utils import config_util as cfg
from llm.liusisi import question, chat

# å®šä¹‰çŠ¶æ€ç±»å‹
class AgentState(TypedDict):
    """AgentçŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[BaseMessage], add_messages]
    observation: Optional[str]
    uid: int
    tools: List[BaseTool]
    meta: Dict[str, Any]

# å¯¼å…¥A2Aå·¥å…·èŠ‚ç‚¹
from llm.agent.a2a_adapter import A2AToolNode, create_a2a_tools

# ä½¿ç”¨ç±»å‹æç¤º
ToolNodeType = Union[ToolNode, A2AToolNode]

class ToolCallingGraph:
    """åŸºäºLangGraphçš„å·¥å…·è°ƒç”¨å›¾"""
    
    # å•ä¾‹æ¨¡å¼çš„å®ä¾‹å˜é‡
    _instance = None
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–å·¥å…·è°ƒç”¨å›¾"""
        self.config = config or {}
        self.uid = 0
        self.observation = "æ— è§‚æµ‹æ•°æ®"
        
        # åŠ è½½é…ç½®
        cfg.load_config()
        
        # åˆå§‹åŒ–å·¥å…·åˆ—è¡¨ï¼ˆè¿™é‡Œå·²åŒæ—¶åŠ è½½æ ‡å‡†å·¥å…·å’ŒA2Aå·¥å…·ï¼‰
        self.tools = self._init_tools()
        
        # åˆ›å»ºå†…å­˜å­˜å‚¨ - å¿…é¡»åœ¨æ„å»ºå›¾ä¹‹å‰åˆå§‹åŒ–
        self.memory = MemorySaver()
        self.thread_id = f"thread_{os.urandom(8).hex()}"
        
        # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
        self.tool_node = ToolNode(self.tools)
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹ - åˆå§‹åŒ–ç©ºå€¼ï¼Œåœ¨æ„å»ºå›¾æ—¶å®é™…åˆ›å»º
        self.llm = None
        
        # æ„å»ºçŠ¶æ€å›¾
        self.graph = self._build_graph()
        
        util.log(1, f"[ToolCallingGraph] å·¥å…·è°ƒç”¨å›¾å·²åˆå§‹åŒ–ï¼Œçº¿ç¨‹ID: {self.thread_id}")
    
    def _init_tools(self) -> List[BaseTool]:
        """åˆå§‹åŒ–å·¥å…·åˆ—è¡¨"""
        util.log(1, "[ToolCallingGraph] ğŸš€ å¼€å§‹åˆå§‹åŒ–å·¥å…·ç³»ç»Ÿ...")
        
        # 1. åŠ è½½æœ¬åœ°å·¥å…·
        tools = [
            Weather(),              # å¤©æ°”æŸ¥è¯¢å·¥å…·
            TimeQuery(),            # æ—¶é—´æŸ¥è¯¢å·¥å…·
            ToRemind(),             # æé†’å·¥å…·
            QueryTimerDB(),         # æŸ¥è¯¢å®šæ—¶å™¨æ•°æ®åº“
            DeleteTimer()           # åˆ é™¤å®šæ—¶å™¨
        ]
        
        util.log(1, "ğŸ“¦ æœ¬åœ°æ ¸å¿ƒå·¥å…·åŠ è½½å®Œæˆ:")
        for i, tool in enumerate(tools, 1):
            util.log(1, f"  {i}. {tool.name}: {tool.description[:50]}...")
        
        # 2. åŠ è½½A2Aå·¥å…·å¹¶åˆå¹¶åˆ°å·¥å…·åˆ—è¡¨ä¸­
        util.log(1, "[ToolCallingGraph] ğŸ”— æ­£åœ¨æ£€æŸ¥A2Aå·¥å…·è¿æ¥...")
        if self.config.get("a2a_server_url"):
            util.log(1, f"[ToolCallingGraph] ğŸŒ A2AæœåŠ¡å™¨åœ°å€: {self.config['a2a_server_url']}")
            a2a_tools = create_a2a_tools(self.config["a2a_server_url"])
            if a2a_tools:
                tools.extend(a2a_tools)
                util.log(1, f"[ToolCallingGraph] âœ… å·²åŠ è½½ {len(a2a_tools)} ä¸ªA2Aå·¥å…·")
                for tool in a2a_tools:
                    util.log(1, f"[ToolCallingGraph]   - A2Aå·¥å…·: {tool.name}")
            else:
                util.log(1, "[ToolCallingGraph] âš ï¸ æœªå‘ç°ä»»ä½•A2Aå·¥å…·")
        else:
            util.log(1, "[ToolCallingGraph] â„¹ï¸ æœªé…ç½®A2AæœåŠ¡å™¨ï¼Œè·³è¿‡A2Aå·¥å…·åŠ è½½")
        
        # è¾“å‡ºå·¥å…·åŠ è½½æ€»ç»“
        util.log(1, f"[ToolCallingGraph] ğŸ“Š å·¥å…·ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        util.log(1, f"[ToolCallingGraph] ğŸ”§ æ€»å…±åŠ è½½ {len(tools)} ä¸ªå·¥å…·:")
        for i, tool in enumerate(tools, 1):
            util.log(1, f"[ToolCallingGraph]   {i}. {tool.name}: {tool.description[:50]}...")
        
        util.log(1, f"[ToolCallingGraph] ğŸ¯ å¯ç”¨å·¥å…·åˆ—è¡¨: {', '.join(t.name for t in tools)}")
        return tools
    
    def _build_graph(self) -> StateGraph:
        """æ„å»ºçŠ¶æ€å›¾"""
        # åˆ›å»ºçŠ¶æ€å›¾
        graph = StateGraph(MessageState)
        
        # åŠ è½½GPTæ¨¡å‹
        from .model_adapter import SisiLLMAdapter
        from utils.config_util import Config 
        cfg = Config()
        
        self.llm = SisiLLMAdapter(model=cfg.get_value("model"), streaming=True)
        
        # æ”¶é›†å·¥å…·è¯´æ˜
        tools_desc = [f"{tool.name}: {tool.description}" for tool in self.tools]
        
        # åˆ›å»ºç³»ç»Ÿæç¤º
        system_prompt = get_tool_calling_prompt(self.config, tools_desc)
        
        # ä¸ºLLMåˆ›å»ºå¤„ç†èŠ‚ç‚¹
        llm_node = create_runnable(self.llm, system_prompt)
        
        # æ·»åŠ LLMèŠ‚ç‚¹
        graph.add_node("llm", llm_node)
        
        # æ·»åŠ å·¥å…·èŠ‚ç‚¹ - æ ‡å‡†å·¥å…·èŠ‚ç‚¹å·²åŒ…å«A2Aå·¥å…·
        graph.add_node("tools", self.tool_node)
        
        # è®¾ç½®å¼€å§‹èŠ‚ç‚¹
        graph.set_entry_point("llm")
        
        # æ·»åŠ æ¡ä»¶è¾¹ç¼˜
        graph.add_conditional_edges(
            "llm",
            tools_condition,
            {
                True: "tools",  # å¦‚æœéœ€è¦å·¥å…·è°ƒç”¨ï¼Œè½¬åˆ°å·¥å…·èŠ‚ç‚¹
                False: END  # å¦‚æœä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
            }
        )
        
        # ä»å·¥å…·èŠ‚ç‚¹åˆ°LLMèŠ‚ç‚¹çš„è¾¹ç¼˜
        graph.add_edge("tools", "llm")
        
        # æ·»åŠ èŠ‚ç‚¹è½¬æ¢ç›‘å¬
        @graph.on_transition()
        def log_transition(state, prev_step, current_step):
            """
            è®°å½•èŠ‚ç‚¹è½¬æ¢å¹¶å‘ä¸­è½¬ç«™å‘é€ä¸­é—´çŠ¶æ€
            
            Args:
                state: å½“å‰çŠ¶æ€
                prev_step: å‰ä¸€ä¸ªèŠ‚ç‚¹
                current_step: å½“å‰èŠ‚ç‚¹
            """
            try:
                print(f"[ToolCallingGraph] è½¬æ¢: {prev_step} -> {current_step}")
                util.log(1, f"[LGèŠ‚ç‚¹] è½¬æ¢: {prev_step} -> {current_step}")
                
                # å‘ä¸­è½¬ç«™å‘é€çŠ¶æ€è½¬æ¢ä¿¡æ¯å’Œä¸­é—´çŠ¶æ€
                if prev_step != current_step:
                    try:
                        # å°è¯•å¯¼å…¥ä¸­è½¬ç«™
                        from llm.transit_station import TransitStation
                        transit = TransitStation.get_instance()
                        
                        # æ·»åŠ èŠ‚ç‚¹è½¬æ¢é€šçŸ¥
                        transit.add_intermediate_state(
                            f"LangGraphèŠ‚ç‚¹è½¬æ¢: {prev_step} -> {current_step}",
                            "LGèŠ‚ç‚¹è½¬æ¢"
                        )
                        
                        # æ ¹æ®ä¸åŒèŠ‚ç‚¹ç±»å‹ï¼Œå‘é€è¯¦ç»†çš„ä¸­é—´çŠ¶æ€ä¿¡æ¯
                        if current_step == "llm":
                            transit.add_intermediate_state("æ­£åœ¨æ€è€ƒæ‚¨çš„é—®é¢˜...", "LangGraphæ€è€ƒèŠ‚ç‚¹")
                            
                            # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
                            messages = state.get("messages", [])
                            if messages and len(messages) > 0:
                                last_msg = messages[-1]
                                if hasattr(last_msg, "content"):
                                    msg_content = last_msg.content[:100] + ("..." if len(last_msg.content) > 100 else "")
                                    transit.add_intermediate_state(
                                        f"æ€è€ƒå†…å®¹: {msg_content}",
                                        "LangGraphæ€è€ƒä¸Šä¸‹æ–‡"
                                    )
                                    
                        elif current_step == "tool_calling":
                            transit.add_intermediate_state("æ­£åœ¨åˆ†æéœ€è¦ä½¿ç”¨çš„å·¥å…·...", "LangGraphå·¥å…·é€‰æ‹©èŠ‚ç‚¹")
                            
                        elif current_step == "tools":
                            # æå–å·¥å…·ä¿¡æ¯
                            try:
                                messages = state.get("messages", [])
                                if messages:
                                    last_message = messages[-1]
                                    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                                        tool_names = [t.get("name", "æœªçŸ¥å·¥å…·") for t in last_message.tool_calls]
                                        tool_list = ", ".join(tool_names)
                                        transit.add_intermediate_state(
                                            f"æ­£åœ¨ä½¿ç”¨å·¥å…·: {tool_list}",
                                            "LangGraphå·¥å…·è°ƒç”¨èŠ‚ç‚¹"
                                        )
                                        
                                        # æ·»åŠ æ¯ä¸ªå·¥å…·çš„è¯¦ç»†ä¿¡æ¯
                                        for i, tool_call in enumerate(last_message.tool_calls):
                                            tool_name = tool_call.get("name", "æœªçŸ¥å·¥å…·")
                                            tool_args = tool_call.get("args", {})
                                            transit.add_intermediate_state(
                                                f"å·¥å…·{i+1} - {tool_name}: {json.dumps(tool_args, ensure_ascii=False)}",
                                                "LangGraphå·¥å…·è¯¦æƒ…"
                                            )
                            except Exception as tool_err:
                                print(f"[ToolCallingGraph] æå–å·¥å…·ä¿¡æ¯å‡ºé”™: {str(tool_err)}")
                        
                        # å·¥å…·ç»“æœèŠ‚ç‚¹
                        elif current_step.startswith("tool_result"):
                            transit.add_intermediate_state("å·²è·å¾—å·¥å…·ç»“æœï¼Œæ­£åœ¨å¤„ç†...", "LangGraphå·¥å…·ç»“æœèŠ‚ç‚¹")
                            
                            # å°è¯•æ·»åŠ å·¥å…·ç»“æœè¯¦æƒ…
                            try:
                                if state.get("observations"):
                                    result = state.get("observations")
                                    transit.add_intermediate_state(
                                        f"å·¥å…·è¿”å›ç»“æœ: {result[:200]}...",
                                        "LangGraphå·¥å…·ç»“æœè¯¦æƒ…"
                                    )
                            except Exception as result_err:
                                print(f"[ToolCallingGraph] æå–å·¥å…·ç»“æœå‡ºé”™: {str(result_err)}")
                    except Exception as transit_err:
                        print(f"[ToolCallingGraph] ä¸­è½¬ç«™è¿æ¥å‡ºé”™: {str(transit_err)}")
            except Exception as e:
                print(f"[ToolCallingGraph] è®°å½•è½¬æ¢å‡ºé”™: {str(e)}")
                util.log(3, f"[LGèŠ‚ç‚¹] è½¬æ¢å‡ºé”™: {str(e)}")
        
        # ç¼–è¯‘å›¾
        memory_graph = graph.compile()
        
        # è®°å½•æ—¥å¿—
        util.log(1, f"[ToolCallingGraph] å·¥å…·è°ƒç”¨å›¾å·²æ„å»º, {len(self.tools)} ä¸ªå·¥å…·å¯ç”¨")
        
        return memory_graph
        
    def invoke(self, query: str, uid: int = 0, observation: str = None) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        try:
            # è®¾ç½®ç”¨æˆ·IDå’Œè§‚æµ‹æ•°æ®
            self.uid = uid
            self.observation = observation or "æ— è§‚æµ‹æ•°æ®"
            
            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=query)],
                "uid": uid,
                "observation": self.observation,
                "tools": self.tools,
                "meta": {"query_time": time.time()}
            }
            
            # è°ƒç”¨å›¾æ¥å¤„ç†æŸ¥è¯¢
            result = self.graph.invoke(
                initial_state, 
                config={"configurable": {"thread_id": self.thread_id}}
            )
            
            # æå–å“åº”å†…å®¹
            messages = result.get("messages", [])
            if messages and len(messages) > 0:
                last_message = messages[-1]
                if isinstance(last_message, AIMessage):
                    return {
                        "response": last_message.content,
                        "has_tool_calls": hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0
                    }
            
            return {"response": "æ— æ³•å¤„ç†è¯·æ±‚", "has_tool_calls": False}
            
        except Exception as e:
            util.log(3, f"[ToolCallingGraph] å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
            return {"response": f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}", "has_tool_calls": False}

    @classmethod
    def get_instance(cls, config=None):
        """è·å–ToolCallingGraphå•ä¾‹"""
        if cls._instance is None:
            cls._instance = cls(config)
            util.log(1, f"[ToolCallingGraph] åˆ›å»ºæ–°çš„å·¥å…·è°ƒç”¨å›¾å®ä¾‹ï¼Œé…ç½®: {config}")
        else:
            util.log(1, f"[ToolCallingGraph] ä½¿ç”¨ç°æœ‰å·¥å…·è°ƒç”¨å›¾å®ä¾‹")
        return cls._instance

def process_query(query: str, uid: int = 0, observation: str = None) -> Dict[str, Any]:
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¾¿æ·å‡½æ•°"""
    graph = ToolCallingGraph.get_instance()
    return graph.invoke(query, uid, observation)

def build_agent_graph():
    """
    æ„å»ºLangGraph Agentå›¾
    
    è¯¥å›¾åŒ…å«ä»¥ä¸‹èŠ‚ç‚¹:
    - llm: å¤„ç†è¾“å…¥å¹¶å†³å®šä¸‹ä¸€æ­¥æ“ä½œ
    - tool_calling: å¤„ç†å·¥å…·è°ƒç”¨
    - end: ç»“æŸå¯¹è¯
    
    Returns:
        (compiled_graph, llm_node): ç¼–è¯‘åçš„å›¾å’ŒLLMèŠ‚ç‚¹çš„å¼•ç”¨
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    graph = StateGraph(AgentState)
    
    # æ·»åŠ LLMèŠ‚ç‚¹
    graph.add_node("llm", llm_node)
    
    # æ·»åŠ å·¥å…·è°ƒç”¨èŠ‚ç‚¹
    graph.add_node("tool_calling", tool_calling_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    graph.set_entry_point("llm")
    
    # æ·»åŠ æ¡ä»¶è¾¹
    graph.add_conditional_edges(
        "llm",
        route_next_step,
        {
            "tool": "tool_calling",
            "end": END
        }
    )
    
    # å·¥å…·è°ƒç”¨èŠ‚ç‚¹è¿”å›åˆ°LLMèŠ‚ç‚¹
    graph.add_edge("tool_calling", "llm")
    
    # æ·»åŠ æµå¼æ—¥å¿—æ‹¦æˆªå™¨ - è®°å½•æ¯æ¬¡èŠ‚ç‚¹é—´è½¬æ¢
    @graph.on_transition()
    def log_transition(state, prev_step, current_step):
        """è®°å½•èŠ‚ç‚¹è½¬æ¢å’Œå½“å‰çŠ¶æ€"""
        util.log(1, f"[LangGraphæµ] èŠ‚ç‚¹è½¬æ¢: {prev_step} â†’ {current_step}")
        
        # è®°å½•æœ€æ–°æ¶ˆæ¯çš„è¯¦ç»†å†…å®¹
        messages = state.get("messages", [])
        if messages and len(messages) > 0:
            last_msg = messages[-1]
            msg_type = type(last_msg).__name__
            
            # è®°å½•æ¶ˆæ¯ç±»å‹å’Œå†…å®¹å‰ç¼€
            if hasattr(last_msg, "content"):
                content = last_msg.content[:100] + ("..." if len(last_msg.content) > 100 else "")
                util.log(1, f"[LangGraphæµ] å½“å‰æ¶ˆæ¯({msg_type}): {content}")
            
            # è®°å½•å·¥å…·è°ƒç”¨è¯¦æƒ…
            if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                for tool_call in last_msg.tool_calls:
                    util.log(1, f"[LangGraphæµ] å·¥å…·è°ƒç”¨: {tool_call}")
    
    # ç¼–è¯‘çŠ¶æ€å›¾
    compiled_graph = graph.compile()
    
    return compiled_graph, llm_node

def llm_node(state: AgentState):
    """
    LLMèŠ‚ç‚¹å¤„ç†å‡½æ•°
    
    è¯»å–æ¶ˆæ¯å†å²ï¼Œæ·»åŠ å·¥å…·ä¿¡æ¯ï¼Œè°ƒç”¨LLMè·å–å“åº”
    """
    # è·å–æ¶ˆæ¯å†å²
    messages = state.get("messages", [])
    # å°†_init_tools()ç”¨ä½œget_tools()çš„æ›¿ä»£
    tools = state.get("tools", _init_tools_static())
    
    # è®°å½•çŠ¶æ€
    util.log(1, f"[LangGraph] LLMèŠ‚ç‚¹æ”¶åˆ° {len(messages)} æ¡æ¶ˆæ¯")
    
    # è®°å½•è¾“å…¥æ¶ˆæ¯è¯¦æƒ…
    if messages:
        last_msg = messages[-1]
        if hasattr(last_msg, "content"):
            util.log(1, f"[LangGraphæµ] LLMè¾“å…¥({type(last_msg).__name__}): {last_msg.content[:100]}...")
    
    # TODO: è°ƒç”¨å®é™…çš„LLMå¤„ç†å‡½æ•°
    # è¿™é‡Œåªæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå®ç°
    try:
        # ç§»é™¤ç¡¬ç¼–ç å¤©æ°”å…³é”®è¯åˆ¤æ–­
        # è°ƒç”¨æ­£ç¡®çš„LLMå¤„ç†é€»è¾‘
        from llm.agent.sisi_agent import call_llm
        
        # åˆ›å»ºç®€å•å“åº”ç”¨äºæµ‹è¯•
        response = "æˆ‘æ˜¯Assistantï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ"
        
        # åˆ›å»ºAIæ¶ˆæ¯
        ai_message = AIMessage(content=response)
        
        # æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
        messages.append(ai_message)
        
        # è®°å½•LLMçš„æ€è€ƒè¿‡ç¨‹å’Œè¾“å‡ºç»“æœ
        util.log(1, f"[LangGraphæµ] LLMæ€è€ƒè¿‡ç¨‹: {response[:150]}...")
        
        return {"messages": messages}
    except Exception as e:
        # è®°å½•é”™è¯¯
        util.log(3, f"[LangGraph] LLMèŠ‚ç‚¹å¤„ç†å‡ºé”™: {str(e)}")
        # åˆ›å»ºé”™è¯¯æ¶ˆæ¯
        error_message = AIMessage(content=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        messages.append(error_message)
        return {"messages": messages}

# æ·»åŠ é™æ€å·¥å…·åˆå§‹åŒ–å‡½æ•°
def _init_tools_static() -> List[BaseTool]:
    """åˆå§‹åŒ–å·¥å…·åˆ—è¡¨ï¼ˆé™æ€æ–¹æ³•ï¼‰"""
    util.log(1, "[Agentå·¥å…·] ğŸš€ å¼€å§‹åˆå§‹åŒ–å·¥å…·ç³»ç»Ÿ...")
    
    tools = [
        Weather(),              # å¤©æ°”æŸ¥è¯¢å·¥å…·
        TimeQuery(),            # æ—¶é—´æŸ¥è¯¢å·¥å…·
        ToRemind(),             # æé†’å·¥å…·
        QueryTimerDB(),         # æŸ¥è¯¢å®šæ—¶å™¨æ•°æ®åº“
        DeleteTimer()           # åˆ é™¤å®šæ—¶å™¨
    ]
    
    # è¾“å‡ºè¯¦ç»†çš„å·¥å…·å¯åŠ¨æ—¥å¿—
    util.log(1, f"[Agentå·¥å…·] âœ… æˆåŠŸåŠ è½½ {len(tools)} ä¸ªæ ¸å¿ƒå·¥å…·:")
    for i, tool in enumerate(tools, 1):
        util.log(1, f"[Agentå·¥å…·]   {i}. {tool.name}: {tool.description[:50]}...")
    
    util.log(1, f"[Agentå·¥å…·] ğŸ’¼ å·¥å…·ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼")
    util.log(1, f"[Agentå·¥å…·] ğŸ”§ å¯ç”¨å·¥å…·: {', '.join(t.name for t in tools)}")
    
    return tools

def tool_calling_node(state: AgentState):
    """
    å·¥å…·è°ƒç”¨èŠ‚ç‚¹å¤„ç†å‡½æ•°
    
    è§£æLLMè¾“å‡ºä¸­çš„å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨
    """
    # è·å–æ¶ˆæ¯å†å²å’Œå·¥å…·åˆ—è¡¨
    messages = state.get("messages", [])
    # ä½¿ç”¨_init_tools_staticæ›¿ä»£get_tools
    tools = state.get("tools", _init_tools_static())
    
    # è®°å½•å¯ç”¨å·¥å…·åˆ—è¡¨
    tool_names = [t.name for t in tools]
    util.log(1, f"[LangGraphæµ] å¯ç”¨å·¥å…·åˆ—è¡¨: {tool_names}")
    
    # è·å–æœ€åä¸€æ¡AIæ¶ˆæ¯
    if not messages:
        return state
    
    last_ai_message = None
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            last_ai_message = msg
            break
    
    if not last_ai_message:
        return state
    
    # è§£æå·¥å…·è°ƒç”¨
    content = last_ai_message.content
    util.log(1, f"[LangGraphæµ] è§£æå·¥å…·è°ƒç”¨: {content[:100]}...")
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æå·¥å…·è°ƒç”¨
    import re
    import json
    
    # æ”¶é›†æ‰€æœ‰å·¥å…·è°ƒç”¨
    tool_calls = []
    
    # 1. æå–å•ä¸ªå·¥å…·è°ƒç”¨ - ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼
    single_tool_matches = re.findall(r'```(?:json)?\s*\n?\s*(\{.*?"action":\s*"([^"]+)".*?"action_input":\s*(.+?)\s*\})\s*\n?```', content, re.DOTALL)
    for match_full, tool_name, tool_input_raw in single_tool_matches:
        try:
            # å°è¯•è§£æå®Œæ•´çš„JSON
            full_json = json.loads(match_full)
            if "action" in full_json and "action_input" in full_json:
                tool_calls.append({"name": full_json["action"], "input": full_json["action_input"]})
                util.log(1, f"[LangGraphæµ] è§£æåˆ°å•ä¸ªå·¥å…·è°ƒç”¨: {full_json['action']}")
        except:
            # å¦‚æœå®Œæ•´è§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™æå–çš„ç»“æœ
            try:
                # å°è¯•è§£æå·¥å…·è¾“å…¥
                if tool_input_raw.startswith('"') and tool_input_raw.endswith('"'):
                    # å­—ç¬¦ä¸²ç±»å‹çš„è¾“å…¥
                    tool_input = tool_input_raw.strip('"')
                else:
                    # å¯èƒ½æ˜¯å¯¹è±¡æˆ–å…¶ä»–JSONç»“æ„
                    tool_input = json.loads(tool_input_raw)
                
                tool_calls.append({"name": tool_name, "input": tool_input})
                util.log(1, f"[LangGraphæµ] è§£æåˆ°å•ä¸ªå·¥å…·è°ƒç”¨(é™çº§å¤„ç†): {tool_name}")
            except Exception as e:
                util.log(2, f"[LangGraphæµ] è§£æå·¥å…·è¾“å…¥å¤±è´¥: {str(e)}")
    
    # 2. æå–å¤šå·¥å…·æ•°ç»„æ ¼å¼ - æ›´çµæ´»çš„åŒ¹é…
    array_matches = re.findall(r'```(?:json)?\s*\n?\s*(\[.*?\])\s*\n?```', content, re.DOTALL)
    for array_json_str in array_matches:
        try:
            # è§£æJSONæ•°ç»„
            tool_array = json.loads(array_json_str)
            if isinstance(tool_array, list):
                for tool_obj in tool_array:
                    if isinstance(tool_obj, dict) and "action" in tool_obj and "action_input" in tool_obj:
                        tool_calls.append({"name": tool_obj["action"], "input": tool_obj["action_input"]})
                        util.log(1, f"[LangGraphæµ] è§£æåˆ°æ•°ç»„ä¸­çš„å·¥å…·è°ƒç”¨: {tool_obj['action']}")
        except Exception as e:
            util.log(2, f"[LangGraphæµ] è§£æJSONå·¥å…·æ•°ç»„å‡ºé”™: {str(e)}")
    
    # 3. æ£€æŸ¥ReActæ ¼å¼
    react_patterns = [
        r'Action\s*:\s*(\w+)\[(.*?)\]',
        r'è¡ŒåŠ¨\s*:\s*(\w+)\[(.*?)\]',
        r'åŠ¨ä½œ\s*:\s*(\w+)\[(.*?)\]'
    ]
    
    for pattern in react_patterns:
        react_matches = re.findall(pattern, content, re.DOTALL)
        for tool_name, args_str in react_matches:
            # å°è¯•è§£æå‚æ•°ä¸ºJSON
            try:
                args = json.loads(args_str)
            except:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œä½œä¸ºæ™®é€šå­—ç¬¦ä¸²å¤„ç†
                args = {"input": args_str.strip()}
            
            # ç”Ÿæˆå”¯ä¸€ID
            import time
            tool_call_id = f"call_{tool_name}_{int(time.time()*1000)}"
            
            tool_calls.append({"name": tool_name, "input": args})
            util.log(1, f"[LangGraphæµ] è§£æåˆ°ReActæ ¼å¼å·¥å…·è°ƒç”¨: {tool_name}")
    
    if not tool_calls:
        # æ²¡æœ‰æ‰¾åˆ°å·¥å…·è°ƒç”¨
        util.log(1, f"[LangGraphæµ] æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œå†…å®¹: {content[:200]}")
        return state
    
    # è®°å½•æ‰¾åˆ°çš„å·¥å…·è°ƒç”¨
    util.log(1, f"[LangGraphæµ] æ‰¾åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
    for i, call in enumerate(tool_calls):
        util.log(1, f"[LangGraphæµ] å·¥å…·è°ƒç”¨ #{i+1}: {call['name']} - å‚æ•°: {call['input']}")
    
    # å¯¼å…¥ä¸­è½¬ç«™
    try:
        from llm.transit_station import TransitStation
        transit = TransitStation.get_instance()
        transit.add_intermediate_state(
            f"æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨",
            "LangGraphå·¥å…·è°ƒç”¨èŠ‚ç‚¹"
        )
    except:
        pass
    
    # å¯¼å…¥å¼‚æ­¥åº“
    import asyncio
    from concurrent.futures import ThreadPoolExecutor
    
    # å®šä¹‰å¹¶è¡Œæ‰§è¡Œå·¥å…·çš„å‡½æ•°
    async def execute_tools_in_parallel(tool_calls):
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨"""
        results = []
        
        # å®šä¹‰å•ä¸ªå·¥å…·æ‰§è¡Œå‡½æ•°
        def execute_single_tool(call):
            tool_name = call["name"]
            tool_input = call["input"]
            
            # æŸ¥æ‰¾åŒ¹é…çš„å·¥å…·
            tool = None
            normalized_request = tool_name.lower().replace("_", "").replace("-", "")
            
            # å°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            for t in tools:
                if t.name.lower() == tool_name.lower():
                    tool = t
                    break
            
            # æ£€æŸ¥åˆ«å
            if not tool:
                for t in tools:
                    if hasattr(t, "aliases") and isinstance(t.aliases, list):
                        for alias in t.aliases:
                            if alias.lower() == tool_name.lower():
                                tool = t
                                break
                        if tool:
                            break
            
            # å°è¯•æ›´çµæ´»çš„åŒ¹é…
            if not tool:
                for t in tools:
                    normalized_tool = t.name.lower().replace("_", "").replace("-", "")
                    if normalized_tool == normalized_request:
                        tool = t
                        break
                    
                    # æ£€æŸ¥æ ‡å‡†åŒ–åˆ«å
                    if hasattr(t, "aliases") and isinstance(t.aliases, list):
                        for alias in t.aliases:
                            normalized_alias = alias.lower().replace("_", "").replace("-", "")
                            if normalized_alias == normalized_request:
                                tool = t
                                break
                        if tool:
                            break
            
            if not tool:
                return {"name": tool_name, "result": f"æ‰¾ä¸åˆ°å·¥å…·: {tool_name}", "error": True}
            
            # æ‰§è¡Œå·¥å…·
            try:
                util.log(1, f"[LangGraphæµ] å¼€å§‹æ‰§è¡Œå·¥å…·: {tool.name}")
                result = tool.run(tool_input)
                return {"name": tool.name, "result": str(result), "error": False}
            except Exception as e:
                error_msg = f"å·¥å…· {tool_name} æ‰§è¡Œå‡ºé”™: {str(e)}"
                util.log(2, f"[LangGraphæµ] {error_msg}")
                return {"name": tool_name, "result": error_msg, "error": True}
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œå·¥å…·
        with ThreadPoolExecutor() as executor:
            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = [
                asyncio.get_event_loop().run_in_executor(executor, execute_single_tool, call)
                for call in tool_calls
            ]
            
            # å¹¶è¡Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks)
            return results
    
    # æ‰§è¡Œå·¥å…·å¹¶è·å–ç»“æœ
    try:
        # åˆ›å»ºäº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # æ‰§è¡Œå¹¶è¡Œå·¥å…·è°ƒç”¨
        tool_results = loop.run_until_complete(execute_tools_in_parallel(tool_calls))
        
        # ä»ç»“æœåˆ›å»ºå‡½æ•°æ¶ˆæ¯
        function_messages = []
        for result in tool_results:
            function_messages.append(
                FunctionMessage(content=result["result"], name=result["name"])
            )
        
        # æ·»åŠ ç»“æœåˆ°æ¶ˆæ¯å†å²
        state["messages"] = messages + function_messages
        
        # è®°å½•å·¥å…·æ‰§è¡Œç»“æœ
        for result in tool_results:
            result_preview = result["result"][:100] + ("..." if len(result["result"]) > 100 else "")
            util.log(1, f"[LangGraphæµ] å·¥å…· {result['name']} æ‰§è¡Œç»“æœ: {result_preview}")
        
        # å°è¯•å‘ä¸­è½¬ç«™å‘é€å·¥å…·æ‰§è¡Œç»“æœ
        try:
            from llm.transit_station import TransitStation
            transit = TransitStation.get_instance()
            for result in tool_results:
                transit.add_intermediate_state(
                    f"å·¥å…· {result['name']} æ‰§è¡Œç»“æœ: {result['result'][:200]}...",
                    "LangGraphå·¥å…·æ‰§è¡Œç»“æœ"
                )
        except:
            pass
            
    except Exception as e:
        # å¤„ç†æ•´ä½“æ‰§è¡Œé”™è¯¯
        error_msg = f"å¹¶è¡Œæ‰§è¡Œå·¥å…·è°ƒç”¨å‡ºé”™: {str(e)}"
        util.log(2, f"[LangGraphæµ] {error_msg}")
        state["messages"] = messages + [FunctionMessage(content=error_msg, name="tool_executor")]
    
    return state

def route_next_step(state: AgentState):
    """
    è·¯ç”±å†³ç­–å‡½æ•°ï¼Œå†³å®šä¸‹ä¸€æ­¥èµ°å‘
    
    åˆ†ææœ€åä¸€æ¡AIæ¶ˆæ¯ï¼Œå†³å®šæ˜¯æ‰§è¡Œå·¥å…·è°ƒç”¨è¿˜æ˜¯ç»“æŸå¯¹è¯
    """
    # è·å–æ¶ˆæ¯å†å²
    messages = state.get("messages", [])
    
    # è·å–æœ€åä¸€æ¡AIæ¶ˆæ¯
    if not messages:
        return "end"
    
    last_ai_message = None
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            last_ai_message = msg
            break
    
    if not last_ai_message:
        return "end"
    
    # åˆ†ææ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    content = last_ai_message.content
    
    # æ£€æµ‹å·¥å…·è°ƒç”¨æ ¼å¼ - æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼
    import re
    import json
    
    # å•å·¥å…·è°ƒç”¨æ£€æµ‹ - æ›´å®½æ¾ä»¥åŒ¹é…æ›´å¤šæ ¼å¼
    single_tool_pattern = r'```(?:json)?\s*\n?\s*\{\s*"action":\s*"([^"]+)"'
    
    # å·¥å…·è°ƒç”¨æ•°ç»„æ£€æµ‹ - æ›´å®½æ¾
    # å…ˆå°è¯•æå–JSONä»£ç å—
    has_tool_call = False
    
    # 1. æ£€æŸ¥å•ä¸ªå·¥å…·è°ƒç”¨
    if re.search(single_tool_pattern, content):
        has_tool_call = True
        util.log(1, f"[LangGraphæµ] æ£€æµ‹åˆ°å•ä¸ªå·¥å…·è°ƒç”¨")
    
    # 2. æ£€æŸ¥å¯èƒ½çš„å·¥å…·æ•°ç»„
    if not has_tool_call:
        array_matches = re.findall(r'```(?:json)?\s*\n?\s*(\[.*?\])\s*\n?```', content, re.DOTALL)
        for array_json in array_matches:
            try:
                # è§£æJSONæ•°ç»„
                tool_array = json.loads(array_json)
                if isinstance(tool_array, list) and len(tool_array) > 0:
                    for item in tool_array:
                        if isinstance(item, dict) and "action" in item:
                            has_tool_call = True
                            util.log(1, f"[LangGraphæµ] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨æ•°ç»„ï¼ŒåŒ…å«{len(tool_array)}ä¸ªå·¥å…·")
                            break
            except Exception as e:
                # è§£æå¤±è´¥è®°å½•æ—¥å¿—
                util.log(2, f"[LangGraphæµ] JSONæ•°ç»„è§£æå¤±è´¥: {str(e)}")
                pass
    
    # 3. æ£€æŸ¥ReActæ ¼å¼
    if not has_tool_call:
        react_patterns = [
            r'Action\s*:\s*(\w+)\[',
            r'è¡ŒåŠ¨\s*:\s*(\w+)\[',
            r'åŠ¨ä½œ\s*:\s*(\w+)\['
        ]
        for pattern in react_patterns:
            if re.search(pattern, content):
                has_tool_call = True
                util.log(1, f"[LangGraphæµ] æ£€æµ‹åˆ°ReActæ ¼å¼å·¥å…·è°ƒç”¨")
                break
    
    # æ—¥å¿—è®°å½•åˆ¤æ–­ç»“æœ
    util.log(1, f"[LangGraphæµ] è·¯ç”±å†³ç­–: {'æ‰§è¡Œå·¥å…·' if has_tool_call else 'ç»“æŸå¯¹è¯'}")
    
    if has_tool_call:
        return "tool"
    else:
        return "end"

def invoke(query, tools=None):
    """
    è°ƒç”¨Agentå¤„ç†æŸ¥è¯¢
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        tools: å¯é€‰çš„å·¥å…·åˆ—è¡¨
        
    Returns:
        str: Agentçš„å“åº”
    """
    try:
        # æ„å»ºAgentå›¾
        graph, _ = build_agent_graph()
        
        # å‡†å¤‡åˆå§‹çŠ¶æ€
        if tools is None:
            # ä½¿ç”¨_init_tools_staticæ›¿ä»£get_tools
            tools = _init_tools_static()
        
        initial_state = {
            "messages": [HumanMessage(content=query)],
            "tools": tools,
            "meta": {"query_time": time.time()}
        }
        
        # æ‰§è¡Œå›¾ - ä½¿ç”¨æµå¼å¤„ç†è·å–è¯¦ç»†æ—¥å¿—
        util.log(1, f"[LangGraphæµ] å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        
        # å¯ç”¨æµå¼æ‰§è¡Œ
        for event in graph.stream(initial_state, stream_mode="values"):
            if "messages" in event:
                # è®°å½•æ¯ä¸€æ­¥çš„æ¶ˆæ¯å˜åŒ–
                messages = event["messages"]
                if messages and len(messages) > 0:
                    last_msg = messages[-1]
                    msg_type = type(last_msg).__name__
                    if hasattr(last_msg, "content"):
                        content = last_msg.content[:100] + ("..." if len(last_msg.content) > 100 else "")
                        util.log(1, f"[LangGraphæµäº‹ä»¶] {msg_type}: {content}")
        
        # è·å–æœ€ç»ˆçŠ¶æ€
        final_state = graph.get_state()
        
        # æå–ç»“æœ
        messages = final_state.get("messages", [])
        if not messages:
            return "æœªèƒ½ç”Ÿæˆå“åº”"
        
        # è·å–æœ€åä¸€æ¡AIæ¶ˆæ¯
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                return msg.content
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°AIæ¶ˆæ¯ï¼Œè¿”å›æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
        last_message = messages[-1]
        if hasattr(last_message, "content"):
            return last_message.content
        
        return str(last_message)
    
    except Exception as e:
        util.log(2, f"[LangGraph] å¤„ç†æŸ¥è¯¢å‡ºé”™: {str(e)}")
        traceback.print_exc()
        return f"å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    result = invoke("ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    print(f"æŸ¥è¯¢ç»“æœ: {result}")
