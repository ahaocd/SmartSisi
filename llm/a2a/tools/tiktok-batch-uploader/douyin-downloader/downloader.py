#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æŠ–éŸ³ä¸‹è½½å™¨ - ç»Ÿä¸€å¢å¼ºç‰ˆ
æ”¯æŒè§†é¢‘ã€å›¾æ–‡ã€ç”¨æˆ·ä¸»é¡µã€åˆé›†ç­‰å¤šç§å†…å®¹çš„æ‰¹é‡ä¸‹è½½
"""

# ğŸ”¥ Windows UTF-8 ç¼–ç ä¿®å¤ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰å¯¼å…¥ä¹‹å‰ï¼‰
import sys
import os

# å¼ºåˆ¶è®¾ç½®stdout/stderrä¸ºUTF-8ç¼–ç ï¼ˆä¿®å¤emojiæ˜¾ç¤ºå’ŒGBKç¼–ç é”™è¯¯ï¼‰
if os.name == 'nt':  # Windowsç³»ç»Ÿ
    os.system('chcp 65001 >nul 2>&1')
    import codecs
    # æ£€æŸ¥æ˜¯å¦æœ‰bufferå±æ€§ï¼ˆé¿å…é‡å¤åŒ…è£…ï¼‰
    if hasattr(sys.stdout, 'buffer'):
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    if hasattr(sys.stderr, 'buffer'):
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# é¦–å…ˆç¦ç”¨æ‰€æœ‰HTTPä»£ç†ï¼ˆå¿…é¡»åœ¨importå…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰
os.environ['NO_PROXY'] = '*'
os.environ['no_proxy'] = '*'
if 'HTTP_PROXY' in os.environ:
    del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ:
    del os.environ['HTTPS_PROXY']
if 'http_proxy' in os.environ:
    del os.environ['http_proxy']
if 'https_proxy' in os.environ:
    del os.environ['https_proxy']

import asyncio
import json
import logging
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urlparse
import argparse
import yaml
try:
    from utils import util
except Exception:
    util = None

# ç¬¬ä¸‰æ–¹åº“
try:
    import aiohttp
    import requests
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
    from rich.table import Table
    from rich.panel import Panel
    from rich.live import Live
    from rich import print as rprint
except ImportError as e:
    print(f"è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install aiohttp requests rich pyyaml")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from apiproxy.douyin import douyin_headers
from apiproxy.douyin.urls import Urls
from apiproxy.douyin.result import Result
from apiproxy.common.utils import Utils
from apiproxy.douyin.auth.cookie_manager import AutoCookieManager
from apiproxy.douyin.database import DataBase

# é…ç½®æ—¥å¿— - ç»Ÿä¸€æ—¥å¿—ç›®å½•ï¼ˆä¼˜å…ˆ SmartSisi/logsï¼‰
_current_dir = Path(__file__).resolve().parent
try:
    if util:
        _log_dir = Path(util.ensure_log_dir("tools", "tiktok_batch"))
        _log_file = _log_dir / "douyin_downloader.log"
    else:
        _log_file = _current_dir / "downloader.log"
except Exception:
    _log_file = _current_dir / "downloader.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(str(_log_file), encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Rich console
console = Console()


class ContentType:
    """å†…å®¹ç±»å‹æšä¸¾"""
    VIDEO = "video"
    IMAGE = "image" 
    USER = "user"
    MIX = "mix"
    MUSIC = "music"
    LIVE = "live"


class DownloadStats:
    """ä¸‹è½½ç»Ÿè®¡"""
    def __init__(self):
        self.total = 0
        self.success = 0
        self.failed = 0
        self.skipped = 0
        self.start_time = time.time()
    
    @property
    def success_rate(self):
        return (self.success / self.total * 100) if self.total > 0 else 0
    
    @property
    def elapsed_time(self):
        return time.time() - self.start_time
    
    def to_dict(self):
        return {
            'total': self.total,
            'success': self.success,
            'failed': self.failed,
            'skipped': self.skipped,
            'success_rate': f"{self.success_rate:.1f}%",
            'elapsed_time': f"{self.elapsed_time:.1f}s"
        }


class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨ - éšæœº2-8ç§’é—´éš”ï¼ˆæ¨¡æ‹ŸçœŸäººè¡Œä¸ºï¼‰"""
    def __init__(self, min_interval: float = 2.0, max_interval: float = 8.0):
        self.min_interval = min_interval  # æœ€å°é—´éš”2ç§’
        self.max_interval = max_interval  # æœ€å¤§é—´éš”8ç§’
        self.last_request = 0
    
    async def acquire(self):
        """è·å–è®¸å¯ - ä½¿ç”¨éšæœºé—´éš”"""
        import random
        
        current = time.time()
        time_since_last = current - self.last_request
        
        # ç”Ÿæˆéšæœºé—´éš”æ—¶é—´ï¼ˆ2-8ç§’ï¼‰
        random_interval = random.uniform(self.min_interval, self.max_interval)
        
        if time_since_last < random_interval:
            wait_time = random_interval - time_since_last
            logger.debug(f"â±ï¸ ç­‰å¾… {wait_time:.1f} ç§’åç»§ç»­...")
            await asyncio.sleep(wait_time)
        
        self.last_request = time.time()


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""
    def __init__(self, max_retries: int = 3):
        self.max_retries = max_retries
        self.retry_delays = [1, 2, 5]  # é‡è¯•å»¶è¿Ÿ
    
    async def execute_with_retry(self, func, *args, **kwargs):
        """æ‰§è¡Œå‡½æ•°å¹¶è‡ªåŠ¨é‡è¯•"""
        last_error = None
        for attempt in range(self.max_retries):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    delay = self.retry_delays[min(attempt, len(self.retry_delays) - 1)]
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}, {delay}ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)
        raise last_error


class UnifiedDownloader:
    """ç»Ÿä¸€ä¸‹è½½å™¨"""
    
    def __init__(self, config_path: str = "config.yml"):
        self.config = self._load_config(config_path)
        self.urls_helper = Urls()
        self.result_helper = Result()
        self.utils = Utils()
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.stats = DownloadStats()
        self.rate_limiter = RateLimiter(min_interval=2.0)  # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
        self.retry_manager = RetryManager(max_retries=self.config.get('retry_times', 3))
        
        # Cookieä¸è¯·æ±‚å¤´ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œæ”¯æŒè‡ªåŠ¨è·å–ï¼‰
        self.cookies = self.config.get('cookies') if 'cookies' in self.config else self.config.get('cookie')
        self.auto_cookie = bool(self.config.get('auto_cookie')) or (isinstance(self.config.get('cookie'), str) and self.config.get('cookie') == 'auto') or (isinstance(self.config.get('cookies'), str) and self.config.get('cookies') == 'auto')
        self.headers = {**douyin_headers}
        # é¿å…æœåŠ¡ç«¯ä½¿ç”¨brotliå¯¼è‡´aiohttpæ— æ³•è§£å‹ï¼ˆæœªå®‰è£…brotliåº“æ—¶ä¼šå‡ºç°ç©ºå“åº”ï¼‰
        self.headers['accept-encoding'] = 'gzip, deflate'
        # å¢é‡ä¸‹è½½ä¸æ•°æ®åº“
        self.increase_cfg: Dict[str, Any] = self.config.get('increase', {}) or {}
        self.enable_database: bool = bool(self.config.get('database', True))
        self.db: Optional[DataBase] = DataBase() if self.enable_database else None
        
        # ä¿å­˜è·¯å¾„ - ä¿®å¤ï¼šç›¸å¯¹è·¯å¾„è½¬ä¸ºç»å¯¹è·¯å¾„ï¼ˆåŸºäºdouyin-downloaderç›®å½•ï¼‰
        path_config = self.config.get('path', './Downloaded')
        self.save_path = Path(path_config)
        if not self.save_path.is_absolute():
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºdouyin-downloaderç›®å½•çš„ç»å¯¹è·¯å¾„
            downloader_dir = Path(__file__).resolve().parent
            self.save_path = (downloader_dir / self.save_path).resolve()
        self.save_path.mkdir(parents=True, exist_ok=True)
        
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(config_path):
            # å…¼å®¹é…ç½®æ–‡ä»¶å‘½åï¼šä¼˜å…ˆ config.ymlï¼Œå…¶æ¬¡ config_simple.yml
            alt_path = 'config_simple.yml'
            if os.path.exists(alt_path):
                config_path = alt_path
            else:
                # è¿”å›ä¸€ä¸ªç©ºé…ç½®ï¼Œç”±å‘½ä»¤è¡Œå‚æ•°å†³å®š
                return {}
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ç®€åŒ–é…ç½®å…¼å®¹ï¼šlinks/link, output_dir/path, cookie/cookies
        if 'links' in config and 'link' not in config:
            config['link'] = config['links']
        if 'output_dir' in config and 'path' not in config:
            config['path'] = config['output_dir']
        if 'cookie' in config and 'cookies' not in config:
            config['cookies'] = config['cookie']
        if isinstance(config.get('cookies'), str) and config.get('cookies') == 'auto':
            config['auto_cookie'] = True
        
        # å…è®¸æ—  linkï¼ˆé€šè¿‡å‘½ä»¤è¡Œä¼ å…¥ï¼‰
        # å¦‚æœä¸¤è€…éƒ½æ²¡æœ‰ï¼Œåç»­ä¼šåœ¨è¿è¡Œæ—¶æç¤º
        
        return config
    
    def _build_cookie_string(self) -> str:
        """æ„å»ºCookieå­—ç¬¦ä¸²"""
        if isinstance(self.cookies, str):
            return self.cookies
        elif isinstance(self.cookies, dict):
            return '; '.join([f'{k}={v}' for k, v in self.cookies.items()])
        elif isinstance(self.cookies, list):
            # æ”¯æŒæ¥è‡ªAutoCookieManagerçš„cookiesåˆ—è¡¨
            try:
                kv = {c.get('name'): c.get('value') for c in self.cookies if c.get('name') and c.get('value')}
                return '; '.join([f'{k}={v}' for k, v in kv.items()])
            except Exception:
                return ''
        return ''

    async def _initialize_cookies_and_headers(self):
        """åˆå§‹åŒ–Cookieä¸è¯·æ±‚å¤´ï¼ˆæ”¯æŒè‡ªåŠ¨è·å–ï¼‰"""
        # è‹¥é…ç½®ä¸ºå­—ç¬¦ä¸² 'auto'ï¼Œè§†ä¸ºæœªæä¾›ï¼Œè§¦å‘è‡ªåŠ¨è·å–
        if isinstance(self.cookies, str) and self.cookies.strip().lower() == 'auto':
            self.cookies = None
        
        # è‹¥å·²æ˜¾å¼æä¾›cookiesï¼Œåˆ™ç›´æ¥ä½¿ç”¨
        cookie_str = self._build_cookie_string()
        if cookie_str:
            self.headers['Cookie'] = cookie_str
            # åŒæ—¶è®¾ç½®åˆ°å…¨å±€ douyin_headersï¼Œç¡®ä¿æ‰€æœ‰ API è¯·æ±‚éƒ½èƒ½ä½¿ç”¨
            from apiproxy.douyin import douyin_headers
            douyin_headers['Cookie'] = cookie_str
            return
        
        # è‡ªåŠ¨è·å–Cookie
        if self.auto_cookie:
            try:
                console.print("[cyan]ğŸ” æ­£åœ¨è‡ªåŠ¨è·å–Cookie...[/cyan]")
                # ä½¿ç”¨ç»å¯¹è·¯å¾„ä¿å­˜Cookieåˆ°douyin-downloaderç›®å½•
                import os
                cookie_path = os.path.join(os.path.dirname(__file__), 'cookies.pkl')
                async with AutoCookieManager(cookie_file=cookie_path, headless=False) as cm:
                    cookies_list = await cm.get_cookies()
                    if cookies_list:
                        self.cookies = cookies_list
                        cookie_str = self._build_cookie_string()
                        if cookie_str:
                            self.headers['Cookie'] = cookie_str
                            # åŒæ—¶è®¾ç½®åˆ°å…¨å±€ douyin_headersï¼Œç¡®ä¿æ‰€æœ‰ API è¯·æ±‚éƒ½èƒ½ä½¿ç”¨
                            from apiproxy.douyin import douyin_headers
                            douyin_headers['Cookie'] = cookie_str
                            console.print("[green]âœ… Cookieè·å–æˆåŠŸ[/green]")
                            return
                console.print("[yellow]âš ï¸ è‡ªåŠ¨è·å–Cookieå¤±è´¥æˆ–ä¸ºç©ºï¼Œç»§ç»­å°è¯•æ— Cookieæ¨¡å¼[/yellow]")
            except Exception as e:
                logger.warning(f"è‡ªåŠ¨è·å–Cookieå¤±è´¥: {e}")
                console.print("[yellow]âš ï¸ è‡ªåŠ¨è·å–Cookieå¤±è´¥ï¼Œç»§ç»­å°è¯•æ— Cookieæ¨¡å¼[/yellow]")
        
        # æœªèƒ½è·å–Cookieåˆ™ä¸è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤headers
    
    def detect_content_type(self, url: str) -> ContentType:
        """æ£€æµ‹URLå†…å®¹ç±»å‹"""
        if '/user/' in url:
            return ContentType.USER
        elif '/video/' in url or 'v.douyin.com' in url:
            return ContentType.VIDEO
        elif '/note/' in url:
            return ContentType.IMAGE
        elif '/collection/' in url or '/mix/' in url:
            return ContentType.MIX
        elif '/music/' in url:
            return ContentType.MUSIC
        elif 'live.douyin.com' in url:
            return ContentType.LIVE
        else:
            return ContentType.VIDEO  # é»˜è®¤å½“ä½œè§†é¢‘
    
    async def resolve_short_url(self, url: str) -> str:
        """è§£æçŸ­é“¾æ¥"""
        if 'v.douyin.com' in url:
            try:
                # ä½¿ç”¨åŒæ­¥è¯·æ±‚è·å–é‡å®šå‘
                response = requests.get(url, headers=self.headers, allow_redirects=True, timeout=10, proxies={'http': None, 'https': None})
                final_url = response.url
                logger.info(f"è§£æçŸ­é“¾æ¥: {url} -> {final_url}")
                return final_url
            except Exception as e:
                logger.warning(f"è§£æçŸ­é“¾æ¥å¤±è´¥: {e}")
        return url
    
    def extract_id_from_url(self, url: str, content_type: ContentType = None) -> Optional[str]:
        """ä»URLæå–ID
        
        Args:
            url: è¦è§£æçš„URL
            content_type: å†…å®¹ç±»å‹ï¼ˆå¯é€‰ï¼Œç”¨äºæŒ‡å¯¼æå–ï¼‰
        """
        # å¦‚æœå·²çŸ¥æ˜¯ç”¨æˆ·é¡µé¢ï¼Œç›´æ¥æå–ç”¨æˆ·ID
        if content_type == ContentType.USER or '/user/' in url:
            user_patterns = [
                r'/user/([\w-]+)',
                r'sec_uid=([\w-]+)'
            ]
            
            for pattern in user_patterns:
                match = re.search(pattern, url)
                if match:
                    user_id = match.group(1)
                    logger.info(f"æå–åˆ°ç”¨æˆ·ID: {user_id}")
                    return user_id
        
        # è§†é¢‘IDæ¨¡å¼ï¼ˆä¼˜å…ˆï¼‰
        video_patterns = [
            r'/video/(\d+)',
            r'/note/(\d+)',
            r'modal_id=(\d+)',
            r'aweme_id=(\d+)',
            r'item_id=(\d+)'
        ]
        
        for pattern in video_patterns:
            match = re.search(pattern, url)
            if match:
                video_id = match.group(1)
                logger.info(f"æå–åˆ°è§†é¢‘ID: {video_id}")
                return video_id
        
        # å…¶ä»–æ¨¡å¼
        other_patterns = [
            r'/collection/(\d+)',
            r'/music/(\d+)'
        ]
        
        for pattern in other_patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # å°è¯•ä»URLä¸­æå–æ•°å­—ID
        number_match = re.search(r'(\d{15,20})', url)
        if number_match:
            video_id = number_match.group(1)
            logger.info(f"ä»URLæå–åˆ°æ•°å­—ID: {video_id}")
            return video_id
        
        logger.error(f"æ— æ³•ä»URLæå–ID: {url}")
        return None

    def _get_aweme_id_from_info(self, info: Dict) -> Optional[str]:
        """ä» aweme ä¿¡æ¯ä¸­æå– aweme_id"""
        try:
            if 'aweme_id' in info:
                return str(info.get('aweme_id'))
            # aweme_detail ç»“æ„
            return str(info.get('aweme', {}).get('aweme_id') or info.get('aweme_id'))
        except Exception:
            return None

    def _get_sec_uid_from_info(self, info: Dict) -> Optional[str]:
        """ä» aweme ä¿¡æ¯ä¸­æå–ä½œè€… sec_uid"""
        try:
            return info.get('author', {}).get('sec_uid')
        except Exception:
            return None

    def _should_skip_increment(self, context: str, info: Dict, mix_id: Optional[str] = None, music_id: Optional[str] = None, sec_uid: Optional[str] = None) -> bool:
        """æ ¹æ®å¢é‡é…ç½®ã€æ•°æ®åº“è®°å½•å’Œæ–‡ä»¶å¤¹åˆ¤æ–­æ˜¯å¦è·³è¿‡ä¸‹è½½"""
        aweme_id = self._get_aweme_id_from_info(info)
        if not aweme_id:
            return False
        
        # å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        try:
            from pathlib import Path
            # è·å–ä½œè€…æ˜µç§°
            author = info.get('author', {}).get('nickname', 'unknown')
            # æ„é€ å¯èƒ½çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
            download_path = Path(self.save_path) / author
            if download_path.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«aweme_idçš„æ–‡ä»¶/æ–‡ä»¶å¤¹
                for item in download_path.iterdir():
                    if aweme_id in item.name:
                        logger.info(f"â­ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {item.name}")
                        return True
        except Exception as e:
            logger.debug(f"æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}")
        
        # å†æ£€æŸ¥æ•°æ®åº“
        if not self.db:
            return False

        try:
            if context == 'post' and self.increase_cfg.get('post', False):
                sec = sec_uid or self._get_sec_uid_from_info(info) or ''
                return bool(self.db.get_user_post(sec, int(aweme_id)) if aweme_id.isdigit() else None)
            if context == 'like' and self.increase_cfg.get('like', False):
                sec = sec_uid or self._get_sec_uid_from_info(info) or ''
                return bool(self.db.get_user_like(sec, int(aweme_id)) if aweme_id.isdigit() else None)
            if context == 'mix' and self.increase_cfg.get('mix', False):
                sec = sec_uid or self._get_sec_uid_from_info(info) or ''
                mid = mix_id or ''
                return bool(self.db.get_mix(sec, mid, int(aweme_id)) if aweme_id.isdigit() else None)
            if context == 'music' and self.increase_cfg.get('music', False):
                mid = music_id or ''
                return bool(self.db.get_music(mid, int(aweme_id)) if aweme_id.isdigit() else None)
        except Exception:
            return False
        return False

    def _record_increment(self, context: str, info: Dict, mix_id: Optional[str] = None, music_id: Optional[str] = None, sec_uid: Optional[str] = None):
        """ä¸‹è½½æˆåŠŸåå†™å…¥æ•°æ®åº“è®°å½•"""
        if not self.db:
            return
        aweme_id = self._get_aweme_id_from_info(info)
        if not aweme_id or not aweme_id.isdigit():
            return
        try:
            if context == 'post':
                sec = sec_uid or self._get_sec_uid_from_info(info) or ''
                self.db.insert_user_post(sec, int(aweme_id), info)
            elif context == 'like':
                sec = sec_uid or self._get_sec_uid_from_info(info) or ''
                self.db.insert_user_like(sec, int(aweme_id), info)
            elif context == 'mix':
                sec = sec_uid or self._get_sec_uid_from_info(info) or ''
                mid = mix_id or ''
                self.db.insert_mix(sec, mid, int(aweme_id), info)
            elif context == 'music':
                mid = music_id or ''
                self.db.insert_music(mid, int(aweme_id), info)
        except Exception:
            pass
    
    async def download_single_video(self, url: str, progress=None) -> bool:
        """ä¸‹è½½å•ä¸ªè§†é¢‘/å›¾æ–‡"""
        try:
            # è§£æçŸ­é“¾æ¥
            url = await self.resolve_short_url(url)
            
            # æå–ID
            video_id = self.extract_id_from_url(url, ContentType.VIDEO)
            if not video_id:
                logger.error(f"æ— æ³•ä»URLæå–ID: {url}")
                return False
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°è§†é¢‘IDï¼Œå°è¯•ä½œä¸ºè§†é¢‘IDç›´æ¥ä½¿ç”¨
            if not video_id and '/user/' not in url:
                # å¯èƒ½çŸ­é“¾æ¥ç›´æ¥åŒ…å«äº†è§†é¢‘ID
                video_id = url.split('/')[-2] if url.endswith('/') else url.split('/')[-1]
                logger.info(f"å°è¯•ä»çŸ­é“¾æ¥è·¯å¾„æå–ID: {video_id}")
            
            if not video_id:
                logger.error(f"æ— æ³•ä»URLæå–è§†é¢‘ID: {url}")
                return False
            
            # é™é€Ÿ
            await self.rate_limiter.acquire()
            
            # è·å–è§†é¢‘ä¿¡æ¯
            if progress:
                progress.update(task_id=progress.task_ids[-1], description="è·å–è§†é¢‘ä¿¡æ¯...")
            
            video_info = await self.retry_manager.execute_with_retry(
                self._fetch_video_info, video_id
            )
            
            if not video_info:
                logger.error(f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {video_id}")
                self.stats.failed += 1
                return False
            
            # ä¸‹è½½è§†é¢‘æ–‡ä»¶
            if progress:
                progress.update(task_id=progress.task_ids[-1], description="ä¸‹è½½è§†é¢‘æ–‡ä»¶...")
            
            success = await self._download_media_files(video_info, progress)
            
            # ç»Ÿè®¡åœ¨ _download_media_files å†…å·²æ›´æ–°ï¼Œè¿™é‡Œåªè¾“å‡ºæ±‡æ€»ä¿¡æ¯
            if success:
                logger.info(f"âœ… ä¸‹è½½å°±ç»ª(æ–°/å·²å­˜åœ¨): {url}")
            else:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {url}")
            
            return success
            
        except Exception as e:
            logger.error(f"ä¸‹è½½è§†é¢‘å¼‚å¸¸ {url}: {e}")
            self.stats.failed += 1
            return False
        finally:
            self.stats.total += 1
    
    async def _fetch_video_info(self, video_id: str) -> Optional[Dict]:
        """è·å–è§†é¢‘ä¿¡æ¯"""
        try:
            # ç›´æ¥ä½¿ç”¨ DouYinCommand.py ä¸­æˆåŠŸçš„ Douyin ç±»
            from apiproxy.douyin.douyin import Douyin
            
            # åˆ›å»º Douyin å®ä¾‹
            dy = Douyin(database=False)
            
            # è®¾ç½®æˆ‘ä»¬çš„ cookies åˆ° douyin_headers
            if hasattr(self, 'cookies') and self.cookies:
                cookie_str = self._build_cookie_string()
                if cookie_str:
                    from apiproxy.douyin import douyin_headers
                    douyin_headers['Cookie'] = cookie_str
                    logger.info(f"è®¾ç½® Cookie åˆ° Douyin ç±»: {cookie_str[:100]}...")
            
            try:
                # ä½¿ç”¨ç°æœ‰çš„æˆåŠŸå®ç°
                result = dy.getAwemeInfo(video_id)
                if result:
                    logger.info(f"Douyin ç±»æˆåŠŸè·å–è§†é¢‘ä¿¡æ¯: {result.get('desc', '')[:30]}")
                    return result
                else:
                    logger.error("Douyin ç±»è¿”å›ç©ºç»“æœ")
                    
            except Exception as e:
                logger.error(f"Douyin ç±»è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
                
        except Exception as e:
            logger.error(f"å¯¼å…¥æˆ–ä½¿ç”¨ Douyin ç±»å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # å¦‚æœ Douyin ç±»å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¥å£ï¼ˆiesdouyinï¼Œæ— éœ€X-Bogusï¼‰
        try:
            fallback_url = f"https://www.iesdouyin.com/web/api/v2/aweme/iteminfo/?item_ids={video_id}"
            logger.info(f"å°è¯•å¤‡ç”¨æ¥å£è·å–è§†é¢‘ä¿¡æ¯: {fallback_url}")
            
            # è®¾ç½®æ›´é€šç”¨çš„è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.douyin.com/',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(fallback_url, headers=headers, timeout=15) as response:
                    logger.info(f"å¤‡ç”¨æ¥å£å“åº”çŠ¶æ€: {response.status}")
                    if response.status != 200:
                        logger.error(f"å¤‡ç”¨æ¥å£è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return None
                    
                    text = await response.text()
                    logger.info(f"å¤‡ç”¨æ¥å£å“åº”å†…å®¹é•¿åº¦: {len(text)}")
                    
                    if not text:
                        logger.error("å¤‡ç”¨æ¥å£å“åº”ä¸ºç©º")
                        return None
                    
                    try:
                        data = json.loads(text)
                        logger.info(f"å¤‡ç”¨æ¥å£è¿”å›æ•°æ®: {data}")
                        
                        item_list = (data or {}).get('item_list') or []
                        if item_list:
                            aweme_detail = item_list[0]
                            logger.info("å¤‡ç”¨æ¥å£æˆåŠŸè·å–è§†é¢‘ä¿¡æ¯")
                            return aweme_detail
                        else:
                            logger.error("å¤‡ç”¨æ¥å£è¿”å›çš„æ•°æ®ä¸­æ²¡æœ‰ item_list")
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"å¤‡ç”¨æ¥å£JSONè§£æå¤±è´¥: {e}")
                        logger.error(f"åŸå§‹å“åº”å†…å®¹: {text}")
                        return None
                        
        except Exception as e:
            logger.error(f"å¤‡ç”¨æ¥å£è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        
        return None
    
    def _build_detail_params(self, aweme_id: str) -> str:
        """æ„å»ºè¯¦æƒ…APIå‚æ•°"""
        # ä½¿ç”¨ä¸ç°æœ‰ douyinapi.py ç›¸åŒçš„å‚æ•°æ ¼å¼
        params = [
            f'aweme_id={aweme_id}',
            'device_platform=webapp',
            'aid=6383'
        ]
        return '&'.join(params)
    
    async def _download_media_files(self, video_info: Dict, progress=None) -> bool:
        """ä¸‹è½½åª’ä½“æ–‡ä»¶
        è¿”å›Trueè¡¨ç¤ºæœ€ç»ˆå°±ç»ªï¼ˆå·²å­˜åœ¨æˆ–æ–°ä¸‹è½½æˆåŠŸï¼‰ï¼›
        ç”±è°ƒç”¨æ–¹æ ¹æ®æ˜¯å¦å®é™…ä¸‹è½½æ¥ç»Ÿè®¡successæˆ–skippedã€‚
        """
        try:
            # åˆ¤æ–­ç±»å‹
            is_image = bool(video_info.get('images'))
            
            # æ„å»ºä¿å­˜è·¯å¾„
            author_name = video_info.get('author', {}).get('nickname', 'unknown')
            desc = video_info.get('desc', '')[:50].replace('/', '_')
            # å…¼å®¹ create_time ä¸ºæ—¶é—´æˆ³æˆ–æ ¼å¼åŒ–å­—ç¬¦ä¸²
            raw_create_time = video_info.get('create_time')
            dt_obj = None
            if isinstance(raw_create_time, (int, float)):
                dt_obj = datetime.fromtimestamp(raw_create_time)
            elif isinstance(raw_create_time, str) and raw_create_time:
                for fmt in ('%Y-%m-%d %H.%M.%S', '%Y-%m-%d_%H-%M-%S', '%Y-%m-%d %H:%M:%S'):
                    try:
                        dt_obj = datetime.strptime(raw_create_time, fmt)
                        break
                    except Exception:
                        pass
            if dt_obj is None:
                dt_obj = datetime.fromtimestamp(time.time())
            create_time = dt_obj.strftime('%Y-%m-%d_%H-%M-%S')
            
            folder_name = f"{create_time}_{desc}" if desc else create_time
            save_dir = self.save_path / author_name / folder_name
            save_dir.mkdir(parents=True, exist_ok=True)
            
            success = True
            actually_downloaded = False
            
            if is_image:
                # ä¸‹è½½å›¾æ–‡ï¼ˆæ— æ°´å°ï¼‰
                images = video_info.get('images', [])
                for i, img in enumerate(images):
                    img_url = self._get_best_quality_url(img.get('url_list', []))
                    if img_url:
                        file_path = save_dir / f"image_{i+1}.jpg"
                        if await self._download_file(img_url, file_path):
                            logger.info(f"ä¸‹è½½å›¾ç‰‡ {i+1}/{len(images)}: {file_path.name}")
                        else:
                            success = False
            else:
                # ä¸‹è½½è§†é¢‘ï¼ˆæ— æ°´å°ï¼‰
                video_url = self._get_no_watermark_url(video_info)
                if video_url:
                    file_path = save_dir / f"{folder_name}.mp4"
                    # å…ˆåˆ¤æ–­æ˜¯å¦å·²å­˜åœ¨ï¼Œç”¨äºç»Ÿè®¡skipped
                    already_exists = file_path.exists()
                    if await self._download_file(video_url, file_path):
                        if not already_exists:
                            actually_downloaded = True
                        logger.info(f"ä¸‹è½½è§†é¢‘: {file_path.name}")
                    else:
                        success = False
                
                # ä¸‹è½½éŸ³é¢‘
                if self.config.get('music', True):
                    music_url = self._get_music_url(video_info)
                    if music_url:
                        file_path = save_dir / f"{folder_name}_music.mp3"
                        await self._download_file(music_url, file_path)
            
            # ä¸‹è½½å°é¢
            if self.config.get('cover', True):
                cover_url = self._get_cover_url(video_info)
                if cover_url:
                    file_path = save_dir / f"{folder_name}_cover.jpg"
                    await self._download_file(cover_url, file_path)
            
            # ä¿å­˜JSONæ•°æ®
            if self.config.get('json', True):
                json_path = save_dir / f"{folder_name}_data.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(video_info, f, ensure_ascii=False, indent=2)
            
            # æ ¹æ®å®é™…ä¸‹è½½ä¸å¦æ›´æ–°ç»Ÿè®¡ï¼ˆé¿å…æŠŠå·²å­˜åœ¨è®¡ä¸ºæˆåŠŸï¼‰
            if success:
                if actually_downloaded:
                    self.stats.success += 1
                else:
                    self.stats.skipped += 1
            else:
                self.stats.failed += 1

            return success
            
        except Exception as e:
            logger.error(f"ä¸‹è½½åª’ä½“æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _get_no_watermark_url(self, video_info: Dict) -> Optional[str]:
        """è·å–æ— æ°´å°è§†é¢‘URL"""
        try:
            # ä¼˜å…ˆä½¿ç”¨play_addr_h264
            play_addr = video_info.get('video', {}).get('play_addr_h264') or \
                       video_info.get('video', {}).get('play_addr')
            
            if play_addr:
                url_list = play_addr.get('url_list', [])
                if url_list:
                    # æ›¿æ¢URLä»¥è·å–æ— æ°´å°ç‰ˆæœ¬
                    url = url_list[0]
                    url = url.replace('playwm', 'play')
                    url = url.replace('720p', '1080p')
                    return url
            
            # å¤‡ç”¨ï¼šdownload_addr
            download_addr = video_info.get('video', {}).get('download_addr')
            if download_addr:
                url_list = download_addr.get('url_list', [])
                if url_list:
                    return url_list[0]
                    
        except Exception as e:
            logger.error(f"è·å–æ— æ°´å°URLå¤±è´¥: {e}")
        
        return None
    
    def _get_best_quality_url(self, url_list: List[str]) -> Optional[str]:
        """è·å–æœ€é«˜è´¨é‡çš„URL"""
        if not url_list:
            return None
        
        # ä¼˜å…ˆé€‰æ‹©åŒ…å«ç‰¹å®šå…³é”®è¯çš„URL
        for keyword in ['1080', 'origin', 'high']:
            for url in url_list:
                if keyword in url:
                    return url
        
        # è¿”å›ç¬¬ä¸€ä¸ª
        return url_list[0]
    
    def _get_music_url(self, video_info: Dict) -> Optional[str]:
        """è·å–éŸ³ä¹URL"""
        try:
            music = video_info.get('music', {})
            play_url = music.get('play_url', {})
            url_list = play_url.get('url_list', [])
            return url_list[0] if url_list else None
        except:
            return None
    
    def _get_cover_url(self, video_info: Dict) -> Optional[str]:
        """è·å–å°é¢URL"""
        try:
            cover = video_info.get('video', {}).get('cover', {})
            url_list = cover.get('url_list', [])
            return self._get_best_quality_url(url_list)
        except:
            return None
    
    async def _download_file(self, url: str, save_path: Path, max_retries: int = 3) -> bool:
        """ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        if save_path.exists():
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {save_path.name}")
            return True
        
        for attempt in range(max_retries):
            try:
                timeout = aiohttp.ClientTimeout(total=300, connect=30)  # 5åˆ†é’Ÿæ€»è¶…æ—¶
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url, headers=self.headers) as response:
                        if response.status == 200:
                            content = await response.read()
                            with open(save_path, 'wb') as f:
                                f.write(content)
                            return True
                        else:
                            logger.error(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                            
            except Exception as e:
                logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 * (attempt + 1))  # é€’å¢ç­‰å¾…
                    continue
        
        return False
    
    async def download_user_page(self, url: str) -> bool:
        """ä¸‹è½½ç”¨æˆ·ä¸»é¡µå†…å®¹"""
        try:
            # åˆå§‹åŒ–Cookieï¼ˆé‡è¦ï¼ï¼‰
            await self._initialize_cookies_and_headers()
            
            # æå–ç”¨æˆ·ID
            user_id = self.extract_id_from_url(url, ContentType.USER)
            if not user_id:
                logger.error(f"æ— æ³•ä»URLæå–ç”¨æˆ·ID: {url}")
                return False
            
            console.print(f"\n[cyan]æ­£åœ¨è·å–ç”¨æˆ· {user_id} çš„ä½œå“åˆ—è¡¨...[/cyan]")
            
            # æ ¹æ®é…ç½®ä¸‹è½½ä¸åŒç±»å‹çš„å†…å®¹
            mode = self.config.get('mode', ['post'])
            if isinstance(mode, str):
                mode = [mode]
            
            # å¢åŠ æ€»ä»»åŠ¡æ•°ç»Ÿè®¡
            total_posts = 0
            if 'post' in mode:
                total_posts += self.config.get('number', {}).get('post', 0) or 1
            if 'like' in mode:
                total_posts += self.config.get('number', {}).get('like', 0) or 1
            if 'mix' in mode:
                total_posts += self.config.get('number', {}).get('allmix', 0) or 1
            
            self.stats.total += total_posts
            
            for m in mode:
                if m == 'post':
                    await self._download_user_posts(user_id)
                elif m == 'like':
                    await self._download_user_likes(user_id)
                elif m == 'mix':
                    await self._download_user_mixes(user_id)
            
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è½½ç”¨æˆ·ä¸»é¡µå¤±è´¥: {e}")
            return False
    
    async def _download_user_posts(self, user_id: str):
        """ä¸‹è½½ç”¨æˆ·å‘å¸ƒçš„ä½œå“"""
        max_count = self.config.get('number', {}).get('post', 0)
        min_pages = self.config.get('min_fetch_pages', 3)  # æœ€å°‘è·å–3é¡µä½œå“åˆ—è¡¨
        cursor = 0
        downloaded = 0
        page_num = 0
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå…ˆè·å–è‡³å°‘min_pagesé¡µçš„ä½œå“åˆ—è¡¨
        all_aweme_list = []
        console.print(f"\n[green]å¼€å§‹è·å–ç”¨æˆ·ä½œå“åˆ—è¡¨ï¼ˆè‡³å°‘{min_pages}é¡µï¼‰...[/green]")
        
        while True:
            await self.rate_limiter.acquire()
            posts_data = await self._fetch_user_posts(user_id, cursor)
            if not posts_data:
                break
            
            aweme_list = posts_data.get('aweme_list', [])
            if not aweme_list:
                break
            
            page_num += 1
            all_aweme_list.extend(aweme_list)
            console.print(f"[cyan]ğŸ“„ ç¬¬{page_num}é¡µ: è·å–åˆ° {len(aweme_list)} ä¸ªä½œå“ï¼ˆç´¯è®¡ {len(all_aweme_list)} ä¸ªï¼‰[/cyan]")
            
            has_more = posts_data.get('has_more', False)
            if not has_more:
                console.print(f"[green]âœ… å·²è·å–æ‰€æœ‰ä½œå“[/green]")
                break
            
            # è‡³å°‘è·å–min_pagesé¡µï¼Œæˆ–è€…å·²ç»å¤Ÿä¸‹è½½æ•°é‡äº†
            if page_num >= min_pages and (max_count > 0 and len(all_aweme_list) >= max_count):
                break
            
            cursor = posts_data.get('max_cursor', 0)
            import random
            delay = random.uniform(2.0, 4.0)
            console.print(f"[dim]â³ ç­‰å¾… {delay:.1f} ç§’åè·å–ä¸‹ä¸€é¡µ...[/dim]")
            await asyncio.sleep(delay)
        
        console.print(f"[green]âœ… å…±è·å– {len(all_aweme_list)} ä¸ªä½œå“ï¼Œå¼€å§‹ä¸‹è½½...[/green]")
        
        # ç¬¬äºŒé˜¶æ®µï¼šä¸‹è½½ä½œå“
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
            console=console
        ) as progress:
            
            for aweme in all_aweme_list:
                if max_count > 0 and downloaded >= max_count:
                    console.print(f"[yellow]å·²è¾¾åˆ°ä¸‹è½½æ•°é‡é™åˆ¶: {max_count}[/yellow]")
                    break
                
                # æ—¶é—´è¿‡æ»¤
                if not self._check_time_filter(aweme):
                    continue
                
                # åˆ›å»ºä¸‹è½½ä»»åŠ¡
                task_id = progress.add_task(
                    f"ä¸‹è½½ä½œå“ {downloaded + 1}", 
                    total=100
                )
                
                # å¢é‡åˆ¤æ–­
                if self._should_skip_increment('post', aweme, sec_uid=user_id):
                    continue
                
                # ä¸‹è½½
                success = await self._download_media_files(aweme, progress)
                
                if success:
                    downloaded += 1
                    progress.update(task_id, completed=100)
                    self._record_increment('post', aweme, sec_uid=user_id)
                    import random
                    await asyncio.sleep(random.uniform(1.0, 3.0))
                else:
                    progress.update(task_id, description="[red]ä¸‹è½½å¤±è´¥[/red]")
                    await asyncio.sleep(2.0)
        
        console.print(f"[green]âœ… ç”¨æˆ·ä½œå“ä¸‹è½½å®Œæˆï¼Œå…±ä¸‹è½½ {downloaded} ä¸ª[/green]")
    
    async def _fetch_user_posts(self, user_id: str, cursor: int = 0) -> Optional[Dict]:
        """è·å–ç”¨æˆ·ä½œå“åˆ—è¡¨ï¼ˆæ”¯æŒçœŸæ­£çš„åˆ†é¡µï¼‰+ é‡è¯•æœºåˆ¶"""
        max_retries = 3  # æœ€å¤šé‡è¯•3æ¬¡
        retry_delay = 3.0  # æ¯æ¬¡é‡è¯•ç­‰å¾…3ç§’
        
        for attempt in range(max_retries):
            try:
                from apiproxy.douyin import douyin_headers
                from apiproxy.douyin.urls import Urls
                from apiproxy.common.utils import Utils
                import json
                
                urls = Urls()
                utils = Utils()
                
                # æ¯æ¬¡è·å–35ä¸ªä½œå“ï¼ˆä¸€é¡µï¼‰
                count = 35
                
                # æ„å»ºè¯·æ±‚å‚æ•°
                base_params = f'sec_user_id={user_id}&count={count}&max_cursor={cursor}&device_platform=webapp&aid=6383&channel=channel_pc_web&pc_client_type=1&version_code=170400&version_name=17.4.0&cookie_enabled=true&screen_width=1920&screen_height=1080&browser_language=zh-CN&browser_platform=MacIntel&browser_name=Chrome&browser_version=122.0.0.0&browser_online=true&engine_name=Blink&engine_version=122.0.0.0&os_name=Mac&os_version=10.15.7&cpu_core_num=8&device_memory=8&platform=PC&downlink=10&effective_type=4g&round_trip_time=50'
                
                url = urls.USER_POST + utils.getXbogus(base_params)
                
                # ä½¿ç”¨å¼‚æ­¥è¯·æ±‚ï¼ˆè¶…æ—¶æ—¶é—´åŠ é•¿åˆ°15ç§’ï¼‰
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, headers=douyin_headers, timeout=aiohttp.ClientTimeout(total=15)) as res:
                        if res.status != 200:
                            logger.error(f"HTTPè¯·æ±‚å¤±è´¥: {res.status}ï¼Œå°è¯• {attempt + 1}/{max_retries}")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(retry_delay)
                                continue
                            return None
                        
                        text = await res.text()
                
                # è§£æå“åº”
                datadict = json.loads(text)
                
                if datadict.get("status_code") != 0:
                    logger.error(f"APIè¿”å›é”™è¯¯: status_code={datadict.get('status_code')}ï¼Œå°è¯• {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    return None
                
                # æ£€æŸ¥aweme_list
                if "aweme_list" not in datadict:
                    logger.warning(f"å“åº”ä¸­ç¼ºå°‘aweme_listå­—æ®µï¼Œå°è¯• {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    return None
                
                aweme_list = datadict.get("aweme_list", [])
                has_more = datadict.get("has_more", False)
                max_cursor = datadict.get("max_cursor", cursor)
                
                logger.info(f"âœ… æˆåŠŸè·å– {len(aweme_list)} ä¸ªä½œå“ï¼Œhas_more={has_more}, cursor={max_cursor}")
                
                # è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                from apiproxy.douyin.douyin import Douyin
                from apiproxy.douyin.result import Result
                
                dy = Douyin(database=False)
                result_obj = Result()
                converted_list = []
                
                for aweme in aweme_list:
                    result_obj.clearDict(result_obj.awemeDict)
                    aweme_type = 1 if aweme.get("images") else 0
                    result_obj.dataConvert(aweme_type, result_obj.awemeDict, aweme)
                    import copy
                    converted_list.append(copy.deepcopy(result_obj.awemeDict))
                
                return {
                    'status_code': 0,
                    'aweme_list': converted_list,
                    'max_cursor': max_cursor,
                    'has_more': has_more  # âœ… çœŸå®çš„has_moreå€¼
                }
                    
            except Exception as e:
                logger.error(f"è·å–ç”¨æˆ·ä½œå“åˆ—è¡¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)
                    continue
                import traceback
                traceback.print_exc()
        
        logger.error(f"é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥")
        return None
    
    async def _download_user_likes(self, user_id: str):
        """ä¸‹è½½ç”¨æˆ·å–œæ¬¢çš„ä½œå“"""
        max_count = 0
        try:
            max_count = int(self.config.get('number', {}).get('like', 0))
        except Exception:
            max_count = 0
        cursor = 0
        downloaded = 0

        console.print(f"\n[green]å¼€å§‹ä¸‹è½½ç”¨æˆ·å–œæ¬¢çš„ä½œå“...[/green]")

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
            console=console
        ) as progress:

            while True:
                # é™é€Ÿ
                await self.rate_limiter.acquire()

                # è·å–å–œæ¬¢åˆ—è¡¨
                likes_data = await self._fetch_user_likes(user_id, cursor)
                if not likes_data:
                    break

                aweme_list = likes_data.get('aweme_list', [])
                if not aweme_list:
                    break

                # ä¸‹è½½ä½œå“
                for aweme in aweme_list:
                    if max_count > 0 and downloaded >= max_count:
                        console.print(f"[yellow]å·²è¾¾åˆ°ä¸‹è½½æ•°é‡é™åˆ¶: {max_count}[/yellow]")
                        return

                    if not self._check_time_filter(aweme):
                        continue

                    task_id = progress.add_task(
                        f"ä¸‹è½½å–œæ¬¢ {downloaded + 1}",
                        total=100
                    )

                    # å¢é‡åˆ¤æ–­
                    if self._should_skip_increment('like', aweme, sec_uid=user_id):
                        continue

                    success = await self._download_media_files(aweme, progress)

                    if success:
                        downloaded += 1
                        progress.update(task_id, completed=100)
                        self._record_increment('like', aweme, sec_uid=user_id)
                    else:
                        progress.update(task_id, description="[red]ä¸‹è½½å¤±è´¥[/red]")

                # ç¿»é¡µ
                if not likes_data.get('has_more'):
                    break
                cursor = likes_data.get('max_cursor', 0)

        console.print(f"[green]âœ… å–œæ¬¢ä½œå“ä¸‹è½½å®Œæˆï¼Œå…±ä¸‹è½½ {downloaded} ä¸ª[/green]")

    async def _fetch_user_likes(self, user_id: str, cursor: int = 0) -> Optional[Dict]:
        """è·å–ç”¨æˆ·å–œæ¬¢çš„ä½œå“åˆ—è¡¨"""
        try:
            params_list = [
                f'sec_user_id={user_id}',
                f'max_cursor={cursor}',
                'count=35',
                'aid=6383',
                'device_platform=webapp',
                'channel=channel_pc_web',
                'pc_client_type=1',
                'version_code=170400',
                'version_name=17.4.0',
                'cookie_enabled=true',
                'screen_width=1920',
                'screen_height=1080',
                'browser_language=zh-CN',
                'browser_platform=MacIntel',
                'browser_name=Chrome',
                'browser_version=122.0.0.0',
                'browser_online=true'
            ]
            params = '&'.join(params_list)

            api_url = self.urls_helper.USER_FAVORITE_A

            try:
                xbogus = self.utils.getXbogus(params)
                full_url = f"{api_url}{params}&X-Bogus={xbogus}"
            except Exception as e:
                logger.warning(f"è·å–X-Boguså¤±è´¥: {e}, å°è¯•ä¸å¸¦X-Bogus")
                full_url = f"{api_url}{params}"

            logger.info(f"è¯·æ±‚ç”¨æˆ·å–œæ¬¢åˆ—è¡¨: {full_url[:100]}...")

            async with aiohttp.ClientSession() as session:
                async with session.get(full_url, headers=self.headers, timeout=10) as response:
                    if response.status != 200:
                        logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return None

                    text = await response.text()
                    if not text:
                        logger.error("å“åº”å†…å®¹ä¸ºç©º")
                        return None

                    data = json.loads(text)
                    if data.get('status_code') == 0:
                        return data
                    else:
                        logger.error(f"APIè¿”å›é”™è¯¯: {data.get('status_msg', 'æœªçŸ¥é”™è¯¯')}")
                        return None
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·å–œæ¬¢åˆ—è¡¨å¤±è´¥: {e}")
        return None

    async def _download_user_mixes(self, user_id: str):
        """ä¸‹è½½ç”¨æˆ·çš„æ‰€æœ‰åˆé›†ï¼ˆæŒ‰é…ç½®å¯é™åˆ¶æ•°é‡ï¼‰"""
        max_allmix = 0
        try:
            # å…¼å®¹æ—§é”®å allmix æˆ– mix
            number_cfg = self.config.get('number', {}) or {}
            max_allmix = int(number_cfg.get('allmix', number_cfg.get('mix', 0)) or 0)
        except Exception:
            max_allmix = 0

        cursor = 0
        fetched = 0

        console.print(f"\n[green]å¼€å§‹è·å–ç”¨æˆ·åˆé›†åˆ—è¡¨...[/green]")
        while True:
            await self.rate_limiter.acquire()
            mix_list_data = await self._fetch_user_mix_list(user_id, cursor)
            if not mix_list_data:
                break

            mix_infos = mix_list_data.get('mix_infos') or []
            if not mix_infos:
                break

            for mix in mix_infos:
                if max_allmix > 0 and fetched >= max_allmix:
                    console.print(f"[yellow]å·²è¾¾åˆ°åˆé›†æ•°é‡é™åˆ¶: {max_allmix}[/yellow]")
                    return
                mix_id = mix.get('mix_id')
                mix_name = mix.get('mix_name', '')
                console.print(f"[cyan]ä¸‹è½½åˆé›†[/cyan]: {mix_name} ({mix_id})")
                await self._download_mix_by_id(mix_id)
                fetched += 1

            if not mix_list_data.get('has_more'):
                break
            cursor = mix_list_data.get('cursor', 0)

        console.print(f"[green]âœ… ç”¨æˆ·åˆé›†ä¸‹è½½å®Œæˆï¼Œå…±å¤„ç† {fetched} ä¸ª[/green]")

    async def _fetch_user_mix_list(self, user_id: str, cursor: int = 0) -> Optional[Dict]:
        """è·å–ç”¨æˆ·åˆé›†åˆ—è¡¨"""
        try:
            params_list = [
                f'sec_user_id={user_id}',
                f'cursor={cursor}',
                'count=35',
                'aid=6383',
                'device_platform=webapp',
                'channel=channel_pc_web',
                'pc_client_type=1',
                'version_code=170400',
                'version_name=17.4.0',
                'cookie_enabled=true',
                'screen_width=1920',
                'screen_height=1080',
                'browser_language=zh-CN',
                'browser_platform=MacIntel',
                'browser_name=Chrome',
                'browser_version=122.0.0.0',
                'browser_online=true'
            ]
            params = '&'.join(params_list)

            api_url = self.urls_helper.USER_MIX_LIST
            try:
                xbogus = self.utils.getXbogus(params)
                full_url = f"{api_url}{params}&X-Bogus={xbogus}"
            except Exception as e:
                logger.warning(f"è·å–X-Boguså¤±è´¥: {e}, å°è¯•ä¸å¸¦X-Bogus")
                full_url = f"{api_url}{params}"

            logger.info(f"è¯·æ±‚ç”¨æˆ·åˆé›†åˆ—è¡¨: {full_url[:100]}...")
            async with aiohttp.ClientSession() as session:
                async with session.get(full_url, headers=self.headers, timeout=10) as response:
                    if response.status != 200:
                        logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return None
                    text = await response.text()
                    if not text:
                        logger.error("å“åº”å†…å®¹ä¸ºç©º")
                        return None
                    data = json.loads(text)
                    if data.get('status_code') == 0:
                        return data
                    else:
                        logger.error(f"APIè¿”å›é”™è¯¯: {data.get('status_msg', 'æœªçŸ¥é”™è¯¯')}")
                        return None
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·åˆé›†åˆ—è¡¨å¤±è´¥: {e}")
        return None

    async def download_mix(self, url: str) -> bool:
        """æ ¹æ®åˆé›†é“¾æ¥ä¸‹è½½åˆé›†å†…æ‰€æœ‰ä½œå“"""
        try:
            mix_id = None
            for pattern in [r'/collection/(\d+)', r'/mix/detail/(\d+)']:
                m = re.search(pattern, url)
                if m:
                    mix_id = m.group(1)
                    break
            if not mix_id:
                logger.error(f"æ— æ³•ä»åˆé›†é“¾æ¥æå–ID: {url}")
                return False
            await self._download_mix_by_id(mix_id)
            return True
        except Exception as e:
            logger.error(f"ä¸‹è½½åˆé›†å¤±è´¥: {e}")
            return False

    async def _download_mix_by_id(self, mix_id: str):
        """æŒ‰åˆé›†IDä¸‹è½½å…¨éƒ¨ä½œå“"""
        cursor = 0
        downloaded = 0

        console.print(f"\n[green]å¼€å§‹ä¸‹è½½åˆé›† {mix_id} ...[/green]")

        while True:
            await self.rate_limiter.acquire()
            data = await self._fetch_mix_awemes(mix_id, cursor)
            if not data:
                break

            aweme_list = data.get('aweme_list') or []
            if not aweme_list:
                break

            for aweme in aweme_list:
                success = await self._download_media_files(aweme)
                if success:
                    downloaded += 1

            if not data.get('has_more'):
                break
            cursor = data.get('cursor', 0)

        console.print(f"[green]âœ… åˆé›†ä¸‹è½½å®Œæˆï¼Œå…±ä¸‹è½½ {downloaded} ä¸ª[/green]")

    async def _fetch_mix_awemes(self, mix_id: str, cursor: int = 0) -> Optional[Dict]:
        """è·å–åˆé›†ä¸‹ä½œå“åˆ—è¡¨"""
        try:
            params_list = [
                f'mix_id={mix_id}',
                f'cursor={cursor}',
                'count=35',
                'aid=6383',
                'device_platform=webapp',
                'channel=channel_pc_web',
                'pc_client_type=1',
                'version_code=170400',
                'version_name=17.4.0',
                'cookie_enabled=true',
                'screen_width=1920',
                'screen_height=1080',
                'browser_language=zh-CN',
                'browser_platform=MacIntel',
                'browser_name=Chrome',
                'browser_version=122.0.0.0',
                'browser_online=true'
            ]
            params = '&'.join(params_list)

            api_url = self.urls_helper.USER_MIX
            try:
                xbogus = self.utils.getXbogus(params)
                full_url = f"{api_url}{params}&X-Bogus={xbogus}"
            except Exception as e:
                logger.warning(f"è·å–X-Boguså¤±è´¥: {e}, å°è¯•ä¸å¸¦X-Bogus")
                full_url = f"{api_url}{params}"

            logger.info(f"è¯·æ±‚åˆé›†ä½œå“åˆ—è¡¨: {full_url[:100]}...")
            async with aiohttp.ClientSession() as session:
                async with session.get(full_url, headers=self.headers, timeout=10) as response:
                    if response.status != 200:
                        logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return None
                    text = await response.text()
                    if not text:
                        logger.error("å“åº”å†…å®¹ä¸ºç©º")
                        return None
                    data = json.loads(text)
                    # USER_MIX è¿”å›æ²¡æœ‰ç»Ÿä¸€çš„ status_codeï¼Œè¿™é‡Œç›´æ¥è¿”å›
                    return data
        except Exception as e:
            logger.error(f"è·å–åˆé›†ä½œå“å¤±è´¥: {e}")
        return None

    async def download_music(self, url: str) -> bool:
        """æ ¹æ®éŸ³ä¹é¡µé“¾æ¥ä¸‹è½½éŸ³ä¹ä¸‹çš„æ‰€æœ‰ä½œå“ï¼ˆæ”¯æŒå¢é‡ï¼‰"""
        try:
            # æå– music_id
            music_id = None
            m = re.search(r'/music/(\d+)', url)
            if m:
                music_id = m.group(1)
            if not music_id:
                logger.error(f"æ— æ³•ä»éŸ³ä¹é“¾æ¥æå–ID: {url}")
                return False

            cursor = 0
            downloaded = 0
            limit_num = 0
            try:
                limit_num = int((self.config.get('number', {}) or {}).get('music', 0))
            except Exception:
                limit_num = 0

            console.print(f"\n[green]å¼€å§‹ä¸‹è½½éŸ³ä¹ {music_id} ä¸‹çš„ä½œå“...[/green]")

            while True:
                await self.rate_limiter.acquire()
                data = await self._fetch_music_awemes(music_id, cursor)
                if not data:
                    break
                aweme_list = data.get('aweme_list') or []
                if not aweme_list:
                    break

                for aweme in aweme_list:
                    if limit_num > 0 and downloaded >= limit_num:
                        console.print(f"[yellow]å·²è¾¾åˆ°éŸ³ä¹ä¸‹è½½æ•°é‡é™åˆ¶: {limit_num}[/yellow]")
                        return True
                    if self._should_skip_increment('music', aweme, music_id=music_id):
                        continue
                    success = await self._download_media_files(aweme)
                    if success:
                        downloaded += 1
                        self._record_increment('music', aweme, music_id=music_id)

                if not data.get('has_more'):
                    break
                cursor = data.get('cursor', 0)

            console.print(f"[green]âœ… éŸ³ä¹ä½œå“ä¸‹è½½å®Œæˆï¼Œå…±ä¸‹è½½ {downloaded} ä¸ª[/green]")
            return True
        except Exception as e:
            logger.error(f"ä¸‹è½½éŸ³ä¹é¡µå¤±è´¥: {e}")
            return False

    async def _fetch_music_awemes(self, music_id: str, cursor: int = 0) -> Optional[Dict]:
        """è·å–éŸ³ä¹ä¸‹ä½œå“åˆ—è¡¨"""
        try:
            params_list = [
                f'music_id={music_id}',
                f'cursor={cursor}',
                'count=35',
                'aid=6383',
                'device_platform=webapp',
                'channel=channel_pc_web',
                'pc_client_type=1',
                'version_code=170400',
                'version_name=17.4.0',
                'cookie_enabled=true',
                'screen_width=1920',
                'screen_height=1080',
                'browser_language=zh-CN',
                'browser_platform=MacIntel',
                'browser_name=Chrome',
                'browser_version=122.0.0.0',
                'browser_online=true'
            ]
            params = '&'.join(params_list)

            api_url = self.urls_helper.MUSIC
            try:
                xbogus = self.utils.getXbogus(params)
                full_url = f"{api_url}{params}&X-Bogus={xbogus}"
            except Exception as e:
                logger.warning(f"è·å–X-Boguså¤±è´¥: {e}, å°è¯•ä¸å¸¦X-Bogus")
                full_url = f"{api_url}{params}"

            logger.info(f"è¯·æ±‚éŸ³ä¹ä½œå“åˆ—è¡¨: {full_url[:100]}...")
            async with aiohttp.ClientSession() as session:
                async with session.get(full_url, headers=self.headers, timeout=10) as response:
                    if response.status != 200:
                        logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return None
                    text = await response.text()
                    if not text:
                        logger.error("å“åº”å†…å®¹ä¸ºç©º")
                        return None
                    data = json.loads(text)
                    return data
        except Exception as e:
            logger.error(f"è·å–éŸ³ä¹ä½œå“å¤±è´¥: {e}")
        return None
    
    def _check_time_filter(self, aweme: Dict) -> bool:
        """æ£€æŸ¥æ—¶é—´è¿‡æ»¤"""
        start_time = self.config.get('start_time')
        end_time = self.config.get('end_time')
        
        if not start_time and not end_time:
            return True
        
        raw_create_time = aweme.get('create_time')
        if not raw_create_time:
            return True
        
        create_date = None
        if isinstance(raw_create_time, (int, float)):
            try:
                create_date = datetime.fromtimestamp(raw_create_time)
            except Exception:
                create_date = None
        elif isinstance(raw_create_time, str):
            for fmt in ('%Y-%m-%d %H.%M.%S', '%Y-%m-%d_%H-%M-%S', '%Y-%m-%d %H:%M:%S'):
                try:
                    create_date = datetime.strptime(raw_create_time, fmt)
                    break
                except Exception:
                    pass
        
        if create_date is None:
            return True
        
        if start_time:
            start_date = datetime.strptime(start_time, '%Y-%m-%d')
            if create_date < start_date:
                return False
        
        if end_time:
            end_date = datetime.strptime(end_time, '%Y-%m-%d')
            if create_date > end_date:
                return False
        
        return True
    
    async def run(self):
        """è¿è¡Œä¸‹è½½å™¨"""
        # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
        console.print(Panel.fit(
            "[bold cyan]æŠ–éŸ³ä¸‹è½½å™¨ v3.0 - ç»Ÿä¸€å¢å¼ºç‰ˆ[/bold cyan]\n"
            "[dim]æ”¯æŒè§†é¢‘ã€å›¾æ–‡ã€ç”¨æˆ·ä¸»é¡µã€åˆé›†æ‰¹é‡ä¸‹è½½[/dim]",
            border_style="cyan"
        ))
        
        # åˆå§‹åŒ–Cookieä¸è¯·æ±‚å¤´
        await self._initialize_cookies_and_headers()
        
        # è·å–URLåˆ—è¡¨
        urls = self.config.get('link', [])
        # å…¼å®¹ï¼šå•æ¡å­—ç¬¦ä¸²
        if isinstance(urls, str):
            urls = [urls]
        if not urls:
            console.print("[red]æ²¡æœ‰æ‰¾åˆ°è¦ä¸‹è½½çš„é“¾æ¥ï¼[/red]")
            return
        
        # åˆ†æURLç±»å‹
        console.print(f"\n[cyan]ğŸ“Š é“¾æ¥åˆ†æ[/cyan]")
        url_types = {}
        for url in urls:
            content_type = self.detect_content_type(url)
            url_types[url] = content_type
            console.print(f"  â€¢ {content_type.upper()}: {url[:50]}...")
        
        # å¼€å§‹ä¸‹è½½
        console.print(f"\n[green]â³ å¼€å§‹ä¸‹è½½ {len(urls)} ä¸ªé“¾æ¥...[/green]\n")
        
        for i, url in enumerate(urls, 1):
            content_type = url_types[url]
            console.print(f"[{i}/{len(urls)}] å¤„ç†: {url}")
            
            if content_type == ContentType.VIDEO or content_type == ContentType.IMAGE:
                await self.download_single_video(url)
            elif content_type == ContentType.USER:
                await self.download_user_page(url)
                # è‹¥é…ç½®åŒ…å« like æˆ– mixï¼Œé¡ºå¸¦å¤„ç†
                modes = self.config.get('mode', ['post'])
                if 'like' in modes:
                    user_id = self.extract_id_from_url(url, ContentType.USER)
                    if user_id:
                        await self._download_user_likes(user_id)
                if 'mix' in modes:
                    user_id = self.extract_id_from_url(url, ContentType.USER)
                    if user_id:
                        await self._download_user_mixes(user_id)
            elif content_type == ContentType.MIX:
                await self.download_mix(url)
            elif content_type == ContentType.MUSIC:
                await self.download_music(url)
            else:
                console.print(f"[yellow]ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {content_type}[/yellow]")
            
            # æ˜¾ç¤ºè¿›åº¦
            console.print(f"è¿›åº¦: {i}/{len(urls)} | æˆåŠŸ: {self.stats.success} | å¤±è´¥: {self.stats.failed}")
            console.print("-" * 60)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        self._show_stats()
    
    def _show_stats(self):
        """æ˜¾ç¤ºä¸‹è½½ç»Ÿè®¡"""
        console.print("\n" + "=" * 60)
        
        # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
        table = Table(title="ğŸ“Š ä¸‹è½½ç»Ÿè®¡", show_header=True, header_style="bold magenta")
        table.add_column("é¡¹ç›®", style="cyan", width=12)
        table.add_column("æ•°å€¼", style="green")
        
        stats = self.stats.to_dict()
        table.add_row("æ€»ä»»åŠ¡æ•°", str(stats['total']))
        table.add_row("æˆåŠŸ", str(stats['success']))
        table.add_row("å¤±è´¥", str(stats['failed']))
        table.add_row("è·³è¿‡", str(stats['skipped']))
        table.add_row("æˆåŠŸç‡", stats['success_rate'])
        table.add_row("ç”¨æ—¶", stats['elapsed_time'])
        
        console.print(table)
        console.print("\n[bold green]âœ… ä¸‹è½½ä»»åŠ¡å®Œæˆï¼[/bold green]")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æŠ–éŸ³ä¸‹è½½å™¨ - ç»Ÿä¸€å¢å¼ºç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '-c', '--config',
        default='config.yml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.ymlï¼Œè‡ªåŠ¨å…¼å®¹ config_simple.yml)'
    )
    
    parser.add_argument(
        '-u', '--url',
        nargs='+',
        help='ç›´æ¥æŒ‡å®šè¦ä¸‹è½½çš„URL'
    )
    parser.add_argument(
        '-p', '--path',
        default=None,
        help='ä¿å­˜è·¯å¾„ (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )
    parser.add_argument(
        '--auto-cookie',
        action='store_true',
        help='è‡ªåŠ¨è·å–Cookieï¼ˆéœ€è¦å·²å®‰è£…Playwrightï¼‰'
    )
    parser.add_argument(
        '--cookie',
        help='æ‰‹åŠ¨æŒ‡å®šCookieå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "msToken=xxx; ttwid=yyy"'
    )
    
    args = parser.parse_args()
    
    # ç»„åˆé…ç½®æ¥æºï¼šä¼˜å…ˆå‘½ä»¤è¡Œ
    temp_config = {}
    if args.url:
        temp_config['link'] = args.url
    
    # è¦†ç›–ä¿å­˜è·¯å¾„
    if args.path:
        temp_config['path'] = args.path
    
    # Cookieé…ç½®
    if args.auto_cookie:
        temp_config['auto_cookie'] = True
        temp_config['cookies'] = 'auto'
    if args.cookie:
        temp_config['cookies'] = args.cookie
        temp_config['auto_cookie'] = False
    
    # å¦‚æœå­˜åœ¨ä¸´æ—¶é…ç½®ï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¾›ç°æœ‰æ„é€ å‡½æ•°ä½¿ç”¨
    if temp_config:
        # åˆå¹¶æ–‡ä»¶é…ç½®ï¼ˆå¦‚å­˜åœ¨ï¼‰
        file_config = {}
        if os.path.exists(args.config):
            try:
                with open(args.config, 'r', encoding='utf-8') as f:
                    file_config = yaml.safe_load(f) or {}
            except Exception:
                file_config = {}
        
        # å…¼å®¹ç®€åŒ–é”®å
        if 'links' in file_config and 'link' not in file_config:
            file_config['link'] = file_config['links']
        if 'output_dir' in file_config and 'path' not in file_config:
            file_config['path'] = file_config['output_dir']
        if 'cookie' in file_config and 'cookies' not in file_config:
            file_config['cookies'] = file_config['cookie']
        
        merged = {**(file_config or {}), **temp_config}
        with open('temp_config.yml', 'w', encoding='utf-8') as f:
            yaml.dump(merged, f, allow_unicode=True)
        config_path = 'temp_config.yml'
    else:
        config_path = args.config
    
    # è¿è¡Œä¸‹è½½å™¨
    try:
        downloader = UnifiedDownloader(config_path)
        asyncio.run(downloader.run())
    except KeyboardInterrupt:
        console.print("\n[yellow]âš ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½[/yellow]")
    except Exception as e:
        console.print(f"\n[red]âŒ ç¨‹åºå¼‚å¸¸: {e}[/red]")
        logger.exception("ç¨‹åºå¼‚å¸¸")
    finally:
        # æ¸…ç†ä¸´æ—¶é…ç½®
        if args.url and os.path.exists('temp_config.yml'):
            os.remove('temp_config.yml')


if __name__ == '__main__':
    main()
