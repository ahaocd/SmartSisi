"""
ğŸ”¥ æŠ–éŸ³è¯„è®ºæŠ“å–å™¨ - 4æ–¹æ¡ˆè‡ªåŠ¨åˆ‡æ¢ç‰ˆ

æ–¹æ¡ˆ1: è¯­ä¹‰æœç´¢è¯„è®ºåŒºDOM
æ–¹æ¡ˆ2: JSæ³¨å…¥ç›´æ¥è¯»å–
æ–¹æ¡ˆ3: HTMLæ­£åˆ™è§£æ
æ–¹æ¡ˆ4: äº¤äº’å…ƒç´ è·å–

è‡ªåŠ¨åˆ‡æ¢ç›´åˆ°æˆåŠŸï¼
"""

import asyncio
import json
import logging
import aiohttp
import re
from typing import List, Dict, Optional
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CommentFetcher:
    """æŠ–éŸ³è¯„è®ºæŠ“å–å™¨ - 4æ–¹æ¡ˆè‡ªåŠ¨åˆ‡æ¢"""

    def __init__(self, mcp_chrome_url: str = "http://127.0.0.1:12306/mcp", chrome_debug_port: int = 9222):
        self.mcp_chrome_url = mcp_chrome_url
        self.chrome_debug_port = chrome_debug_port
        self.session_id = None
        self.current_tab_id = None

    async def get_comments(self, video_url: str, limit: int = 500, method: str = "auto") -> List[Dict]:
        """
        è·å–è¯„è®º - 4æ–¹æ¡ˆè‡ªåŠ¨åˆ‡æ¢

        Args:
            video_url: è§†é¢‘URL
            limit: æœ€å¤šè·å–å¤šå°‘æ¡
            method: "auto"(è‡ªåŠ¨åˆ‡æ¢)

        Returns:
            [{"nickname": "", "comment_text": "", "profile_url": "", "user_id": ""}]
        """
        logger.info(f"ğŸš€ å¼€å§‹æŠ“å–è¯„è®ºï¼ˆ4æ–¹æ¡ˆè‡ªåŠ¨åˆ‡æ¢ï¼‰: {video_url}")

        # ç­‰å¾…è¯„è®ºåŒºåŠ è½½
        logger.info("   â³ ç­‰å¾…è¯„è®ºåŒºåŠ è½½ï¼ˆ10ç§’ï¼‰...")
        await asyncio.sleep(10)

        methods = [
            ("æ–¹æ¡ˆ1_è¯­ä¹‰æœç´¢", self._method1_semantic_search),
            ("æ–¹æ¡ˆ2_JSæ³¨å…¥", self._method2_inject_script),
            ("æ–¹æ¡ˆ3_HTMLæ­£åˆ™", self._method3_html_regex),
            ("æ–¹æ¡ˆ4_äº¤äº’å…ƒç´ ", self._method4_interactive_elements)
        ]

        for method_name, method_func in methods:
            logger.info(f"   ğŸ”„ å°è¯• {method_name}...")
            try:
                comments = await method_func(video_url, limit)

                # éªŒè¯æ•°æ®è´¨é‡
                if comments and self._validate_comments(comments):
                    logger.info(f"   âœ… {method_name} æˆåŠŸï¼è·å– {len(comments)} æ¡è¯„è®º")
                    return comments
                else:
                    logger.warning(f"   âš ï¸ {method_name} æ•°æ®æ— æ•ˆï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ª")
            except Exception as e:
                logger.warning(f"   âŒ {method_name} å¤±è´¥: {e}")
                continue

        logger.error("   âŒ æ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥")
        return []

    async def _method1_semantic_search(self, video_url: str, limit: int) -> List[Dict]:
        """æ–¹æ¡ˆ1: è¯­ä¹‰æœç´¢æ‰¾è¯„è®ºåŒºDOMç‰¹å¾"""
        logger.info("      [è¯­ä¹‰æœç´¢] æœç´¢è¯„è®ºåŒº...")

        # æœç´¢åŒ…å«è¯„è®ºç»“æ„çš„åŒºåŸŸ
        result = await self._call_mcp("search_tabs_content", {
            "query": "ç”¨æˆ·æ˜µç§° è¯„è®ºå†…å®¹ data-e2e comment-item comment-user-nickname"
        })

        # è§£ææœç´¢ç»“æœï¼Œæ‰¾åˆ°ç›¸å…³DOMåŒºåŸŸ
        if not result:
            logger.warning("      [è¯­ä¹‰æœç´¢] æœªæ‰¾åˆ°ç›¸å…³åŒºåŸŸ")
            return []

        logger.info(f"      [è¯­ä¹‰æœç´¢] æ‰¾åˆ°ç›¸å…³åŒºåŸŸï¼Œå°è¯•æå–...")

        # è·å–HTMLå†…å®¹
        html_result = await self._call_mcp("chrome_get_web_content", {
            "selector": "body",
            "htmlContent": True
        })

        html_content = self._extract_html_content(html_result)
        if not html_content:
            return []

        # ç”¨æ­£åˆ™æå–
        return self._parse_comments_from_html(html_content, limit)

    async def _method2_inject_script(self, video_url: str, limit: int) -> List[Dict]:
        """æ–¹æ¡ˆ2: JSæ³¨å…¥ç›´æ¥è¯»å–è¯„è®ºDOM"""
        logger.info("      [JSæ³¨å…¥] æ‰§è¡Œè„šæœ¬...")

        js_code = """
        (() => {
            const comments = [];

            // æŠ–éŸ³è¯„è®ºåŒºå›ºå®šç‰¹å¾
            const items = document.querySelectorAll('[data-e2e="comment-item"]');

            items.forEach(item => {
                // æ˜µç§°
                const nicknameElem = item.querySelector('[data-e2e="comment-user-nickname"]') ||
                                    item.querySelector('.nickname') ||
                                    item.querySelector('a[href*="/user/"] span');
                const nickname = nicknameElem?.textContent?.trim() || '';

                // è¯„è®ºæ–‡æœ¬
                const textElem = item.querySelector('[data-e2e="comment-text"]') ||
                                item.querySelector('.comment-text') ||
                                item.querySelector('[class*="comment"]');
                const text = textElem?.textContent?.trim() || '';

                // ä¸»é¡µé“¾æ¥
                const profileElem = item.querySelector('a[href*="/user/"]');
                const profileUrl = profileElem?.href || '';
                const userId = profileUrl ? profileUrl.split('/user/')[1]?.split('?')[0] : '';

                if (nickname && text && text.length > 3) {
                    // è¿‡æ»¤åƒåœ¾æ–‡æœ¬
                    const invalid = ['ä¸¾æŠ¥', 'è¿æ³•', 'SVG', 'Icon', 'ç®—æ³•æ¨è'];
                    if (!invalid.some(bad => text.includes(bad))) {
                        comments.push({
                            nickname: nickname,
                            comment_text: text,
                            profile_url: profileUrl,
                            user_id: userId
                        });
                    }
                }
            });

            return {success: true, count: comments.length, comments: comments};
        })()
        """

        result = await self._call_mcp("chrome_inject_script", {
            "type": "MAIN",
            "jsScript": js_code
        })

        # è§£æJSè¿”å›ç»“æœ
        comments = self._parse_js_result(result)
        logger.info(f"      [JSæ³¨å…¥] æå–åˆ° {len(comments)} æ¡è¯„è®º")
        return comments[:limit]

    async def _method3_html_regex(self, video_url: str, limit: int) -> List[Dict]:
        """æ–¹æ¡ˆ3: HTMLæ­£åˆ™è§£æï¼ˆåŸæ–¹æ³•ï¼‰"""
        logger.info("      [HTMLæ­£åˆ™] è·å–HTML...")

        html_result = await self._call_mcp("chrome_get_web_content", {
            "selector": "body",
            "htmlContent": True
        })

        html_content = self._extract_html_content(html_result)
        if not html_content:
            return []

        return self._parse_comments_from_html(html_content, limit)

    async def _method4_interactive_elements(self, video_url: str, limit: int) -> List[Dict]:
        """æ–¹æ¡ˆ4: è·å–äº¤äº’å…ƒç´ """
        logger.info("      [äº¤äº’å…ƒç´ ] æŸ¥æ‰¾è¯„è®ºå…ƒç´ ...")

        result = await self._call_mcp("chrome_get_interactive_elements", {
            "textQuery": "å›å¤ ç‚¹èµ",
            "includeCoordinates": True
        })

        # è¿™ä¸ªæ–¹æ³•å¯èƒ½æ‹¿ä¸åˆ°å®Œæ•´è¯„è®ºå†…å®¹ï¼Œä½œä¸ºæœ€åå¤‡ç”¨
        # è¿”å›ç©ºè®©å…¶ä»–æ–¹æ³•å¤„ç†
        logger.warning("      [äº¤äº’å…ƒç´ ] æ­¤æ–¹æ³•æ— æ³•è·å–å®Œæ•´è¯„è®ºå†…å®¹")
        return []

    def _validate_comments(self, comments: List[Dict]) -> bool:
        """éªŒè¯è¯„è®ºæ•°æ®è´¨é‡"""
        if not comments:
            return False

        invalid_texts = ['ä¸¾æŠ¥', 'è¿æ³•å’Œä¸è‰¯ä¿¡æ¯', 'SVG Icon', 'ç®—æ³•æ¨è', 'http']

        # æ£€æŸ¥å‰3æ¡è¯„è®º
        for c in comments[:3]:
            nickname = c.get('nickname', '').strip()
            text = c.get('comment_text', '').strip()

            # å¿…é¡»æœ‰æ˜µç§°å’Œè¯„è®º
            if not nickname or not text:
                logger.warning(f"      [éªŒè¯å¤±è´¥] æ˜µç§°æˆ–è¯„è®ºä¸ºç©º")
                return False

            # è¯„è®ºé•¿åº¦åˆç†
            if len(text) < 2 or len(text) > 500:
                logger.warning(f"      [éªŒè¯å¤±è´¥] è¯„è®ºé•¿åº¦å¼‚å¸¸: {len(text)}")
                return False

            # ä¸èƒ½æ˜¯é¡µé¢å›ºå®šæ–‡æ¡ˆ
            if any(bad in text for bad in invalid_texts):
                logger.warning(f"      [éªŒè¯å¤±è´¥] åŒ…å«åƒåœ¾æ–‡æœ¬: {text[:30]}")
                return False

            # æ˜µç§°ä¸èƒ½æ˜¯å›ºå®šæ–‡å­—
            if nickname in ['ä½œè€…', 'å›å¤', 'åˆ é™¤', 'ä¸¾æŠ¥']:
                logger.warning(f"      [éªŒè¯å¤±è´¥] æ˜µç§°æ˜¯å›ºå®šæ–‡å­—: {nickname}")
                return False

        logger.info(f"      [éªŒè¯é€šè¿‡] æ•°æ®è´¨é‡åˆæ ¼")
        return True

    def _extract_html_content(self, html_result: Dict) -> str:
        """ä»MCPç»“æœæå–HTMLå†…å®¹"""
        html_content = ""
        try:
            if isinstance(html_result, dict):
                content_list = html_result.get('content', [])
                if content_list and isinstance(content_list, list):
                    text_str = content_list[0].get('text', '')

                    if isinstance(text_str, str) and text_str.startswith('{'):
                        try:
                            data = json.loads(text_str)
                            html_content = data.get('htmlContent', '')
                        except:
                            html_content = text_str
                    else:
                        html_content = text_str

            if html_content:
                logger.info(f"      æå–HTMLæˆåŠŸï¼Œé•¿åº¦: {len(html_content)}")
            return html_content
        except Exception as e:
            logger.error(f"      æå–HTMLå¤±è´¥: {e}")
            return ""

    def _parse_comments_from_html(self, html_content: str, limit: int) -> List[Dict]:
        """ä»HTMLä¸­è§£æè¯„è®ºï¼ˆæ­£åˆ™æ–¹æ³•ï¼‰"""
        comments = []

        # æ‰¾åˆ°æ‰€æœ‰è¯„è®ºå—
        comment_starts = [m.start() for m in re.finditer(r'<div[^>]*data-e2e="comment-item"', html_content)]

        comment_blocks = []
        for i, start_pos in enumerate(comment_starts):
            end_pos = comment_starts[i + 1] if i + 1 < len(comment_starts) else len(html_content)
            block_html = html_content[start_pos:end_pos]
            comment_blocks.append(block_html)

        logger.info(f"      æ‰¾åˆ° {len(comment_blocks)} ä¸ªè¯„è®ºå—")

        for block in comment_blocks[:limit]:
            try:
                # æå–æ˜µç§°
                nickname = ""
                nickname_match = re.search(r'data-e2e="comment-user-nickname"[^>]*>([^<]+)<', block)
                if nickname_match:
                    nickname = nickname_match.group(1).strip()

                if not nickname:
                    nickname_match = re.search(r'href="[^"]*\/user\/[^"]*"[^>]*>.*?<span[^>]*>([^<]+)</span>', block, re.DOTALL)
                    if nickname_match:
                        nickname = nickname_match.group(1).strip()

                # æå–è¯„è®º
                comment_text = ""
                text_match = re.search(r'data-e2e="comment-text"[^>]*>([^<]+)<', block)
                if text_match:
                    comment_text = text_match.group(1).strip()

                if not comment_text:
                    all_texts = re.findall(r'>([^<]+)<', block)
                    valid_texts = []
                    for t in all_texts:
                        t = t.strip()
                        if (4 <= len(t) <= 500 and
                            t not in ['ä½œè€…', 'å›å¤', 'åˆ é™¤', 'ä¸¾æŠ¥', 'ç‚¹èµ', 'è¯„è®º', 'åˆ†äº«'] and
                            t != nickname and
                            'ä¸¾æŠ¥' not in t and
                            'è¿æ³•' not in t and
                            'SVG' not in t and
                            not t.startswith('http')):
                            valid_texts.append(t)

                    if valid_texts:
                        comment_text = max(valid_texts, key=len)

                # æå–ä¸»é¡µé“¾æ¥
                profile_match = re.search(r'href="(/user/([^"?]+))', block)
                if profile_match:
                    profile_url = f"https://www.douyin.com{profile_match.group(1)}"
                    user_id = profile_match.group(2)
                else:
                    profile_url = ""
                    user_id = ""

                if nickname and comment_text:
                    comments.append({
                        "nickname": nickname,
                        "comment_text": comment_text,
                        "profile_url": profile_url,
                        "user_id": user_id,
                        "signature": ""
                    })
            except Exception as e:
                logger.warning(f"      è§£æè¯„è®ºå—å¤±è´¥: {e}")
                continue

        # å»é‡
        seen = set()
        unique = []
        for c in comments:
            key = f"{c.get('user_id', '')}_{c.get('comment_text', '')}"
            if key not in seen:
                seen.add(key)
                unique.append(c)

        return unique

    def _parse_js_result(self, result: Dict) -> List[Dict]:
        """è§£æJSæ‰§è¡Œç»“æœ"""
        comments = []
        try:
            if isinstance(result, dict):
                content_list = result.get('content', [])
                if isinstance(content_list, list) and len(content_list) > 0:
                    text_str = content_list[0].get('text', '{}')
                    data = json.loads(text_str) if isinstance(text_str, str) else text_str

                    if isinstance(data, dict):
                        comments = data.get('comments', [])
                        if isinstance(comments, list):
                            return comments
        except Exception as e:
            logger.error(f"      è§£æJSç»“æœå¤±è´¥: {e}")

        return []

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        pass

    async def _call_mcp(self, tool_name: str, args: Dict) -> Dict:
        """è°ƒç”¨mcp-chromeå·¥å…·"""
        connector = aiohttp.TCPConnector()
        async with aiohttp.ClientSession(connector=connector, trust_env=False) as session:
            if self.current_tab_id and isinstance(args, dict):
                args.setdefault("tabId", self.current_tab_id)

            payload = {
                "jsonrpc": "2.0",
                "method": "tools/call",
                "params": {"name": tool_name, "arguments": args},
                "id": int(asyncio.get_event_loop().time() * 1000)
            }
            headers = {
                "mcp-session-id": self.session_id,
                "Accept": "application/json, text/event-stream"
            }

            async with session.post(self.mcp_chrome_url, json=payload, headers=headers, timeout=60) as resp:
                text = await resp.text()

                try:
                    result = json.loads(text)
                    result_obj = result.get("result", {})
                    if isinstance(result_obj, dict) and "tabId" in result_obj:
                        self.current_tab_id = result_obj.get("tabId")
                    return result_obj
                except json.JSONDecodeError:
                    # SSEæ ¼å¼
                    lines = text.split('\n')
                    data_lines = [line[6:] for line in lines if line.startswith('data: ')]
                    if data_lines:
                        for data_json in reversed(data_lines):
                            try:
                                last_data = json.loads(data_json)
                                result_obj = last_data.get("result", {})
                                if isinstance(result_obj, dict) and "tabId" in result_obj:
                                    self.current_tab_id = result_obj.get("tabId")
                                return result_obj
                            except:
                                continue
                    return {}
