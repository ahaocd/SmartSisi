"""
æ¸…ç†å·¥å…· - è‡ªåŠ¨æ¸…ç†media_marketingæ–‡ä»¶å¤¹çš„ä¸´æ—¶æ–‡ä»¶

åŠŸèƒ½ï¼š
1. æ¸…ç†Chromeç¼“å­˜ï¼ˆchrome_data/ï¼‰
2. æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶ï¼ˆè¶…è¿‡30å¤©ï¼‰
3. æ¸…ç†Pythonç¼“å­˜
4. ä¿ç•™å…³é”®æ•°æ®ï¼ˆæ•°æ®åº“ã€Cookieï¼‰
"""

import os
import shutil
from pathlib import Path
from datetime import datetime, timedelta
import logging
from utils import util

logger = logging.getLogger("CleanupUtils")


class MediaMarketingCleaner:
    """åª’ä½“è¥é”€æ–‡ä»¶å¤¹æ¸…ç†å™¨"""
    
    def __init__(self, base_dir: Path = None):
        """åˆå§‹åŒ–æ¸…ç†å™¨
        
        Args:
            base_dir: media_marketingæ–‡ä»¶å¤¹è·¯å¾„
        """
        if base_dir is None:
            base_dir = Path(__file__).parent
        
        self.base_dir = base_dir
        self.chrome_data_dir = base_dir / "chrome_data"
        self.logs_dir = Path(util.ensure_log_dir("tools", "media_marketing"))
        self.pycache_dir = base_dir / "__pycache__"
        
        # ä¸èƒ½åˆ é™¤çš„å…³é”®æ–‡ä»¶/æ–‡ä»¶å¤¹
        self.protected_items = [
            "douyin_marketing.db",
            "cookies",
            "ai_engine.py",
            "prompts.py",
            "test_ai_prompts.py",
            "ç›®æ ‡ä¸»æ’­é…ç½®.json",
            "è¶³æµ´è¿è¥ä¸»æ’­.txt",
            "çƒæˆ¿è¿è¥ä¸»æ’­.txt",
            "æŠ€æœ¯å®ç°è¯´æ˜.md",
            "cleanup_utils.py"
        ]
    
    def clean_chrome_data(self, keep_days: int = 0):
        """æ¸…ç†Chromeç¼“å­˜æ•°æ®
        
        Args:
            keep_days: ä¿ç•™æœ€è¿‘Nå¤©çš„æ•°æ®ï¼Œ0è¡¨ç¤ºå…¨éƒ¨åˆ é™¤
        """
        if not self.chrome_data_dir.exists():
            logger.info("   âœ… chrome_dataä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return
        
        try:
            # è®¡ç®—æ¸…ç†å‰å¤§å°
            before_size = sum(f.stat().st_size for f in self.chrome_data_dir.rglob('*') if f.is_file())
            before_mb = before_size / (1024 * 1024)
            
            if keep_days == 0:
                # å…¨éƒ¨åˆ é™¤
                shutil.rmtree(self.chrome_data_dir)
                self.chrome_data_dir.mkdir(exist_ok=True)
                logger.info(f"   âœ… æ¸…ç†chrome_data: {before_mb:.2f}MB â†’ 0MB")
            else:
                # åªåˆ é™¤æ—§æ–‡ä»¶
                cutoff_time = datetime.now() - timedelta(days=keep_days)
                deleted_count = 0
                
                for item in self.chrome_data_dir.rglob('*'):
                    if item.is_file():
                        mtime = datetime.fromtimestamp(item.stat().st_mtime)
                        if mtime < cutoff_time:
                            item.unlink()
                            deleted_count += 1
                
                after_size = sum(f.stat().st_size for f in self.chrome_data_dir.rglob('*') if f.is_file())
                after_mb = after_size / (1024 * 1024)
                logger.info(f"   âœ… æ¸…ç†chrome_dataæ—§æ–‡ä»¶: {before_mb:.2f}MB â†’ {after_mb:.2f}MB (åˆ é™¤{deleted_count}ä¸ª)")
        
        except Exception as e:
            logger.error(f"   âŒ æ¸…ç†chrome_dataå¤±è´¥: {e}")
    
    def clean_old_logs(self, keep_days: int = 30):
        """æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶
        
        Args:
            keep_days: ä¿ç•™æœ€è¿‘Nå¤©çš„æ—¥å¿—
        """
        if not self.logs_dir.exists():
            logger.info("   âœ… logsç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return
        
        try:
            cutoff_time = datetime.now() - timedelta(days=keep_days)
            deleted_count = 0
            deleted_size = 0
            
            for log_file in self.logs_dir.glob('*.log'):
                # è·³è¿‡README
                if log_file.name.lower() == 'readme.txt':
                    continue
                
                mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                if mtime < cutoff_time:
                    file_size = log_file.stat().st_size
                    log_file.unlink()
                    deleted_count += 1
                    deleted_size += file_size
            
            if deleted_count > 0:
                logger.info(f"   âœ… æ¸…ç†æ—§æ—¥å¿—: åˆ é™¤{deleted_count}ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾{deleted_size/(1024*1024):.2f}MB")
            else:
                logger.info(f"   âœ… æ— éœ€æ¸…ç†æ—¥å¿—ï¼ˆæœ€è¿‘{keep_days}å¤©å†…ï¼‰")
        
        except Exception as e:
            logger.error(f"   âŒ æ¸…ç†æ—¥å¿—å¤±è´¥: {e}")
    
    def clean_pycache(self):
        """æ¸…ç†Pythonç¼“å­˜"""
        if not self.pycache_dir.exists():
            logger.info("   âœ… __pycache__ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return
        
        try:
            shutil.rmtree(self.pycache_dir)
            logger.info("   âœ… æ¸…ç†__pycache__å®Œæˆ")
        except Exception as e:
            logger.error(f"   âŒ æ¸…ç†__pycache__å¤±è´¥: {e}")
    
    def clean_temp_screenshots(self):
        """æ¸…ç†ä¸´æ—¶æˆªå›¾æ–‡ä»¶"""
        try:
            deleted_count = 0
            deleted_size = 0
            
            # åœ¨base_dirå’Œä¸Šçº§ç›®å½•æŸ¥æ‰¾ä¸´æ—¶æˆªå›¾
            for pattern in ['*screenshot*.png', '*ocr*.png', '*temp*.png']:
                for img_file in self.base_dir.glob(pattern):
                    file_size = img_file.stat().st_size
                    img_file.unlink()
                    deleted_count += 1
                    deleted_size += file_size
                
                # ä¹Ÿæ£€æŸ¥ä¸Šçº§ç›®å½•ï¼ˆæ ¹ç›®å½•ï¼‰
                parent_dir = self.base_dir.parent.parent.parent.parent
                for img_file in parent_dir.glob(pattern):
                    if 'SmartSisi' not in str(img_file):
                        file_size = img_file.stat().st_size
                        img_file.unlink()
                        deleted_count += 1
                        deleted_size += file_size
            
            if deleted_count > 0:
                logger.info(f"   âœ… æ¸…ç†ä¸´æ—¶æˆªå›¾: åˆ é™¤{deleted_count}ä¸ªï¼Œé‡Šæ”¾{deleted_size/(1024*1024):.2f}MB")
            else:
                logger.info("   âœ… æ— ä¸´æ—¶æˆªå›¾éœ€è¦æ¸…ç†")
        
        except Exception as e:
            logger.error(f"   âŒ æ¸…ç†ä¸´æ—¶æˆªå›¾å¤±è´¥: {e}")
    
    def run_full_cleanup(self, chrome_keep_days: int = 0, log_keep_days: int = 30):
        """æ‰§è¡Œå®Œæ•´æ¸…ç†
        
        Args:
            chrome_keep_days: Chromeç¼“å­˜ä¿ç•™å¤©æ•°ï¼ˆ0=å…¨åˆ ï¼‰
            log_keep_days: æ—¥å¿—ä¿ç•™å¤©æ•°
        """
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†media_marketingæ–‡ä»¶å¤¹...")
        
        # 1. æ¸…ç†Chromeç¼“å­˜
        self.clean_chrome_data(keep_days=chrome_keep_days)
        
        # 2. æ¸…ç†æ—§æ—¥å¿—
        self.clean_old_logs(keep_days=log_keep_days)
        
        # 3. æ¸…ç†Pythonç¼“å­˜
        self.clean_pycache()
        
        # 4. æ¸…ç†ä¸´æ—¶æˆªå›¾
        self.clean_temp_screenshots()
        
        logger.info("âœ… æ¸…ç†å®Œæˆï¼")
    
    def get_folder_sizes(self) -> dict:
        """è·å–å„æ–‡ä»¶å¤¹å¤§å°ï¼ˆMBï¼‰"""
        sizes = {}
        
        for item in self.base_dir.iterdir():
            if item.is_dir():
                total_size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                sizes[item.name] = round(total_size / (1024 * 1024), 2)
        
        return sizes


# ==================== å‘½ä»¤è¡Œå·¥å…· ====================
if __name__ == "__main__":
    import argparse
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s'
    )
    
    parser = argparse.ArgumentParser(description="æ¸…ç†media_marketingæ–‡ä»¶å¤¹")
    parser.add_argument("--chrome-days", type=int, default=0, help="Chromeç¼“å­˜ä¿ç•™å¤©æ•°ï¼ˆ0=å…¨åˆ ï¼‰")
    parser.add_argument("--log-days", type=int, default=30, help="æ—¥å¿—ä¿ç•™å¤©æ•°")
    parser.add_argument("--check-only", action="store_true", help="ä»…æ£€æŸ¥å¤§å°ï¼Œä¸æ¸…ç†")
    
    args = parser.parse_args()
    
    cleaner = MediaMarketingCleaner()
    
    if args.check_only:
        print("\nğŸ“Š å½“å‰æ–‡ä»¶å¤¹å¤§å°ï¼š")
        sizes = cleaner.get_folder_sizes()
        for folder, size_mb in sorted(sizes.items(), key=lambda x: x[1], reverse=True):
            print(f"   {folder:20s} {size_mb:8.2f} MB")
    else:
        cleaner.run_full_cleanup(
            chrome_keep_days=args.chrome_days,
            log_keep_days=args.log_days
        )









