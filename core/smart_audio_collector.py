#!/usr/bin/env python3
"""
ğŸµ æ™ºèƒ½éŸ³é¢‘æ”¶é›†ç³»ç»Ÿ
è§£å†³éŸ³ä¹è¯†åˆ«é€»è¾‘é—®é¢˜ï¼šåå°å¹¶è¡Œæ”¶é›†â†’æœ¬åœ°åˆ¤æ–­â†’æ‰¹é‡å¤„ç†â†’åœ¨çº¿è¯†åˆ«

æ ¸å¿ƒé€»è¾‘ï¼š
1. åå°æŒç»­æ”¶é›†éŸ³é¢‘ç‰‡æ®µ
2. æœ¬åœ°AIåˆ¤æ–­éŸ³ä¹/å™ªéŸ³/è¯­éŸ³
3. æ”¶é›†åˆ°2æ®µéŸ³ä¹ç‰‡æ®µåå‘é€è¯†åˆ«
4. å¤§æ¨¡å‹ç®¡ç†æ•´ä¸ªæ”¶é›†æµç¨‹
"""

import os
import sys
import time
import threading
import queue
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any
import numpy as np
import librosa
import soundfile as sf
from dataclasses import dataclass
from datetime import datetime, timedelta

# Keep startup fast: defer librosa submodule loading until first real use.
try:
    _ = librosa.__version__
    print("librosa preload mode: lazy")
    LIBROSA_AVAILABLE = True
except Exception as e:
    print(f"librosa init failed: {e}")
    LIBROSA_AVAILABLE = False

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from utils import util
from utils import config_util as cfg

@dataclass
class AudioSegment:
    """éŸ³é¢‘ç‰‡æ®µæ•°æ®ç»“æ„"""
    file_path: str
    timestamp: datetime
    duration: float
    audio_type: str  # 'music', 'speech', 'noise', 'unknown'
    confidence: float
    features: Dict[str, Any]

class SmartAudioCollector:
    """ğŸµ æ™ºèƒ½éŸ³é¢‘æ”¶é›†å™¨"""
    
    def __init__(self):
        self.running = False
        self.collection_thread = None
        self.analysis_thread = None

        # ğŸ¯ éŸ³é¢‘æ”¶é›†å¼€å…³
        self.enabled = True  # å¯ç”¨çœŸå®éŸ³é¢‘æ”¶é›†ç³»ç»Ÿ

        # éŸ³é¢‘æ”¶é›†é…ç½®
        self.segment_duration = 10.0  # æ¯æ®µ10ç§’
        self.collection_interval = 30.0  # æ¯30ç§’æ”¶é›†ä¸€æ¬¡ï¼ˆé™ä½é¢‘ç‡ï¼‰
        self.max_segments = 20  # æœ€å¤šä¿å­˜20æ®µ

        # ğŸ¯ ç´¯ç§¯é€»è¾‘é…ç½® - ç®€åŒ–ä¸ºæ¯3æ¬¡ç›´æ¥å‘é€
        self.analysis_batch_size = 3  # ç´¯ç§¯3æ¬¡åˆ†æåç›´æ¥å‘é€ç»™åŠ¨æ€æç¤ºè¯ä¸­æ¢
        self.analysis_count = 0  # å½“å‰åˆ†ææ¬¡æ•°è®¡æ•°å™¨
        
        # éŸ³é¢‘åˆ†æé…ç½®
        self.music_threshold = 0.8  # éŸ³ä¹åˆ¤æ–­é˜ˆå€¼ (æé«˜åˆ°80%)
        self.speech_threshold = 0.6  # è¯­éŸ³åˆ¤æ–­é˜ˆå€¼
        
        # å­˜å‚¨
        self.audio_segments: List[AudioSegment] = []
        self.music_segments: List[AudioSegment] = []
        self.pending_recognition = queue.Queue()
        
        # ç¼“å­˜ç›®å½•
        try:
            cfg.load_config()
        except Exception:
            pass
        base_cache = Path(cfg.cache_root) if getattr(cfg, "cache_root", None) else (Path(__file__).parent.parent / "cache_data")
        self.cache_dir = base_cache / "audio_segments"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ—¥å¿—
        self.logger = self._setup_logger()
        
        # ğŸ¯ éŸ³ä¹è¯†åˆ«è§¦å‘æ¡ä»¶
        self.music_segments_needed = 3  # æ”¶é›†3æ®µéŸ³ä¹åè§¦å‘è¯†åˆ«
        self.last_recognition_time = datetime.now() - timedelta(minutes=5)
        self.recognition_cooldown = timedelta(minutes=1)  # 1åˆ†é’Ÿå†·å´æ—¶é—´

        # ğŸ¯ ç´¯ç§¯ä¸Šä¸‹æ–‡å­˜å‚¨
        self.accumulated_contexts = []  # ä¿®å¤AttributeError

        # ğŸ¯ æ—¥å¿—è¾“å‡ºé¢‘ç‡æ§åˆ¶
        self.log_counter = 0  # æ—¥å¿—è®¡æ•°å™¨
        self.log_interval = 5  # æ¯5æ¬¡æ”¶é›†æ‰è¾“å‡ºä¸€æ¬¡æ—¥å¿—
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger('smart_audio_collector')
        logger.setLevel(logging.INFO)
        
        log_dir = Path(util.ensure_log_dir("core"))
        
        handler = logging.FileHandler(log_dir / "smart_audio_collector.log", encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s [éŸ³é¢‘æ”¶é›†] %(message)s')
        handler.setFormatter(formatter)
        
        if not logger.handlers:
            logger.addHandler(handler)
        
        return logger
    
    def start_collection(self):
        """å¯åŠ¨éŸ³é¢‘æ”¶é›†"""
        if self.running:
            return

        if not self.enabled:
            self.logger.info("ğŸµ æ™ºèƒ½éŸ³é¢‘æ”¶é›†ç³»ç»Ÿå·²ç¦ç”¨ï¼Œè·³è¿‡å¯åŠ¨")
            return

        self.running = True
        self.logger.info("ğŸµ æ™ºèƒ½éŸ³é¢‘æ”¶é›†ç³»ç»Ÿå¯åŠ¨")

        # å¯åŠ¨æ”¶é›†çº¿ç¨‹
        self.collection_thread = threading.Thread(target=self._collection_loop, daemon=True)
        self.collection_thread.start()

        # å¯åŠ¨åˆ†æçº¿ç¨‹
        self.analysis_thread = threading.Thread(target=self._analysis_loop, daemon=True)
        self.analysis_thread.start()
        
    def stop_collection(self):
        """åœæ­¢éŸ³é¢‘æ”¶é›†"""
        self.running = False
        self.logger.info("ğŸµ æ™ºèƒ½éŸ³é¢‘æ”¶é›†ç³»ç»Ÿåœæ­¢")
        
    def _collection_loop(self):
        """éŸ³é¢‘æ”¶é›†ä¸»å¾ªç¯"""
        while self.running:
            try:
                # è®¾å¤‡ä¸“ç”¨è¾“å…¥æ¨¡å¼ä¸‹ï¼Œä¸è¿›è¡Œæœ¬æœºé‡‡é›†
                try:
                    cfg.load_config()
                    input_mode = cfg.config.get('source', {}).get('input_mode', 'device_only')
                except Exception:
                    input_mode = 'device_only'
                if input_mode == 'device_only':
                    # é™é»˜è·³è¿‡ï¼Œä¸ä½¿ç”¨æœ¬æœºéº¦å…‹é£
                    time.sleep(self.collection_interval)
                    continue
                # ğŸ¯ è¿™é‡Œåº”è¯¥æ¥å…¥æ‚¨çš„éŸ³é¢‘è¾“å…¥æº
                # ç›®å‰å…ˆæ¨¡æ‹Ÿæ”¶é›†é€»è¾‘
                self._collect_audio_segment()
                time.sleep(self.collection_interval)
                
            except Exception as e:
                self.logger.error(f"âŒ éŸ³é¢‘æ”¶é›†å¼‚å¸¸: {str(e)}")
                time.sleep(1)
    
    def _collect_audio_segment(self):
        """æ”¶é›†å•ä¸ªéŸ³é¢‘ç‰‡æ®µ - çœŸå®éŸ³é¢‘å½•åˆ¶"""
        try:
            import pyaudio
            import wave

            timestamp = datetime.now()
            segment_file = self.cache_dir / f"segment_{timestamp.strftime('%Y%m%d_%H%M%S')}.wav"

            # ğŸ¯ çœŸå®éŸ³é¢‘å½•åˆ¶é…ç½®
            CHUNK = 1024
            FORMAT = pyaudio.paInt16
            CHANNELS = 1  # å•å£°é“ï¼Œé€‚åˆè¯­éŸ³åˆ†æ
            RATE = 16000  # 16kHzï¼Œä¸ASRæ¨¡å‹åŒ¹é…
            RECORD_SECONDS = self.segment_duration

            # åˆå§‹åŒ–PyAudio
            p = pyaudio.PyAudio()

            # ğŸ”§ æŸ¥æ‰¾å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡
            input_device_index = None
            for i in range(p.get_device_count()):
                device_info = p.get_device_info_by_index(i)
                if device_info['maxInputChannels'] > 0:
                    input_device_index = i
                    break

            if input_device_index is None:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡ï¼Œè·³è¿‡éŸ³é¢‘æ”¶é›†")
                p.terminate()
                return

            # å¼€å§‹å½•åˆ¶
            stream = p.open(format=FORMAT,
                          channels=CHANNELS,
                          rate=RATE,
                          input=True,
                          input_device_index=input_device_index,
                          frames_per_buffer=CHUNK)

            frames = []
            for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
                data = stream.read(CHUNK)
                frames.append(data)

            # åœæ­¢å½•åˆ¶
            stream.stop_stream()
            stream.close()
            p.terminate()

            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            wf = wave.open(str(segment_file), 'wb')
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(p.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))
            wf.close()

            # åˆ›å»ºéŸ³é¢‘ç‰‡æ®µå¯¹è±¡
            segment = AudioSegment(
                file_path=str(segment_file),
                timestamp=timestamp,
                duration=self.segment_duration,
                audio_type='unknown',  # å¾…åˆ†æ
                confidence=0.0,
                features={}
            )

            # æ·»åŠ åˆ°å¾…åˆ†æé˜Ÿåˆ—
            self.audio_segments.append(segment)

            # é™åˆ¶å­˜å‚¨æ•°é‡
            if len(self.audio_segments) > self.max_segments:
                old_segment = self.audio_segments.pop(0)
                self._cleanup_segment(old_segment)

            # ğŸ¯ æ§åˆ¶æ—¥å¿—è¾“å‡ºé¢‘ç‡ - æ¯5æ¬¡æ”¶é›†æ‰è¾“å‡ºä¸€æ¬¡æ—¥å¿—
            self.log_counter += 1
            if self.log_counter >= self.log_interval:
                self.logger.info(f"ğŸ“¥ æ”¶é›†éŸ³é¢‘ç‰‡æ®µ: {segment.file_path} (ç´¯è®¡{self.log_counter}æ¬¡)")
                self.log_counter = 0  # é‡ç½®è®¡æ•°å™¨
            # é™é»˜æ”¶é›†ï¼Œä¸è¾“å‡ºæ—¥å¿—

        except ImportError:
            self.logger.error("âŒ PyAudioæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install pyaudio")
        except Exception as e:
            self.logger.error(f"âŒ éŸ³é¢‘ç‰‡æ®µæ”¶é›†å¤±è´¥: {str(e)}")
    
    def _analysis_loop(self):
        """éŸ³é¢‘åˆ†æä¸»å¾ªç¯"""
        while self.running:
            try:
                # åˆ†ææœªå¤„ç†çš„éŸ³é¢‘ç‰‡æ®µ
                unanalyzed = [s for s in self.audio_segments if s.audio_type == 'unknown']
                
                for segment in unanalyzed:
                    self._analyze_audio_segment(segment)
                    
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘éŸ³ä¹è¯†åˆ«
                self._check_music_recognition_trigger()
                
                time.sleep(10)  # æ¯10ç§’åˆ†æä¸€æ¬¡ï¼ˆé™ä½é¢‘ç‡ï¼‰
                
            except Exception as e:
                self.logger.error(f"âŒ éŸ³é¢‘åˆ†æå¼‚å¸¸: {str(e)}")
                time.sleep(1)
    
    def _analyze_audio_segment(self, segment: AudioSegment):
        """åˆ†æå•ä¸ªéŸ³é¢‘ç‰‡æ®µ - ä½¿ç”¨å½“å‰ASRé€šé“"""
        try:
            # ğŸ¯ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(segment.file_path):
                segment.audio_type = 'noise'
                segment.confidence = 0.1
                return

            # ğŸ¯ ä½¿ç”¨ç°æœ‰çš„éŸ³é¢‘åˆ†ç±»æ–¹æ³•
            audio_type, confidence = self._classify_audio_type(segment.file_path)
            events = []

            segment.audio_type = audio_type
            segment.confidence = confidence
            segment.features = {
                'events': events,
                'analysis_time': datetime.now().isoformat()
            }

            # æ ‡è®°ï¼šå°è¯•é™„åŠ YAMNet/åˆæˆç»“æœåˆ°features
            try:
                if hasattr(self, '_last_complete_analysis') and isinstance(self._last_complete_analysis, dict):
                    yamnet_result = self._last_complete_analysis.get('yamnet_result', {})
                    combined = self._last_complete_analysis.get('combined_analysis', {})
                    if yamnet_result:
                        segment.features['yamnet_result'] = yamnet_result
                    if combined:
                        segment.features['combined_analysis'] = combined
                    self.logger.info(f"[MARK] ATTACH has_yamnet={bool(yamnet_result)} has_combined={bool(combined)}")
                else:
                    self.logger.info("[MARK] ATTACH skipped: _last_complete_analysis unavailable")
            except Exception as _attach_e:
                self.logger.warning(f"[MARK] ATTACH_FAILED: {_attach_e}")

            # ğŸ¯ æ ¹æ®æ£€æµ‹ç»“æœè¿›è¡Œä¸åŒå¤„ç†
            if audio_type == 'music' and confidence > self.music_threshold:
                self.music_segments.append(segment)
                self.logger.info(f"ğŸµ æ£€æµ‹åˆ°éŸ³ä¹ç‰‡æ®µ: {segment.file_path} (ç½®ä¿¡åº¦: {confidence:.2f})")

                # é™åˆ¶éŸ³ä¹ç‰‡æ®µæ•°é‡
                if len(self.music_segments) > 10:
                    old_music = self.music_segments.pop(0)
                    self._cleanup_segment(old_music)

            elif audio_type == 'speech' and confidence > 0.7:
                # ğŸ¯ æ§åˆ¶è¯­éŸ³æ£€æµ‹æ—¥å¿—é¢‘ç‡ - æ¯5æ¬¡æ‰è¾“å‡ºä¸€æ¬¡
                if self.log_counter == 0:  # åªåœ¨ä¸»æ—¥å¿—è¾“å‡ºæ—¶æ‰æ˜¾ç¤ºè¯­éŸ³æ£€æµ‹
                    self.logger.info(f"ğŸ—£ï¸ æ£€æµ‹åˆ°è¯­éŸ³ç‰‡æ®µ: {segment.file_path} (ç½®ä¿¡åº¦: {confidence:.2f})")
                # å¯ä»¥å‘é€ç»™å£°çº¹è¯†åˆ«ç³»ç»Ÿ

            elif events:
                # ğŸ¯ éŸ³é¢‘äº‹ä»¶æ—¥å¿—ä¹Ÿæ§åˆ¶é¢‘ç‡
                if self.log_counter == 0:
                    self.logger.info(f"ğŸ”Š æ£€æµ‹åˆ°éŸ³é¢‘äº‹ä»¶: {events} (ç½®ä¿¡åº¦: {confidence:.2f})")

            # ğŸ¯ ç´¯ç§¯åˆ†æè®¡æ•°
            self.analysis_count += 1

            # ğŸ¯ åªæœ‰ç´¯ç§¯åˆ°ä¸€å®šæ¬¡æ•°æ‰å‘é€ç»™åŠ¨æ€æç¤ºè¯ç³»ç»Ÿ
            if self.analysis_count >= self.analysis_batch_size:
                self._send_to_dynamic_prompt_system(segment)
                self.analysis_count = 0  # é‡ç½®è®¡æ•°å™¨
                self.logger.info(f"ğŸ“Š ç´¯ç§¯{self.analysis_batch_size}æ¬¡åˆ†æï¼Œå‘é€ç»™åŠ¨æ€æç¤ºè¯ç³»ç»Ÿ")

            # ğŸ¯ å¦‚æœæ˜¯éŸ³ä¹ï¼Œå‘é€ç»™ACRCloudè¯†åˆ«
            if audio_type == 'music' and confidence > 0.6:
                self._send_to_music_recognition(segment)

            # ğŸ¯ å¦‚æœæ˜¯è¯­éŸ³ï¼Œå‘é€ç»™å£°çº¹è¯†åˆ«
            elif audio_type == 'speech' and confidence > 0.7:
                self._send_to_voice_recognition(segment)

        except Exception as e:
            self.logger.error(f"âŒ éŸ³é¢‘ç‰‡æ®µåˆ†æå¤±è´¥: {str(e)}")
            # ğŸ”¥ æ·»åŠ è¯¦ç»†çš„é”™è¯¯è¿½è¸ª
            import traceback
            self.logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            segment.audio_type = 'error'
            segment.confidence = 0.0
    
    def _classify_audio_type(self, audio_file: str) -> tuple:
        """åˆ†ç±»éŸ³é¢‘ç±»å‹ - ä¼˜å…ˆä½¿ç”¨å¹¶è¡Œ(SenseVoice+YAMNet+Librosa)ï¼Œå¤±è´¥å†å›é€€"""
        try:
            if LIBROSA_AVAILABLE:
                try:
                    self.logger.info("[MARK] CLASSIFY path=parallel")
                    return self._sensevoice_audio_classification(audio_file)
                except Exception as _par_e:
                    self.logger.warning(f"[MARK] CLASSIFY parallel_failed: {_par_e}")
                    # å¹¶è¡Œå¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°çœŸå®åˆ†ç±»
                    self.logger.info("[MARK] CLASSIFY path=real_local")
                    return self._real_audio_classification(audio_file)
            else:
                self.logger.info("[MARK] CLASSIFY path=basic_fallback (librosa_unavailable)")
                return self._basic_audio_classification(audio_file)

        except Exception as e:
            self.logger.error(f"âŒ éŸ³é¢‘åˆ†ç±»å¤±è´¥: {str(e)}")
            import traceback
            self.logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # å›é€€åˆ°åŸºç¡€åˆ†æ
            self.logger.info("[MARK] CLASSIFY path=basic_fallback (exception)")
            return self._basic_audio_classification(audio_file)

    def _has_audio_libraries(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰éŸ³é¢‘å¤„ç†åº“"""
        try:
            # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥é¢„åŠ è½½çš„librosaæ˜¯å¦å¯ç”¨
            return LIBROSA_AVAILABLE and librosa is not None and np is not None
        except Exception:
            return False

    def _real_audio_classification(self, audio_file: str) -> tuple:
        """çœŸå®çš„éŸ³é¢‘åˆ†ç±»ï¼ˆéœ€è¦librosaï¼‰"""
        try:
            # ğŸ”¥ æ£€æŸ¥librosaæ˜¯å¦å¯ç”¨
            if not LIBROSA_AVAILABLE:
                self.logger.warning("âš ï¸ librosaä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†ç±»")
                return self._basic_audio_classification(audio_file)

            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›é™éŸ³
            if not Path(audio_file).exists():
                return "silence", 0.9

            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            y, sr = librosa.load(audio_file, sr=None, duration=10)  # åªåˆ†æå‰10ç§’

            # è®¡ç®—éŸ³é¢‘ç‰¹å¾
            # 1. èƒ½é‡ç‰¹å¾
            energy = np.sum(y ** 2)

            # 2. è¿‡é›¶ç‡ï¼ˆè¯­éŸ³ç‰¹å¾ï¼‰
            zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))

            # 3. é¢‘è°±è´¨å¿ƒï¼ˆéŸ³ä¹ç‰¹å¾ï¼‰
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)
            spectral_centroid_mean = np.mean(spectral_centroids)

            # åŸºäºç‰¹å¾è¿›è¡Œåˆ†ç±»
            if energy < 0.001:
                return "silence", 0.95
            elif zero_crossing_rate > 0.1:
                # é«˜è¿‡é›¶ç‡ -> è¯­éŸ³
                confidence = min(0.95, 0.7 + zero_crossing_rate * 2)
                return "speech", confidence
            elif spectral_centroid_mean > 2000:
                # é«˜é¢‘è°±è´¨å¿ƒ -> éŸ³ä¹
                confidence = min(0.95, 0.6 + (spectral_centroid_mean / 5000))
                return "music", confidence
            else:
                # å…¶ä»–æƒ…å†µå½’ç±»ä¸ºå™ªéŸ³
                return "noise", 0.8

        except Exception as e:
            # ğŸ”¥ è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¼‚å¸¸ç±»å‹å’Œå †æ ˆä¿¡æ¯
            import traceback
            error_msg = str(e) if str(e) else "æœªçŸ¥é”™è¯¯"
            error_type = type(e).__name__
            stack_trace = traceback.format_exc()

            self.logger.error(f"âŒ çœŸå®éŸ³é¢‘åˆ†ç±»å¤±è´¥:")
            self.logger.error(f"   æ–‡ä»¶: {audio_file}")
            self.logger.error(f"   å¼‚å¸¸ç±»å‹: {error_type}")
            self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
            self.logger.error(f"   å †æ ˆè·Ÿè¸ª: {stack_trace}")

            # å›é€€åˆ°æ¨¡æ‹Ÿç»“æœ
            import random
            return random.choice(["music", "speech", "noise"]), 0.7

    def _basic_audio_classification(self, audio_file: str) -> tuple:
        """åŸºç¡€éŸ³é¢‘åˆ†ç±» - åŸºäºæ–‡ä»¶å¤§å°å’Œæ—¶é•¿çš„ç®€å•åˆ†æ"""
        try:
            if not Path(audio_file).exists():
                return "silence", 0.9

            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(audio_file)

            # åŸºäºæ–‡ä»¶å¤§å°è¿›è¡Œç®€å•åˆ†ç±»
            if file_size < 1000:  # å°äº1KBï¼Œå¯èƒ½æ˜¯é™éŸ³
                return "silence", 0.8
            elif file_size < 50000:  # å°äº50KBï¼Œå¯èƒ½æ˜¯çŸ­è¯­éŸ³
                return "speech", 0.7
            elif file_size > 200000:  # å¤§äº200KBï¼Œå¯èƒ½æ˜¯éŸ³ä¹
                return "music", 0.6
            else:  # ä¸­ç­‰å¤§å°ï¼Œå¯èƒ½æ˜¯è¯­éŸ³æˆ–å™ªéŸ³
                import random
                return random.choice(["speech", "noise"]), 0.6

        except Exception as e:
            self.logger.error(f"âŒ åŸºç¡€éŸ³é¢‘åˆ†ç±»å¤±è´¥: {str(e)}")
            return "noise", 0.5

    def _sensevoice_audio_classification(self, audio_file: str) -> tuple:
        """ä½¿ç”¨SenseVoice + YAMNet + Librosaå¹¶è¡Œè¿›è¡ŒéŸ³é¢‘åˆ†æ"""
        try:
            import asyncio

            # ğŸ”¥ å¹¶è¡Œå¤„ç†ï¼šSenseVoice + YAMNet + Librosa
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                self.logger.info("âœ… [åå°æ”¶é›†] å¼€å§‹SenseVoice + YAMNet + Librosaå¹¶è¡Œåˆ†æ")

                # ğŸ”§ ä¿®å¤å¼‚æ­¥è­¦å‘Šï¼šç¡®ä¿äº‹ä»¶å¾ªç¯å­˜åœ¨
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                sensevoice_result, yamnet_result, librosa_result = loop.run_until_complete(
                    asyncio.gather(
                        self._run_sensevoice_analysis(audio_file),
                        self._run_yamnet_analysis(audio_file),
                        self._run_librosa_analysis(audio_file),
                        return_exceptions=True
                    )
                )
                self.logger.info("âœ… [åå°æ”¶é›†] å¹¶è¡Œåˆ†æå®Œæˆ - asyncio.gatherè¿”å›")
                self.logger.info(f"âœ… [åå°æ”¶é›†] sensevoice_resultç±»å‹: {type(sensevoice_result)}")
                self.logger.info(f"âœ… [åå°æ”¶é›†] yamnet_resultç±»å‹: {type(yamnet_result)}")
                self.logger.info(f"âœ… [åå°æ”¶é›†] librosa_resultç±»å‹: {type(librosa_result)}")

                # å¤„ç†å¼‚å¸¸
                if isinstance(sensevoice_result, Exception):
                    self.logger.error(f"âŒ [åå°æ”¶é›†] SenseVoiceåˆ†æå¼‚å¸¸: {sensevoice_result}")
                    sensevoice_result = {"text": "", "has_bgm": False, "language": "auto", "emotion": "NEUTRAL", "event": "Speech"}
                else:
                    # ğŸ”¥ æ˜¾ç¤ºSenseVoiceå…·ä½“è¯†åˆ«ç»“æœ
                    text = sensevoice_result.get("text", "")
                    has_bgm = sensevoice_result.get("has_bgm", False)
                    language = sensevoice_result.get("language", "auto")
                    emotion = sensevoice_result.get("emotion", "NEUTRAL")
                    self.logger.info(f"âœ… [åå°æ”¶é›†] SenseVoiceåˆ†ææˆåŠŸ: æ–‡æœ¬='{text}', è¯­è¨€={language}, æƒ…æ„Ÿ={emotion}, èƒŒæ™¯éŸ³ä¹={has_bgm}")

                if isinstance(yamnet_result, Exception):
                    self.logger.error(f"âŒ [åå°æ”¶é›†] YAMNetåˆ†æå¼‚å¸¸: {yamnet_result}")
                    yamnet_result = {"top_class": "Unknown", "confidence": 0.0, "yamnet_scores": [], "environment_detection": {}}
                else:
                    top_class = yamnet_result.get('top_class', 'Unknown')
                    confidence = yamnet_result.get('confidence', 0.0)
                    env_detection = yamnet_result.get('environment_detection', {})
                    self.logger.info(f"âœ… [åå°æ”¶é›†] YAMNetåˆ†ææˆåŠŸ: {top_class} (ç½®ä¿¡åº¦: {confidence:.3f})")
                    self.logger.info(f"âœ… [åå°æ”¶é›†] ç¯å¢ƒæ£€æµ‹: {env_detection}")

                if isinstance(librosa_result, Exception):
                    self.logger.error(f"âŒ [åå°æ”¶é›†] Librosaåˆ†æå¼‚å¸¸: {librosa_result}")
                    librosa_result = {"librosa_features": {}, "tempo_analysis": {}, "audio_properties": {}}
                else:
                    tempo = librosa_result.get('tempo_analysis', {}).get('tempo', 0)
                    duration = librosa_result.get('audio_properties', {}).get('duration', 0)
                    self.logger.info(f"âœ… [åå°æ”¶é›†] Librosaåˆ†ææˆåŠŸ: èŠ‚æ‹={tempo:.1f}BPM, æ—¶é•¿={duration:.2f}s")

                # ğŸ¯ ç»¼åˆåˆ†æç»“æœ - åŒ…å«ä¸‰ä¸ªæ¨¡å—çš„å®Œæ•´æ•°æ®
                self.logger.info("âœ… [åå°æ”¶é›†] å¼€å§‹ç»¼åˆåˆ†æç»“æœ")
                combined_result = self._combine_audio_analysis(sensevoice_result, yamnet_result, librosa_result)
                # æ ‡è®°ï¼šåˆæˆæ˜¯å¦å«YAMNet
                try:
                    _has_yam = isinstance(yamnet_result, dict) and bool(yamnet_result.get('top_class', ''))
                    self.logger.info(f"[MARK] COMBINE has_yamnet={_has_yam} top={yamnet_result.get('top_class','Unknown')} conf={yamnet_result.get('confidence',0.0):.3f}")
                except Exception as _cmb_mark:
                    self.logger.warning(f"[MARK] COMBINE_MARK_FAILED: {_cmb_mark}")
                # å®Œæˆå¹¶è¿”å›
                self.logger.info("âœ… [åå°æ”¶é›†] ç»¼åˆåˆ†æå®Œæˆ")
                return combined_result

            finally:
                loop.close()

        except Exception as e:
            self.logger.error(f"âŒ å¹¶è¡ŒéŸ³é¢‘åˆ†ç±»å¤±è´¥: {e}")
            raise e

    async def _run_sensevoice_analysis(self, audio_file: str) -> dict:
        """è¿è¡ŒSenseVoiceåˆ†æ - é€šè¿‡WebSocketè°ƒç”¨ASRæœåŠ¡å™¨"""
        try:
            # ğŸ¯ æ­£ç¡®çš„æ¶æ„ï¼šé€šè¿‡WebSocketè°ƒç”¨ASRæœåŠ¡å™¨ï¼Œä¸ç›´æ¥å¯¼å…¥æ¨¡å—
            from asr.funasr import FunASR
            import tempfile
            import os

            # åˆ›å»ºä¸´æ—¶ASRå®¢æˆ·ç«¯è¿æ¥ASRæœåŠ¡å™¨
            temp_username = "smart_audio_collector"
            funasr_client = FunASR(temp_username)

            # é€šè¿‡WebSocketå‘é€éŸ³é¢‘æ–‡ä»¶ç»™ASRæœåŠ¡å™¨å¤„ç†
            # è¿™é‡Œéœ€è¦å®ç°éŸ³é¢‘æ–‡ä»¶çš„WebSocketä¼ è¾“
            # æš‚æ—¶è¿”å›ç®€åŒ–ç»“æœï¼Œé¿å…ç ´åæ¶æ„

            self.logger.info("âœ… é€šè¿‡WebSocketè°ƒç”¨ASRæœåŠ¡å™¨å®Œæˆ")
            # ğŸ”¥ SenseVoiceå®˜æ–¹æ ‡å‡†æ ¼å¼ï¼ˆæš‚æ—¶è¿”å›ç©ºå€¼ï¼Œç­‰WebSocketå®ç°ï¼‰
            return {
                "text": "",
                "language": "auto",
                "emotion": "NEUTRAL",
                "event": "Speech",
                "speaker": "spk0",
                "timestamp": [],
                "has_bgm": False,
                "raw_text": "",
                "source": "websocket_asr_server"
            }

        except Exception as e:
            self.logger.error(f"âŒ SenseVoiceåˆ†æå¤±è´¥: {e}")
            # å¦‚æœWebSocketè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°SenseVoiceå®ä¾‹ä½œä¸ºå¤‡ç”¨
            return await self._fallback_sensevoice_analysis(audio_file)

    async def _fallback_sensevoice_analysis(self, audio_file: str) -> dict:
        """å¤‡ç”¨çš„æœ¬åœ°SenseVoiceåˆ†æ - ä»…åœ¨WebSocketè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨"""
        try:
            from funasr import AutoModel

            # åˆå§‹åŒ–æœ¬åœ°SenseVoiceæ¨¡å‹ï¼ˆä»…ä½œä¸ºå¤‡ç”¨ï¼‰
            if not hasattr(self, '_fallback_sensevoice_model'):
                self.logger.warning("âš ï¸ WebSocketè°ƒç”¨å¤±è´¥ï¼Œå¯ç”¨å¤‡ç”¨æœ¬åœ°SenseVoiceå®ä¾‹")
                self._fallback_sensevoice_model = AutoModel(
                    model='iic/SenseVoiceSmall',
                    vad_model="fsmn-vad",
                    vad_kwargs={"max_single_segment_time": 30000},
                    trust_remote_code=True,
                    disable_update=True
                )
                self.logger.info("âœ… å¤‡ç”¨SenseVoiceæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")

            # åˆ†æéŸ³é¢‘
            result = self._fallback_sensevoice_model.generate(
                input=audio_file,
                cache={},
                language="auto",
                use_itn=True,
                batch_size_s=60,
                merge_vad=True,
                merge_length_s=15
            )

            if result and len(result) > 0:
                text = result[0].get('text', '')

                # ğŸ¯ è§£æSenseVoiceå®˜æ–¹æ ‡ç­¾
                language = "auto"
                emotion = "NEUTRAL"
                event = "Speech"

                # è§£æè¯­è¨€æ ‡ç­¾
                if "<|zh|>" in text:
                    language = "zh"
                elif "<|en|>" in text:
                    language = "en"
                elif "<|yue|>" in text:
                    language = "yue"
                elif "<|ja|>" in text:
                    language = "ja"
                elif "<|ko|>" in text:
                    language = "ko"

                # è§£ææƒ…æ„Ÿæ ‡ç­¾
                if "<|HAPPY|>" in text:
                    emotion = "HAPPY"
                elif "<|SAD|>" in text:
                    emotion = "SAD"
                elif "<|ANGRY|>" in text:
                    emotion = "ANGRY"
                elif "<|FEARFUL|>" in text:
                    emotion = "FEARFUL"
                elif "<|DISGUSTED|>" in text:
                    emotion = "DISGUSTED"
                elif "<|SURPRISED|>" in text:
                    emotion = "SURPRISED"
                elif "<|EMO_UNKNOWN|>" in text:
                    emotion = "NEUTRAL"

                # è§£æäº‹ä»¶æ ‡ç­¾
                if "<|BGM|>" in text:
                    event = "BGM"
                elif "<|Applause|>" in text:
                    event = "Applause"
                elif "<|Laughter|>" in text:
                    event = "Laughter"
                elif "<|Cry|>" in text:
                    event = "Cry"
                elif "<|Sneeze|>" in text:
                    event = "Sneeze"
                elif "<|Breath|>" in text:
                    event = "Breath"
                elif "<|Cough|>" in text:
                    event = "Cough"

                # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤æ ‡ç­¾ï¼‰
                clean_text = text
                for tag in ["<|zh|>", "<|en|>", "<|yue|>", "<|ja|>", "<|ko|>",
                           "<|HAPPY|>", "<|SAD|>", "<|ANGRY|>", "<|NEUTRAL|>", "<|FEARFUL|>", "<|DISGUSTED|>", "<|SURPRISED|>", "<|EMO_UNKNOWN|>",
                           "<|BGM|>", "<|Speech|>", "<|Applause|>", "<|Laughter|>", "<|Cry|>", "<|Sneeze|>", "<|Breath|>", "<|Cough|>",
                           "<|withitn|>", "<|woitn|>"]:
                    clean_text = clean_text.replace(tag, "")
                clean_text = clean_text.strip()

                return {
                    # SenseVoiceå®˜æ–¹æ ‡å‡†æ ¼å¼
                    "text": clean_text,
                    "language": language,
                    "emotion": emotion,
                    "event": event,
                    "speaker": "spk0",  # é»˜è®¤è¯´è¯äºº
                    "timestamp": [],  # æ—¶é—´æˆ³ï¼ˆéœ€è¦é¢å¤–å¤„ç†ï¼‰
                    "has_bgm": event == "BGM",
                    "raw_text": text,
                    "source": "fallback_local_sensevoice"
                }

            return {
                "text": "",
                "language": "auto",
                "emotion": "NEUTRAL",
                "event": "Speech",
                "speaker": "spk0",
                "timestamp": [],
                "has_bgm": False,
                "raw_text": "",
                "source": "fallback_empty"
            }

        except Exception as e:
            self.logger.error(f"âŒ å¤‡ç”¨SenseVoiceåˆ†æå¤±è´¥: {e}")
            return {
                "text": "",
                "language": "auto",
                "emotion": "NEUTRAL",
                "event": "Speech",
                "speaker": "spk0",
                "timestamp": [],
                "has_bgm": False,
                "raw_text": "",
                "source": "fallback_error"
            }

    async def _run_librosa_analysis(self, audio_file: str) -> dict:
        """è¿è¡ŒLibrosaç‰¹å¾æå– - å®˜æ–¹æ ‡å‡†æ ¼å¼"""
        try:
            self.logger.info("[MARK] LIBROSA_START å¼€å§‹Librosaåˆ†æ...")
            import librosa
            import numpy as np

            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            self.logger.info("[MARK] LIBROSA_LOAD åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            y, sr = librosa.load(audio_file, sr=None)
            self.logger.info(f"[MARK] LIBROSA_LOADED len={len(y)} sr={sr}")

            # ğŸ¯ Librosaå®˜æ–¹æ ‡å‡†ç‰¹å¾æå–
            # MFCCç‰¹å¾ (13, frames)
            self.logger.info("[MARK] LIBROSA_MFCC è®¡ç®—MFCC...")
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            self.logger.info(f"[MARK] LIBROSA_MFCC_DONE shape={mfcc.shape}")

            # è‰²åº¦ç‰¹å¾ (12, frames)
            self.logger.info("[MARK] LIBROSA_CHROMA è®¡ç®—è‰²åº¦ç‰¹å¾...")
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            self.logger.info(f"[MARK] LIBROSA_CHROMA_DONE shape={chroma.shape}")

            # é¢‘è°±è´¨å¿ƒ (1, frames)
            self.logger.info("[MARK] LIBROSA_CENTROID è®¡ç®—é¢‘è°±è´¨å¿ƒ...")
            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
            self.logger.info(f"[MARK] LIBROSA_CENTROID_DONE shape={spectral_centroid.shape}")

            # é¢‘è°±æ»šé™ (1, frames)
            self.logger.info("[MARK] LIBROSA_ROLLOFF è®¡ç®—é¢‘è°±æ»šé™...")
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            self.logger.info(f"[MARK] LIBROSA_ROLLOFF_DONE shape={spectral_rolloff.shape}")

            # è¿‡é›¶ç‡ (1, frames)
            self.logger.info("[MARK] LIBROSA_ZCR è®¡ç®—è¿‡é›¶ç‡...")
            zero_crossing_rate = librosa.feature.zero_crossing_rate(y)
            self.logger.info(f"[MARK] LIBROSA_ZCR_DONE shape={zero_crossing_rate.shape}")

            # å‡æ–¹æ ¹èƒ½é‡ (1, frames)
            self.logger.info("[MARK] LIBROSA_RMS è®¡ç®—RMS...")
            rms = librosa.feature.rms(y=y)
            self.logger.info(f"[MARK] LIBROSA_RMS_DONE shape={rms.shape}")

            # é¢‘è°±å¸¦å®½ (1, frames)
            self.logger.info("[MARK] LIBROSA_BANDWIDTH è®¡ç®—é¢‘è°±å¸¦å®½...")
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
            self.logger.info(f"[MARK] LIBROSA_BANDWIDTH_DONE shape={spectral_bandwidth.shape}")

            # é¢‘è°±å¯¹æ¯”åº¦ (7, frames)
            self.logger.info("[MARK] LIBROSA_CONTRAST è®¡ç®—é¢‘è°±å¯¹æ¯”åº¦...")
            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
            self.logger.info(f"[MARK] LIBROSA_CONTRAST_DONE shape={spectral_contrast.shape}")

            # æ¢…å°”é¢‘è°± (128, frames)
            self.logger.info("[MARK] LIBROSA_MEL è®¡ç®—æ¢…å°”é¢‘è°±...")
            melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr)
            self.logger.info(f"[MARK] LIBROSA_MEL_DONE shape={melspectrogram.shape}")

            # èŠ‚æ‹è·Ÿè¸ª - ğŸ”§ Python 3.12 å…¼å®¹æ€§é—®é¢˜ï¼šbeat_track ä¼šå¯¼è‡´è¿›ç¨‹å´©æºƒï¼Œç›´æ¥è·³è¿‡
            self.logger.info("[MARK] LIBROSA_BEAT è·³è¿‡ï¼ˆPython 3.12å…¼å®¹æ€§é—®é¢˜ï¼‰")
            tempo = 0.0
            beats = np.array([])

            # èµ·å§‹ç‚¹æ£€æµ‹ - ğŸ”§ Python 3.12 å…¼å®¹æ€§é—®é¢˜ï¼šonset_detect å¯èƒ½ä¹Ÿæœ‰é—®é¢˜ï¼Œç›´æ¥è·³è¿‡
            self.logger.info("[MARK] LIBROSA_ONSET è·³è¿‡ï¼ˆPython 3.12å…¼å®¹æ€§é—®é¢˜ï¼‰")
            onset_frames = np.array([])
            onset_times = np.array([])

            # æ ‡è®°ï¼šLibrosaèŠ‚æ‹/æ—¶é•¿
            try:
                self.logger.info(f"[MARK] LIBROSA tempo={float(tempo):.1f} dur={float(len(y)/sr):.2f}s sr={int(sr)}")
            except Exception as _lib_mark:
                self.logger.warning(f"[MARK] LIBROSA_MARK_FAILED: {_lib_mark}")

            # ğŸ”¥ è¿”å›Librosaå®˜æ–¹æ ‡å‡†æ ¼å¼
            return {
                "librosa_features": {
                    "mfcc": mfcc.tolist(),  # (13, frames)
                    "chroma": chroma.tolist(),  # (12, frames)
                    "spectral_centroid": spectral_centroid.tolist(),  # (1, frames)
                    "spectral_rolloff": spectral_rolloff.tolist(),  # (1, frames)
                    "zero_crossing_rate": zero_crossing_rate.tolist(),  # (1, frames)
                    "rms": rms.tolist(),  # (1, frames)
                    "spectral_bandwidth": spectral_bandwidth.tolist(),  # (1, frames)
                    "spectral_contrast": spectral_contrast.tolist(),  # (7, frames)
                    "melspectrogram": melspectrogram.tolist(),  # (128, frames)
                },
                "tempo_analysis": {
                    "tempo": float(tempo),
                    "beats": beats.tolist(),
                    "onset_times": onset_times.tolist()
                },
                "audio_properties": {
                    "duration": float(len(y) / sr),
                    "sample_rate": int(sr),
                    "total_samples": int(len(y))
                }
            }

        except Exception as e:
            self.logger.error(f"âŒ Librosaç‰¹å¾æå–å¤±è´¥: {e}")
            return {
                "librosa_features": {},
                "tempo_analysis": {},
                "audio_properties": {},
                "error": str(e)
            }

    async def _run_yamnet_analysis(self, audio_file: str) -> dict:
        """è¿è¡ŒYAMNetåˆ†æ - ä½¿ç”¨ç°æœ‰çš„YAMNetå®ç°"""
        try:
            # ğŸ¯ ä½¿ç”¨ç°æœ‰çš„YAMNetå®ç° - ä¿®å¤è·¯å¾„é—®é¢˜
            import sys
            import os

            # è·å–æ­£ç¡®çš„YAMNetè·¯å¾„
            current_dir = os.path.dirname(os.path.abspath(__file__))
            yamnet_dir = os.path.join(current_dir, '..', 'asr', 'yamnet')
            yamnet_dir = os.path.abspath(yamnet_dir)

            if yamnet_dir not in sys.path:
                sys.path.insert(0, yamnet_dir)

            import numpy as np
            import soundfile as sf
            import resampy
            import params as yamnet_params
            import yamnet as yamnet_model

            # åˆå§‹åŒ–YAMNetæ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if not hasattr(self, '_yamnet_model'):
                self._yamnet_params = yamnet_params.Params()
                self._yamnet_model = yamnet_model.yamnet_frames_model(self._yamnet_params)

                # åŠ è½½æƒé‡æ–‡ä»¶ - ä½¿ç”¨ç»å¯¹è·¯å¾„
                weights_path = os.path.join(yamnet_dir, 'yamnet.h5')
                if os.path.exists(weights_path):
                    self._yamnet_model.load_weights(weights_path)
                    self.logger.info(f"âœ… YAMNetæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ - æƒé‡æ–‡ä»¶: {weights_path}")
                else:
                    self.logger.error(f"âŒ YAMNetæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")
                    raise FileNotFoundError(f"YAMNetæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")

                # åŠ è½½ç±»åˆ«åç§° - ä½¿ç”¨ç»å¯¹è·¯å¾„
                class_map_path = os.path.join(yamnet_dir, 'yamnet_class_map.csv')
                if os.path.exists(class_map_path):
                    self._yamnet_classes = yamnet_model.class_names(class_map_path)
                    self.logger.info(f"âœ… YAMNetç±»åˆ«æ˜ å°„åŠ è½½æˆåŠŸ - {len(self._yamnet_classes)}ä¸ªç±»åˆ«")
                else:
                    self.logger.error(f"âŒ YAMNetç±»åˆ«æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {class_map_path}")
                    raise FileNotFoundError(f"YAMNetç±»åˆ«æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {class_map_path}")

            # åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘
            wav_data, sr = sf.read(audio_file, dtype=np.int16)
            waveform = wav_data / 32768.0  # Convert to [-1.0, +1.0]
            waveform = waveform.astype('float32')

            # è½¬æ¢ä¸ºå•å£°é“
            if len(waveform.shape) > 1:
                waveform = np.mean(waveform, axis=1)

            # é‡é‡‡æ ·åˆ°YAMNetæœŸæœ›çš„é‡‡æ ·ç‡
            if sr != self._yamnet_params.sample_rate:
                waveform = resampy.resample(waveform, sr, self._yamnet_params.sample_rate)

            # YAMNeté¢„æµ‹
            scores, embeddings, spectrogram = self._yamnet_model(waveform)
            # è½¬ä¸ºnumpyï¼Œé¿å…EagerTensor.tolistæŠ¥é”™
            try:
                scores_np = scores.numpy() if hasattr(scores, 'numpy') else np.array(scores)
                embeddings_np = embeddings.numpy() if hasattr(embeddings, 'numpy') else np.array(embeddings)
            except Exception as _to_np_e:
                self.logger.warning(f"[MARK] YAMNET_TONUMPY_FAIL: {_to_np_e}")
                scores_np = np.array(scores)
                embeddings_np = np.array(embeddings)
            prediction = np.mean(scores_np, axis=0)

            # æ ‡è®°ï¼šYAMNETè¾“å…¥/é¢„æµ‹
            try:
                self.logger.info(f"[MARK] YAMNET_INPUT len={len(waveform)} sr={int(self._yamnet_params.sample_rate)}")
                _top_idx = int(np.argsort(prediction)[::-1][0]) if prediction.size > 0 else -1
                _top_name = self._yamnet_classes[_top_idx] if (0 <= _top_idx < len(self._yamnet_classes)) else "Unknown"
                _top_conf = float(prediction[_top_idx]) if _top_idx >= 0 else 0.0
                self.logger.info(f"[MARK] YAMNET_PRED top0={_top_name} conf={_top_conf:.3f}")
            except Exception as _mark_e:
                self.logger.warning(f"[MARK] YAMNET_MARK_FAILED: {_mark_e}")

            # ğŸ”§ è°ƒè¯•ï¼šæ·»åŠ æ›´å¤šæ—¥å¿—æ¥å®šä½é€€å‡ºé—®é¢˜
            self.logger.info("[MARK] YAMNET_STEP1 å¼€å§‹è·å–top10...")
            import sys
            sys.stdout.flush()
            
            # ğŸ¯ æŒ‰ç…§YAMNetå®˜æ–¹æ ‡å‡†æ ¼å¼è¾“å‡ºå®Œæ•´æ•°æ®
            # è·å–top10ç»“æœï¼ˆå®˜æ–¹æ¨èï¼‰
            top10_i = np.argsort(prediction)[::-1][:10]
            self.logger.info(f"[MARK] YAMNET_STEP2 top10_i={top10_i}")

            # ğŸ”¥ å®˜æ–¹æ ‡å‡†ï¼šå®Œæ•´çš„521ç±»åˆ«ç½®ä¿¡åº¦çŸ©é˜µ
            self.logger.info("[MARK] YAMNET_STEP3 å¼€å§‹tolist()...")
            yamnet_scores = scores_np.tolist()  # (N, 521) å®Œæ•´çŸ©é˜µ
            self.logger.info(f"[MARK] YAMNET_STEP4 scores_np.tolist()å®Œæˆï¼Œé•¿åº¦={len(yamnet_scores)}")
            yamnet_embeddings = embeddings_np.tolist()  # (N, 1024) åµŒå…¥å‘é‡
            self.logger.info(f"[MARK] YAMNET_STEP5 embeddings_np.tolist()å®Œæˆï¼Œé•¿åº¦={len(yamnet_embeddings)}")

            # ğŸ”¥ å®˜æ–¹æ ‡å‡†ï¼šTopç±»åˆ«åˆ—è¡¨
            self.logger.info("[MARK] YAMNET_STEP6 å¼€å§‹æ„å»ºyamnet_top_classes...")
            yamnet_top_classes = []
            if len(self._yamnet_classes) > 0 and len(top10_i) > 0:
                for i in top10_i:
                    yamnet_top_classes.append({
                        "class": self._yamnet_classes[i],
                        "confidence": float(prediction[i]),
                        "index": int(i)
                    })

                top_class = self._yamnet_classes[top10_i[0]]
                confidence = float(prediction[top10_i[0]])
            else:
                top_class = "Unknown"
                confidence = 0.0
            self.logger.info(f"[MARK] YAMNET_STEP7 yamnet_top_classesæ„å»ºå®Œæˆï¼Œtop_class={top_class}")

            # ğŸ¯ æ£€æµ‹ç‰¹å®šç¯å¢ƒç±»åˆ«ï¼ˆä½ è¦çš„é£æ‰‡ã€é›¨å£°ã€å™ªéŸ³ã€äººç¾¤ï¼‰
            self.logger.info("[MARK] YAMNET_STEP8 å¼€å§‹ç¯å¢ƒæ£€æµ‹...")
            environment_detection = {
                "fan_detected": False,
                "rain_detected": False,
                "crowd_detected": False,
                "noise_detected": False,
                "music_detected": False
            }

            for i, class_name in enumerate(self._yamnet_classes):
                conf = float(prediction[i])
                if conf > 0.3:  # ç½®ä¿¡åº¦é˜ˆå€¼
                    if "fan" in class_name.lower() or "air conditioning" in class_name.lower():
                        environment_detection["fan_detected"] = True
                    elif "rain" in class_name.lower():
                        environment_detection["rain_detected"] = True
                    elif "crowd" in class_name.lower() or "chatter" in class_name.lower():
                        environment_detection["crowd_detected"] = True
                    elif "noise" in class_name.lower():
                        environment_detection["noise_detected"] = True
                    elif "music" in class_name.lower():
                        environment_detection["music_detected"] = True
            self.logger.info(f"[MARK] YAMNET_STEP9 ç¯å¢ƒæ£€æµ‹å®Œæˆ: {environment_detection}")

            # ğŸ”¥ è¿”å›å®˜æ–¹æ ‡å‡†æ ¼å¼
            self.logger.info("[MARK] YAMNET_STEP10 æ„å»ºè¿”å›ç»“æœ...")
            result = {
                # YAMNetå®˜æ–¹æ ‡å‡†è¾“å‡º
                "yamnet_scores": yamnet_scores,  # å®Œæ•´521ç±»åˆ«çŸ©é˜µ
                "yamnet_embeddings": yamnet_embeddings,  # éŸ³é¢‘åµŒå…¥å‘é‡
                "yamnet_top_classes": yamnet_top_classes,  # Top10ç±»åˆ«
                "top_class": top_class,
                "confidence": confidence,
                # ç¯å¢ƒæ£€æµ‹ç»“æœ
                "environment_detection": environment_detection
            }
            self.logger.info("[MARK] YAMNET_STEP11 è¿”å›ç»“æœæ„å»ºå®Œæˆï¼Œå‡†å¤‡è¿”å›")
            return result

        except Exception as e:
            self.logger.error(f"âŒ YAMNetåˆ†æå¤±è´¥: {e}")
            raise e

    def _combine_audio_analysis(self, sensevoice_result: dict, yamnet_result: dict, librosa_result: dict = None) -> tuple:
        """ç»¼åˆSenseVoiceã€YAMNetå’ŒLibrosaçš„åˆ†æç»“æœ - å®˜æ–¹æ ‡å‡†æ ¼å¼"""
        try:
            # ğŸ¯ SenseVoiceå®˜æ–¹æ•°æ®
            text = sensevoice_result.get("text", "")
            language = sensevoice_result.get("language", "auto")
            emotion = sensevoice_result.get("emotion", "NEUTRAL")
            event = sensevoice_result.get("event", "Speech")
            has_bgm = sensevoice_result.get("has_bgm", False)

            # ğŸ¯ YAMNetå®˜æ–¹æ•°æ®
            yamnet_class = yamnet_result.get("top_class", "Unknown")
            yamnet_confidence = yamnet_result.get("confidence", 0.0)
            environment_detection = yamnet_result.get("environment_detection", {})

            # ğŸ¯ Librosaå®˜æ–¹æ•°æ®ï¼ˆå¯é€‰ï¼‰
            tempo_analysis = {}
            audio_properties = {}
            if librosa_result:
                tempo_analysis = librosa_result.get("tempo_analysis", {})
                audio_properties = librosa_result.get("audio_properties", {})

            # ğŸ¯ åŸºäºå¤šæ¨¡æ€ç»“æœç¡®å®šéŸ³é¢‘ç±»å‹
            audio_type = "speech"
            confidence = yamnet_confidence

            # ğŸ”¥ å¢å¼ºçš„åˆ¤æ–­é€»è¾‘ - åŸºäºå®˜æ–¹æ ‡å‡†æ•°æ®
            if has_bgm or "Music" in yamnet_class or event == "BGM" or environment_detection.get("music_detected", False):
                if len(text) < 5:
                    # BGMæˆ–æ£€æµ‹åˆ°éŸ³ä¹ï¼Œä¸”æ–‡æœ¬å¾ˆå°‘ -> çº¯éŸ³ä¹
                    confidence = max(0.9 if has_bgm else 0.0, yamnet_confidence)
                    audio_type = "music"
                else:
                    # æœ‰éŸ³ä¹èƒŒæ™¯ä½†æœ‰æ–‡æœ¬ -> å¸¦èƒŒæ™¯éŸ³ä¹çš„è¯­éŸ³
                    audio_type = "music"
                    confidence = 0.7
            elif "Speech" in yamnet_class or len(text) > 5 or environment_detection.get("crowd_detected", False):
                # æ£€æµ‹åˆ°è¯­éŸ³æˆ–æœ‰æ–‡æœ¬æˆ–äººç¾¤ -> è¯­éŸ³
                confidence = max(0.8 if len(text) > 5 else 0.0, yamnet_confidence)
                audio_type = "speech"
            elif environment_detection.get("fan_detected", False) or environment_detection.get("rain_detected", False) or environment_detection.get("noise_detected", False):
                # æ£€æµ‹åˆ°ç¯å¢ƒå™ªéŸ³ -> å™ªéŸ³
                audio_type = "noise"
                confidence = max(0.6, yamnet_confidence * 0.8)
            else:
                # å…¶ä»–æƒ…å†µ -> å™ªéŸ³
                audio_type = "noise"
                confidence = max(0.6, yamnet_confidence * 0.5)

            # ğŸ”¥ ä¿å­˜å®Œæ•´çš„å®˜æ–¹æ ‡å‡†æ ¼å¼ç»“æœä¾›åç»­ä½¿ç”¨
            self._last_complete_analysis = {
                "audio_type": audio_type,
                "confidence": confidence,
                "timestamp": datetime.now().isoformat(),
                "sensevoice_result": sensevoice_result,
                "yamnet_result": yamnet_result,
                "librosa_result": librosa_result or {},
                "combined_analysis": {
                    "primary_type": audio_type,
                    "confidence": confidence,
                    "detected_language": language,
                    "detected_emotion": emotion,
                    "detected_event": event,
                    "environment_summary": environment_detection,
                    "audio_duration": audio_properties.get("duration", 0),
                    "tempo_bpm": tempo_analysis.get("tempo", 0)
                }
            }

            return audio_type, confidence

        except Exception as e:
            self.logger.error(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")
            return "noise", 0.5

    def _check_music_recognition_trigger(self):
        """æ£€æŸ¥æ˜¯å¦è§¦å‘éŸ³ä¹è¯†åˆ«"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éŸ³ä¹ç‰‡æ®µ
            if len(self.music_segments) < self.music_segments_needed:
                return
            
            # æ£€æŸ¥å†·å´æ—¶é—´
            if datetime.now() - self.last_recognition_time < self.recognition_cooldown:
                return
            
            # ğŸ¯ è§¦å‘éŸ³ä¹è¯†åˆ«
            self._trigger_music_recognition()
            
        except Exception as e:
            self.logger.error(f"âŒ éŸ³ä¹è¯†åˆ«è§¦å‘æ£€æŸ¥å¤±è´¥: {str(e)}")
    
    def _trigger_music_recognition(self):
        """è§¦å‘éŸ³ä¹è¯†åˆ«"""
        try:
            # é€‰æ‹©æœ€å¥½çš„2ä¸ªéŸ³ä¹ç‰‡æ®µ
            best_segments = sorted(self.music_segments, key=lambda x: x.confidence, reverse=True)[:2]
            
            self.logger.info(f"ğŸµ è§¦å‘éŸ³ä¹è¯†åˆ«ï¼Œå‘é€ {len(best_segments)} ä¸ªéŸ³ä¹ç‰‡æ®µ")
            
            # ğŸ¯ è¿™é‡Œåº”è¯¥è°ƒç”¨ACRCloudè¯†åˆ«
            # ç›®å‰å…ˆè®°å½•æ—¥å¿—
            for segment in best_segments:
                self.logger.info(f"ğŸ“¤ å‘é€éŸ³ä¹ç‰‡æ®µè¯†åˆ«: {segment.file_path}")
            
            # æ›´æ–°æœ€åè¯†åˆ«æ—¶é—´
            self.last_recognition_time = datetime.now()
            
            # æ¸…ç†å·²è¯†åˆ«çš„ç‰‡æ®µ
            for segment in best_segments:
                if segment in self.music_segments:
                    self.music_segments.remove(segment)
            
        except Exception as e:
            self.logger.error(f"âŒ éŸ³ä¹è¯†åˆ«è§¦å‘å¤±è´¥: {str(e)}")
    
    def _cleanup_segment(self, segment: AudioSegment):
        """æ¸…ç†éŸ³é¢‘ç‰‡æ®µæ–‡ä»¶"""
        try:
            if os.path.exists(segment.file_path):
                os.remove(segment.file_path)
                self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†éŸ³é¢‘ç‰‡æ®µ: {segment.file_path}")
        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†éŸ³é¢‘ç‰‡æ®µå¤±è´¥: {str(e)}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ”¶é›†å™¨çŠ¶æ€"""
        return {
            "running": self.running,
            "total_segments": len(self.audio_segments),
            "music_segments": len(self.music_segments),
            "last_recognition": self.last_recognition_time.isoformat(),
            "segments_needed": self.music_segments_needed,
            "current_music_count": len([s for s in self.music_segments if s.confidence > self.music_threshold])
        }

    def _send_to_dynamic_prompt_system(self, segment: AudioSegment):
        """å‘é€åˆ†æç»“æœç»™åŠ¨æ€æç¤ºè¯ç³»ç»Ÿ"""
        try:
            # ğŸ¯ å…ˆæ„å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡ä¿¡æ¯
            audio_context = {
                "audio_type": segment.audio_type,
                "confidence": segment.confidence,
                "timestamp": segment.timestamp.isoformat(),
                "features": segment.features,
                "file_path": segment.file_path
            }

            # ğŸ¯ ç´¯ç§¯éŸ³é¢‘ä¸Šä¸‹æ–‡ï¼Œä¸ç«‹å³å¤„ç†
            if not hasattr(self, 'accumulated_contexts'):
                self.accumulated_contexts = []

            self.accumulated_contexts.append(audio_context)
            self.logger.info(f"âœ… [åå°æ”¶é›†] éŸ³é¢‘ä¸Šä¸‹æ–‡å·²ç´¯ç§¯: {len(self.accumulated_contexts)}/{self.analysis_batch_size}")

            # ğŸ”§ ä¿å­˜JSONæ•°æ®åˆ°ç»Ÿä¸€ç›®å½•ï¼Œæ–¹ä¾¿æŸ¥çœ‹
            import json
            import os
            json_dir = "E:/liusisi/SmartSisi/sisi_brain/audio_data_cache"
            os.makedirs(json_dir, exist_ok=True)

            # ä¿å­˜å½“å‰éŸ³é¢‘ä¸Šä¸‹æ–‡
            context_file = f"{json_dir}/audio_context_{int(time.time())}.json"
            with open(context_file, 'w', encoding='utf-8') as f:
                json.dump(audio_context, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ğŸ’¾ [åå°æ”¶é›†] éŸ³é¢‘ä¸Šä¸‹æ–‡å·²ä¿å­˜: {context_file}")

            # ğŸ”§ ä¿®å¤ï¼šç°åœ¨æ£€æŸ¥ç´¯ç§¯æ•°é‡ï¼ˆåœ¨æ·»åŠ æ•°æ®ä¹‹åï¼‰
            self.logger.info(f"ğŸ“¤ éŸ³é¢‘ä¸Šä¸‹æ–‡å·²ç´¯ç§¯({len(self.accumulated_contexts)}ä¸ª)ï¼Œå‡†å¤‡å‘é€ç»™å‰è„‘ç³»ç»Ÿ")

            # ğŸ¯ ç´¯ç§¯åˆ°3æ¬¡åç›´æ¥å‘é€ç»™åŠ¨æ€æç¤ºè¯ä¸­æ¢ï¼ˆè·³è¿‡ä¸­é—´å¤§æ¨¡å‹åˆ†æï¼‰
            if len(self.accumulated_contexts) >= self.analysis_batch_size:
                self.logger.info(f"âœ… [åå°æ”¶é›†] è¾¾åˆ°ç´¯ç§¯é˜ˆå€¼({self.analysis_batch_size})ï¼Œç›´æ¥å‘é€ç»™åŠ¨æ€ä¸­æ¢")

                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç´¯ç§¯ç®¡ç†å™¨æ–¹æ³•
                try:
                    from sisi_brain.audio_accumulation_manager import get_audio_accumulation_manager
                    accumulation_manager = get_audio_accumulation_manager()

                    self.logger.info("âœ… [åå°æ”¶é›†] ç´¯ç§¯ç®¡ç†å™¨è·å–æˆåŠŸ")

                    # ğŸ¯ æ‰¹é‡å‘é€ç´¯ç§¯çš„éŸ³é¢‘ä¸Šä¸‹æ–‡
                    for i, context in enumerate(self.accumulated_contexts):
                        accumulation_manager.add_audio_context(context)
                        self.logger.info(f"âœ… [åå°æ”¶é›†] ä¸Šä¸‹æ–‡{i+1}å·²å‘é€ç»™ç´¯ç§¯ç®¡ç†å™¨")

                    self.logger.info(f"âœ… [åå°æ”¶é›†] å·²å‘é€{len(self.accumulated_contexts)}ä¸ªéŸ³é¢‘ä¸Šä¸‹æ–‡ç»™ç´¯ç§¯ç®¡ç†å™¨")

                except ImportError:
                    self.logger.warning("âŒ [åå°æ”¶é›†] åŠ¨æ€ä¸­æ¢æœªæ‰¾åˆ°ï¼Œè·³è¿‡åŠ¨æ€æç¤ºè¯å¤„ç†")
                except Exception as e:
                    self.logger.error(f"âŒ [åå°æ”¶é›†] å‘é€åŠ¨æ€ä¸­æ¢å¼‚å¸¸: {e}")

                self.accumulated_contexts = []  # æ¸…ç©ºç´¯ç§¯
                self.logger.info("âœ… [åå°æ”¶é›†] ç´¯ç§¯ç¼“å­˜å·²æ¸…ç©º")

        except Exception as e:
            self.logger.error(f"âŒ å‘é€åŠ¨æ€æç¤ºè¯å¤±è´¥: {e}")



    def _send_to_music_recognition(self, segment: AudioSegment):
        """å‘é€éŸ³ä¹ç‰‡æ®µç»™ACRCloudè¯†åˆ«"""
        try:
            # ğŸ¯ é›†æˆæ‚¨çš„ACRCloudéŸ³ä¹è¯†åˆ«
            from sisi_brain.acrcloud_music_analyzer import get_music_analyzer
            import asyncio

            analyzer = get_music_analyzer()

            if analyzer.enabled:
                self.logger.info(f"âœ… [åå°æ”¶é›†] å¼€å§‹ACRCloudéŸ³ä¹è¯†åˆ«: {segment.file_path}")

                # ğŸ”¥ çœŸæ­£è°ƒç”¨ACRCloud API
                def run_recognition():
                    try:
                        self.logger.info("âœ… [åå°æ”¶é›†] ACRCloud APIè°ƒç”¨å¼€å§‹")
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        result = loop.run_until_complete(analyzer.identify_music(segment.file_path))

                        if result:
                            self.logger.info(f"âœ… [åå°æ”¶é›†] ACRCloudè¯†åˆ«æˆåŠŸ: {result.song_name} - {result.artist}")
                            # ä¿å­˜è¯†åˆ«ç»“æœåˆ°segment
                            segment.features['music_info'] = {
                                'title': result.song_name,  # ä½¿ç”¨song_nameè€Œä¸æ˜¯title
                                'artist': result.artist,
                                'album': result.album,
                                'confidence': result.confidence
                            }
                        else:
                            self.logger.info("âŒ [åå°æ”¶é›†] ACRCloudæœªè¯†åˆ«åˆ°éŸ³ä¹")

                    except Exception as e:
                        self.logger.error(f"âŒ [åå°æ”¶é›†] ACRCloudè¯†åˆ«å¼‚å¸¸: {e}")
                    finally:
                        loop.close()

                # å¼‚æ­¥æ‰§è¡Œè¯†åˆ«ï¼Œä¸é˜»å¡ä¸»æµç¨‹
                import threading
                threading.Thread(target=run_recognition, daemon=True).start()
            else:
                self.logger.info("âŒ [åå°æ”¶é›†] ACRCloudåˆ†æå™¨æœªå¯ç”¨")

        except Exception as e:
            self.logger.error(f"âŒ å‘é€éŸ³ä¹è¯†åˆ«å¤±è´¥: {e}")

        # å¦‚æœACRCloudå®Œå…¨ç¦ç”¨
        if not hasattr(analyzer, 'enabled') or not analyzer.enabled:
            self.logger.warning("âš ï¸ ACRCloudéŸ³ä¹è¯†åˆ«å·²ç¦ç”¨")

    def _send_to_voice_recognition(self, segment: AudioSegment):
        """å‘é€è¯­éŸ³ç‰‡æ®µç»™å£°çº¹è¯†åˆ« - ä»…åšéŸ³é¢‘æ”¶é›†ï¼Œä¸è°ƒç”¨å¤§æ¨¡å‹"""
        try:
            # ğŸ¯ ä¿®å¤ï¼šä»…åšéŸ³é¢‘æ”¶é›†ï¼Œä¸è°ƒç”¨audio_context_processorï¼ˆé¿å…å¤§æ¨¡å‹APIè°ƒç”¨ï¼‰
            # éŸ³é¢‘æ”¶é›†å™¨çš„èŒè´£ï¼šæ”¶é›†éŸ³é¢‘æ•°æ®ï¼Œä¾›åå°å‰è„‘ç³»ç»Ÿä½¿ç”¨

            # ğŸ¯ æ§åˆ¶å£°çº¹è¯†åˆ«æ—¥å¿—é¢‘ç‡
            if self.log_counter == 0:
                self.logger.info(f"ğŸ—£ï¸ éŸ³é¢‘ç‰‡æ®µå·²æ”¶é›†: {segment.file_path} (ä»…æ”¶é›†ï¼Œä¸åˆ†æ)")

        except Exception as e:
            self.logger.error(f"âŒ éŸ³é¢‘æ”¶é›†å¤±è´¥: {e}")

# å…¨å±€å®ä¾‹
_audio_collector = None

def get_audio_collector() -> SmartAudioCollector:
    """è·å–éŸ³é¢‘æ”¶é›†å™¨å®ä¾‹"""
    global _audio_collector
    if _audio_collector is None:
        _audio_collector = SmartAudioCollector()
    return _audio_collector

def get_smart_audio_collector() -> SmartAudioCollector:
    """è·å–æ™ºèƒ½éŸ³é¢‘æ”¶é›†å™¨å®ä¾‹ - åˆ«åå‡½æ•°"""
    return get_audio_collector()

def start_smart_audio_collection():
    """å¯åŠ¨æ™ºèƒ½éŸ³é¢‘æ”¶é›†"""
    collector = get_audio_collector()
    collector.start_collection()
    return collector

def stop_smart_audio_collection():
    """åœæ­¢æ™ºèƒ½éŸ³é¢‘æ”¶é›†"""
    collector = get_audio_collector()
    collector.stop_collection()

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    collector = start_smart_audio_collection()
    
    try:
        print("ğŸµ æ™ºèƒ½éŸ³é¢‘æ”¶é›†ç³»ç»Ÿæµ‹è¯•å¯åŠ¨...")
        print("æŒ‰ Ctrl+C åœæ­¢æµ‹è¯•")
        
        while True:
            status = collector.get_status()
            print(f"çŠ¶æ€: {status}")
            time.sleep(10)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ åœæ­¢æµ‹è¯•")
        stop_smart_audio_collection()
