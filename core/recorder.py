#æµ£æ»…æ•¤é„ç…¶æ£°æˆç¶é’è®¹ç´ç€µé€›ç°¬aliyun asré‰ãƒ¨é”›å²ƒç«Ÿè¤°æ›åŸ—æˆç®‚tté”›å±¼çµ¾ç€µé€›ç°¬éæœµç²¬é‰ãƒ¨é”›å±¾æ§¸éå œç¹šç€›æ¨»åšé‚å›¦æ¬¢éå¶†å¸¹é–«ä½ºç²°asrå¦¯â€³ç€·é”›å²„â‚¬æ°³ç¹ƒç€¹ç‚µå¹‡ç€›æ„®è¢«é¨å‹¬æŸŸå¯®å¿¥ç´™sisi_booter.py æ¶“å©ƒæ¹ç€¹ç‚µå¹‡é”›å¤‹æ½µç» ï¼„æ‚Šé—ŠæŠ½å¨´ä½ºæ®‘é‰ãƒ¦ç°®
import audioop
import math
import time
import threading
import os
from abc import abstractmethod
from collections import deque
from queue import Queue

from asr.ali_nls import ALiNls
from asr.funasr import FunASR
from core import wsa_server
from scheduler.thread_manager import MyThread
from utils import util
from utils import config_util as cfg
import numpy as np
import wave
from core import sisi_core
from core import shared_state
from core import interact

# æ¥¹ï¹€å æ¤‹åº¡æƒé”ã„¦æ¤‚é—‚?(ç»‰?
_ATTACK = 0.1

# æ¥¹ï¹€å æ¤‹åº¨å™´é€ç‚¬æ¤‚é—‚?(ç»‰?
_RELEASE = 1.0

def _is_enabled_flag(value):
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)):
        return value != 0
    return str(value or "").strip().lower() in ("1", "true", "yes", "on")


class Recorder:

    def __init__(self, sisi):
        self.__sisi = sisi
        self.__running = True
        self.__processing = False
        self.__history_level = []
        self.__history_data = []
        self.__dynamic_threshold = 0.5 # æ¾¹ä¼´ç…¶ç’‡å——åŸ†é¨å‹¯ç…¶é–²å¿›æ§‡éŠ?
        # é—ƒå‰å§ˆæ¶“åº£Ç”ç€¹æ°­â‚¬Ñƒå¯®?
        self.__voiced_run = 0  # æ©ç‚µç”»éˆå¤Šï¼ç”¯Ñ†é?
        self.__silent_run = 0  # æ©ç‚µç”»é—ˆæ¬“ç…¶ç”¯Ñ†é?
        self.__threshold_min = 0.15  # é”ã„¦â‚¬ä¾€æ§‡éŠé—´ç¬…é£?
        self.__threshold_max = 0.75  # é”ã„¦â‚¬ä¾€æ§‡éŠé—´ç¬‚é£?

        self.__MAX_LEVEL = 25000
        self.__MAX_BLOCK = 100
        
        # éˆæ¹´ ASR å¦¯â€³ç´¡é–°å¶‡ç–†
        self.ASRMode = cfg.ASR_mode
        self.__aLiNls = None
        self.is_awake = False
        self.wakeup_matched = False
        # éã‚‰å•‹ç»æ¥€å½›éºÑƒåŸ—é”›å Ÿæ¤‚é—‚ç¿ ç¬Œæé”›?
        self.wake_window_seconds = int(cfg.config['source'].get('wake_window_seconds', 60))
        self.wake_window_turns = int(cfg.config['source'].get('wake_window_turns', 3))
        self.wake_remaining_turns = 0
        self.sleep_phrases = set(cfg.config['source'].get('sleep_phrases', []))
        self.wake_front_window_chars = int(cfg.config['source'].get('wake_front_window_chars', 6))
        if cfg.config['source']['wake_word_enabled']:
            self.timer = threading.Timer(self.wake_window_seconds, self._on_wake_timer_timeout)  # é¸å¤å¤ç¼ƒéƒ?
        self.username = 'User' #æ¦›æ¨¿é¢ã„¦åŸ›é”›å±½ç“™ç»«è¯²ç–„éœç‰ˆæ¤‚æµ¼æ°¶å™¸é?
        self.channels = 1
        self.sample_rate = 16000
        self.is_reading = False
        self.stream = None

        self.__last_ws_notify_time = 0
        self.__ws_notify_interval = 0.5  # éˆâ‚¬çå¿›â‚¬æ°±ç…¡é—‚æ’®æ®§é”›å ¢é”›?
        self.__ws_notify_thread = None
        # æ£°å‹«æ•œé–±æ”ç´¦éè¯§ç´°éˆâ‚¬æ¾¶æ°³æµ£?é™?
        self._prewake_buffer = deque(maxlen=2)

        # AEC / ?????
        self._aec = None
        self._aec_enabled = False
        self._aec_required = False
        self._aec_frame_ms = 16
        self._aec_filter_length_ms = 200
        self._half_duplex_enabled = False
        self._half_duplex_hold_ms = 120
        self._half_duplex_tail_ms = 120
        # AEC diagnostics (throttled) for remote capture troubleshooting
        self._aec_diag_last_ts = 0.0
        self._aec_diag_interval_s = 2.0
        self._aec_diag_frames = 0
        self._aec_diag_ref_silent_frames = 0
        self._aec_diag_half_duplex_drops = 0
        # Server-side wake session authority.
        self._wake_session_epoch = 0
        self._wake_session_id = ""
        self._wake_session_open_ts = 0.0
        self._wake_session_last_ts = 0.0
        self._wake_stale_asr_drops = 0
        self._asr_result_seq = 0

    def asrclient(self):
        if self.ASRMode == "ali":
            asrcli = ALiNls(self.username)
        elif self.ASRMode == "funasr" or self.ASRMode == "sensevoice":
            asrcli = FunASR(self.username)
        return asrcli
    def _make_timestamped_input_path(self, cache_root: str) -> str:
        from datetime import datetime
        ts = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        return os.path.join(cache_root, f"input_{ts}.wav")

    def save_buffer_to_file(self, buffer):
        cache_root = cfg.cache_root or "cache_data"
        os.makedirs(cache_root, exist_ok=True)
        file_path = self._make_timestamped_input_path(cache_root)
        with wave.open(file_path, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(buffer)

        # è®°å½•æœ€è¿‘ä¸€æ¬¡ä¿å­˜çš„éŸ³é¢‘æ–‡ä»¶ï¼Œä¾›åç»­æµç¨‹å¤ç”¨
        self.latest_audio_file = file_path

        return file_path

    def __get_history_average(self, number):
        total = 0
        num = 0
        for i in range(len(self.__history_level) - 1, -1, -1):
            level = self.__history_level[i]
            total += level
            num += 1
            if num >= number:
                break
        return total / num

    def __get_history_percentage(self, number):
        return (self.__get_history_average(number) / self.__MAX_LEVEL) * 1.05 + 0.02

    def _build_wake_session_id(self):
        return f"{self.username}-{self._wake_session_epoch}-{int(time.time() * 1000)}"

    def _restart_wake_timer(self):
        if not cfg.config['source']['wake_word_enabled']:
            return
        try:
            self.timer.cancel()
        except Exception:
            pass
        self.timer = threading.Timer(self.wake_window_seconds, self._on_wake_timer_timeout)
        self.timer.start()

    def _open_wake_session(self, reason):
        now_ts = time.time()
        self._wake_session_epoch += 1
        self._wake_session_id = self._build_wake_session_id()
        self._wake_session_open_ts = now_ts
        self._wake_session_last_ts = now_ts
        self.wakeup_matched = True
        self.wake_remaining_turns = self.wake_window_turns
        with shared_state.auto_play_lock:
            shared_state.can_auto_play = False
        self._restart_wake_timer()
        util.log(
            1,
            "[wake_session] action=open id={} epoch={} reason={} turns={}".format(
                self._wake_session_id,
                int(self._wake_session_epoch),
                reason,
                int(self.wake_remaining_turns),
            ),
        )

    def _touch_wake_session(self):
        if not self.wakeup_matched:
            return
        self._wake_session_last_ts = time.time()

    def _close_wake_session(self, reason, clear_prewake=False):
        had_session = bool(self.wakeup_matched or self._wake_session_id)
        session_id = self._wake_session_id or "none"
        session_epoch = int(self._wake_session_epoch)
        self.wakeup_matched = False
        self.wake_remaining_turns = 0
        if clear_prewake and hasattr(self, "_prewake_buffer"):
            self._prewake_buffer.clear()
        with shared_state.auto_play_lock:
            shared_state.can_auto_play = True
        try:
            self.timer.cancel()
        except Exception:
            pass
        if had_session:
            util.log(
                1,
                "[wake_session] action=close id={} epoch={} reason={}".format(
                    session_id,
                    session_epoch,
                    reason,
                ),
            )

    def _on_wake_timer_timeout(self):
        self._close_wake_session("timer_expired")

    def reset_wakeup_status(self):
        self._close_wake_session("manual_reset")

    def apply_external_wake_hit(self, source="device_kws", keyword="", confidence=""):
        if not cfg.config['source']['wake_word_enabled']:
            util.log(
                1,
                "[wake_session] action=ignore_external_wake reason=wake_disabled source={}".format(
                    source or "unknown",
                ),
            )
            return False
        reason = "external:{}:{}".format(source or "unknown", keyword or "none")
        if self.wakeup_matched:
            self._touch_wake_session()
            self._restart_wake_timer()
            util.log(
                1,
                "[wake_session] action=touch_external_wake id={} epoch={} source={} keyword={} confidence={}".format(
                    self._wake_session_id or "none",
                    int(self._wake_session_epoch),
                    source or "unknown",
                    keyword or "",
                    confidence or "",
                ),
            )
            return True
        self._open_wake_session(reason)
        util.log(
            1,
            "[wake_session] action=apply_external_wake id={} epoch={} source={} keyword={} confidence={}".format(
                self._wake_session_id or "none",
                int(self._wake_session_epoch),
                source or "unknown",
                keyword or "",
                confidence or "",
            ),
        )
        return True

    def _normalize_text(self, text):
        """Normalize recognized text for wake matching."""
        if not text:
            return ""
        t = str(text).strip()
        # é˜å©šæ«ç»Œè™¹æ«§éŠ†ä½¸åç‘™æ”â”–é?
        t = t.replace('\u3000', ' ')
        t = ' '.join(t.split())
        # Replace common punctuation with spaces for stable wake-word matching.
        for ch in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', ',', '.', '!', '?', 'ï¼š', ':', 'ï¼›', ';', 'ã€', 'ï¼ˆ', 'ï¼‰', '(', ')']:
            t = t.replace(ch, ' ')
        return t

    def _contains_sleep_phrase(self, text):
        if not text or not self.sleep_phrases:
            return False
        t = self._normalize_text(text)
        return any(p in t for p in self.sleep_phrases)


    def _load_audio_controls(self):
        try:
            cfg.load_config()
            self._aec_enabled = str(getattr(cfg, 'aec_enabled', False)).lower() == 'true'
            self._aec_required = str(getattr(cfg, 'aec_required', False)).lower() == 'true'
            self._aec_frame_ms = int(getattr(cfg, 'aec_frame_ms', 16))
            self._aec_filter_length_ms = int(getattr(cfg, 'aec_filter_length_ms', 200))
            self._half_duplex_enabled = str(getattr(cfg, 'half_duplex_enabled', False)).lower() == 'true'
            self._half_duplex_hold_ms = int(getattr(cfg, 'half_duplex_hold_ms', 120))
            self._half_duplex_tail_ms = int(getattr(cfg, 'half_duplex_tail_ms', 120))
        except Exception:
            pass

    def _ensure_aec(self):
        if not self._aec_enabled:
            return
        if self._aec is not None:
            return
        from utils.aec import get_aec_processor
        self._aec = get_aec_processor(sample_rate=self.sample_rate)

    def _should_bypass_server_aec(self):
        """Remote device capture always uses Android-side AEC as the single owner."""
        return bool(self.is_remote())

    def _mix_to_mono(self, data):
        if self.channels == 1:
            return data
        try:
            return audioop.tomono(data, 2, 0.5, 0.5)
        except Exception:
            return data

    def _bytes_to_int16(self, data):
        return np.frombuffer(data, dtype=np.int16)
    def _should_process_input(self, text):
        """Filter trivial/noise inputs before processing."""
        try:
            if not text or len(text.strip()) == 0:
                util.log(1, "[interrupt] empty input, ignore")
                return False

            noise_patterns = ["?", "?", "?", "?", "?", "?", "?", "?", "?"]
            if text.strip() in noise_patterns:
                util.log(1, f"[interrupt] noise input ignored: {text}")
                return False

            if len(text.strip()) <= 2 and not any(keyword in text for keyword in ["?", "?", "?", "?", "?"]):
                util.log(1, f"[interrupt] too short, ignored: {text}")
                return False

            util.log(1, f"[interrupt] accept input: {text}")
            return True

        except Exception as e:
            util.log(2, f"[interrupt] check failed: {str(e)}")
            return True

    def _immediate_response(self, response_text):
        """ç»”å¬ªåµ†é¥ç‚²æ£°å‹®é­"""
        try:
            util.log(1, f"[é…é¸¿å…˜éµæ’´æŸ‡] ç»”å¬ªåµ†é¥ç‚²: {response_text}")
            # æ¥‚æ¨¹ç´­éå ¢éª‡TTSæˆæ’³åš­
            self.__sisi.say(response_text, 7)  # éˆâ‚¬æ¥‚æ¨¹ç´­éå ¢éª‡
        except Exception as e:
            util.log(2, f"[é…é¸¿å…˜éµæ’´æŸ‡] ç»”å¬ªåµ†é¥ç‚²å¯®å‚šçˆ¶: {str(e)}")

    def _stop_current_tasks(self):
        """é‹æ»„è¤°æ’³å¢ æµ è¯²å§Ÿ"""
        try:
            util.log(1, f"[é…é¸¿å…˜éµæ’´æŸ‡] é‹æ»„è¤°æ’³å¢ æµ è¯²å§Ÿ")

            # é‹æ»„è¤°æ’³å¢ ç’‡ç£‹ç˜½
            if hasattr(self.__sisi, 'speaking'):
                self.__sisi.speaking = False

            # é‹æ»„è¤°æ’³å¢ æ¾¶å‹­æ‚Š
            if hasattr(self.__sisi, 'chatting'):
                self.__sisi.chatting = False

            # é–«å¤‹å«¨é¬Ñ„ç«»ç»Œæ´ªç…¶æ£°æˆ¦æ§¦é’æ¥‹ç´æ·‡æ¿‡å§¢é—Šå……ç®°é‚å›¦æ¬¢
            if hasattr(self.__sisi, 'sound_query'):
                # æ·‡æ¿†ç“¨é—Šå……ç®°é‚å›¦æ¬¢é”›å±½å½§å¨“å‘¯â”–TTSé—ŠæŠ½
                music_items = []
                while not self.__sisi.sound_query.empty():
                    try:
                        item = self.__sisi.sound_query.get_nowait()
                        # å¦«â‚¬éŒãƒ¦æ§¸éšï¸¿è´Ÿé—Šå……ç®°é‚å›¦æ¬¢é”›å Ÿç‰´é¹æŸŠé¨å‹¯æ§¦é’æ¥ç‰¸å¯®å¿¥ç´š
                        if len(item) >= 2 and ('music_' in str(item[1]) or 'random_generation_music' in str(item[1])):
                            music_items.append(item)  # æ·‡æ¿ˆæš€é—Šå……ç®°é‚å›¦æ¬¢
                            util.log(1, f"[é…é¸¿å…˜éµæ’´æŸ‡] æ·‡æ¿‡å§¢é—Šå……ç®°é‚å›¦æ¬¢: {item[1]}")
                    except:
                        break

                # çå—›ç…¶æ¶”æ„­æƒæµ å •å™¸é‚ç‰ˆæ–é¥ç‚ºæ§¦é’?
                for item in music_items:
                    self.__sisi.sound_query.put(item)

                if music_items:
                    util.log(1, f"[interrupt] preserved_music_items={len(music_items)}")

        except Exception as e:
            util.log(2, f"[é…é¸¿å…˜éµæ’´æŸ‡] é‹æ»„æµ è¯²å§Ÿå¯®å‚šçˆ¶: {str(e)}")

    def _pause_current_tasks(self):
        """Pause current tasks while keeping backend tasks alive."""
        try:
            util.log(1, f"[é…é¸¿å…˜éµæ’´æŸ‡] é†å‚šä» è¤°æ’³å¢ æµ è¯²å§Ÿ")

            # é¦ƒæ•Ÿ æµ£è·¨æ•¤ç¼ç†¶ç«´éºÑƒåŸ—é£ã„§æ®‘é†å‚šä» é”ç†»å…˜
            from core.unified_system_controller import get_unified_controller
            unified_controller = get_unified_controller()

            # é†å‚šä» éµâ‚¬éˆå¤‹æ¤¿é”ã„¤çµ¾æ·‡æ¿‡å¯”APIç’‹å†ªæ•¤ç¼Ñ…ç”»
            unified_controller.pause_all_activities()

            util.log(1, f"[é…é¸¿å…˜éµæ’´æŸ‡] æµ è¯²å§Ÿå®¸å‰æ®é‹æ»ç´éšåº¡å½´ç¼Ñ…ç”»æ©æ„¯")

        except Exception as e:
            util.log(2, f"[é…é¸¿å…˜éµæ’´æŸ‡] é†å‚šä» æµ è¯²å§Ÿå¯®å‚šçˆ¶: {str(e)}")

    def __waitingResult(self, iat: asrclient, audio_data, capture_epoch=None, asr_seq=0):
        self.processing = True
        t = time.time()
        tm = time.time()
        if self.ASRMode == "funasr"  or self.ASRMode == "sensevoice":
            file_url = self.save_buffer_to_file(audio_data)
            self.__aLiNls.send_url(file_url)

        # é¦ƒæ•Ÿ é—ŠæŠ½é’å——å¼¶é”›æ°¬å£‡å¨´ä½ºâ–¼é™æˆ¦â‚¬ä½¸åŸŒé“å¶ˆå‰³ç»¯è¤ç²ºé”›å æ‚—é™æ¿ç´“å§ãƒ¯ç´æ¶“å¶‰æ¨†æ¿‰ç‚°å¯Œå¨´ä½ºâ–¼é”›?
        # é¦ƒå¹† æµ¼çŠ»â‚¬æ‘å‡¡æ·‡æ¿†ç“¨é¨å‹¬æƒæµ æƒ°çŸ¾å¯°å‹¶ç´é–¬å®å¤é–²å¶…æ·‡æ¿†ç“¨
        audio_file_path = getattr(self, 'latest_audio_file', None)
        self._send_to_background_brain(audio_data, audio_file_path)

        # return
        # ç»›å¤Šç·Ÿç¼æ’´ç‰æ©æ–¿æ´–é”›å Ÿå¯œé—ŠæŠ½éƒå •æš±é”ã„¦â‚¬ä½½ç¼ƒç§´éƒè®¹ç´é‘å¿“çš¯é—€å®å½çšå½ç»ŒçŒ´ç´š
        try:
            sr = 16000
            dur_s = len(audio_data) / sr if hasattr(audio_data, '__len__') else 0
        except Exception:
            dur_s = 0
        # éˆâ‚¬é­?sé”›å±¾æ¸¶é—€?0sé”›å²„â‚¬æ°¬çˆ¶æ¶“è¡¡â‚¬æ»ˆç…¶æ£°æˆæ¤‚é—€?1.5séˆ¥?
        timeout_sec = max(3.0, min(10.0, (dur_s or 0) + 1.5))
        while not iat.done and (time.time() - t) < timeout_sec:
            time.sleep(0.02)
        # éå¶‡ç²°ç¼ƒæˆ ç²¶/é¥ç‚¶çšŸæ¶“â‚¬éåœ­ç´¦é?
        if not iat.done and not getattr(iat, 'finalResults', ''):
            time.sleep(0.2)
        text = iat.finalResults
        util.printInfo(1, self.username, "è¯­éŸ³å¤„ç†å®Œæˆï¼è€—æ—¶: {} ms".format(math.floor((time.time() - tm) * 1000)))
        current_epoch = int(self._wake_session_epoch)
        if (
            capture_epoch is not None
            and int(capture_epoch) != current_epoch
            and self.wakeup_matched
        ):
            self._wake_stale_asr_drops += 1
            util.log(
                1,
                "[wake_session] action=drop_stale_asr seq={} capture_epoch={} current_epoch={} drops={}".format(
                    int(asr_seq),
                    int(capture_epoch),
                    int(current_epoch),
                    int(self._wake_stale_asr_drops),
                ),
            )
            self.processing = False
            return
        if len(text) > 0:
            if cfg.config['source']['wake_word_enabled']:
                #é…â‚¬æ°¬æ•œé–±æ“Äå¯®?
                if cfg.config['source']['wake_word_type'] == 'common':

                    if not self.wakeup_matched:
                        #éã‚‰å•‹ç’‡å¶…å½é‚?
                        wake_word =  cfg.config['source']['wake_word']
                        wake_word_list = [w.strip() for w in wake_word.split(',') if w.strip()]
                        wake_up = False
                        norm_text = self._normalize_text(text)
                        for word in wake_word_list:
                            if word and (word in norm_text):
                                    wake_up = True
                        if wake_up:
                            util.printInfo(1, self.username, "å”¤é†’æˆåŠŸï¼")
                            if wsa_server.get_web_instance() and wsa_server.get_web_instance().is_connected(self.username):
                                wsa_server.get_web_instance().add_cmd({"panelMsg": "å”¤é†’æˆåŠŸï¼", "agent_status": "listening", "Username" : self.username , 'robot': f'http://{cfg.sisi_url}:5000/robot/Listening.jpg'})
                            if wsa_server.get_instance() and wsa_server.get_instance().is_connected(self.username):
                                content = {'Topic': 'Unreal', 'Data': {'Key': 'log', 'Value': "å”¤é†’æˆåŠŸï¼"}, 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Listening.jpg'}
                                wsa_server.get_instance().add_cmd(content)
                            self._open_wake_session("common_wake_word")
                            #self.on_speaking(text)
                            intt = interact.Interact("auto_play", 2, {'user': self.username, 'text': "åœ¨å‘¢ï¼Œä½ è¯´å§ï¼Ÿ"})
                            self.__sisi.on_interact(intt)
                            self.processing = False
                            
                        else:
                            util.printInfo(1, self.username, "[!] ç­‰å¾…å”¤é†’ï¼")
                            # ç¼‚æ’³ç“¨éˆæ•œé–±æ•é™ãƒ¯ç´æ¸šæ¶—ç¬…å¨†â€³æ•œé–±æ—æå«¾éº?
                            try:
                                if text and len(text.strip()) > 0:
                                    self._prewake_buffer.append(text.strip())
                                    util.log(1, f"[é¢„å”¤é†’ç¼“å­˜] å·²ç¼“å­˜: {text.strip()}")
                            except Exception:
                                pass
                            if wsa_server.get_web_instance() and wsa_server.get_web_instance().is_connected(self.username):
                                wsa_server.get_web_instance().add_cmd({"panelMsg": "[!] ç­‰å¾…å”¤é†’ï¼", "agent_status": "wake_pending", "Username" : self.username , 'robot': f'http://{cfg.sisi_url}:5000/robot/Normal.jpg'})
                            if wsa_server.get_instance() and wsa_server.get_instance().is_connected(self.username):
                                content = {'Topic': 'Unreal', 'Data': {'Key': 'log', 'Value': "[!] ç­‰å¾…å”¤é†’ï¼"}, 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Normal.jpg'}
                                wsa_server.get_instance().add_cmd(content)
                    else:
                        # éã‚‰å•‹ç»æ¥€å½›éå‘¯æ®‘ç€µç¡…ç˜½æéºÑƒåŸ—
                        if self._contains_sleep_phrase(text):
                            util.printInfo(1, self.username, "æ£€æµ‹åˆ°ä¼‘çœ æŒ‡ä»¤ï¼Œç»“æŸå”¤é†’çª—å£")
                            self._close_wake_session("sleep_phrase", clear_prewake=True)
                            self.processing = False
                        else:
                            # æ££æ ¨æé”›æ°³å«¢é“å¶‰æ½°ç¼‚æ’³ç“¨æµœå—˜æ¹­éã‚‰å•‹ç’‡å½é”›å±¾å«¾éºãƒ¥æ‚—é™æˆ¦â‚¬?
                            if self.wake_remaining_turns == self.wake_window_turns and hasattr(self, '_prewake_buffer') and self._prewake_buffer:
                                prev = " ".join(list(self._prewake_buffer))
                                merged = f"{text} {prev}".strip()
                                self._prewake_buffer.clear()
                                self.on_speaking(merged)
                                self._touch_wake_session()
                            else:
                                self.on_speaking(text)
                                self._touch_wake_session()
                            if self.wake_remaining_turns > 0:
                                self.wake_remaining_turns -= 1
                            if self.wake_remaining_turns == 0:
                                self._close_wake_session("turns_exhausted")
                            else:
                                self._restart_wake_timer()
                        self.processing = False
                
                #é“å¶‡ç–†éã‚‰å•‹ç’‡å¶†Äå¯®?
                elif  cfg.config['source']['wake_word_type'] == 'front':
                    wake_word =  cfg.config['source']['wake_word']
                    wake_word_list = [w.strip() for w in wake_word.split(',') if w.strip()]
                    norm_text = self._normalize_text(text)

                    # å®¸æ’æ•œé–±æ”ç¥é™ï½ç´°éƒçŠ»æ¸¶éå¶†é’ã‚…æ•œé–±æ•ç˜
                    if self.wakeup_matched:
                        # æµ¼æˆ æ¹¢é­
                        if self._contains_sleep_phrase(text):
                            util.printInfo(1, self.username, "[front] æ£€æµ‹åˆ°ä¼‘çœ æŒ‡ä»¤ï¼Œç»“æŸå”¤é†’çª—å£")
                            self._close_wake_session("sleep_phrase", clear_prewake=True)
                            self.processing = False
                        else:
                            # æ££æ ¬ç–†éæ»ƒç°³éšå è‹Ÿ
                            if self.wake_remaining_turns == self.wake_window_turns and hasattr(self, '_prewake_buffer') and self._prewake_buffer:
                                prev = " ".join(list(self._prewake_buffer))
                                merged = f"{text} {prev}".strip()
                                util.log(1, f"[é¢„å”¤é†’æ‹¼æ¥] frontçª—å£å·²å”¤é†’ï¼Œé¦–è½®åˆå¹¶: {merged}")
                                self._prewake_buffer.clear()
                                self.on_speaking(merged)
                                self._touch_wake_session()
                            else:
                                self.on_speaking(text)
                                self._touch_wake_session()

                            # æ-1 æ¶“åº¤éƒè·ºæ«’ç¼æ¹¡
                            if self.wake_remaining_turns > 0:
                                self.wake_remaining_turns -= 1
                            if self.wake_remaining_turns == 0:
                                self._close_wake_session("turns_exhausted")
                            else:
                                self._restart_wake_timer()
                        self.processing = False
                    else:
                        # éˆæ•œé–±æç´°é“å¶‡ç¥é™ï½…å°®é–°å¶ˆĞ•é™?
                        wake_up = False
                        for word in wake_word_list:
                            if not word:
                                continue
                            pos = norm_text.find(word)
                            if pos == 0 or (0 <= pos < self.wake_front_window_chars):
                                util.log(1, f"[å”¤é†’åˆ¤å®š] frontçª—å£è§¦å‘: è¯='{word}', ä½ç½®={pos}, çª—å£={self.wake_front_window_chars}")
                                wake_up = True
                                break
                        if wake_up:
                            util.printInfo(1, self.username, "å”¤é†’æˆåŠŸï¼")
                            if wsa_server.get_web_instance() and wsa_server.get_web_instance().is_connected(self.username):
                                wsa_server.get_web_instance().add_cmd({"panelMsg": "å”¤é†’æˆåŠŸï¼", "agent_status": "listening", "Username" : self.username , 'robot': f'http://{cfg.sisi_url}:5000/robot/Listening.jpg'})
                            if wsa_server.get_instance() and wsa_server.get_instance().is_connected(self.username):
                                content = {'Topic': 'Unreal', 'Data': {'Key': 'log', 'Value': "å”¤é†’æˆåŠŸï¼"}, 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Listening.jpg'}
                                wsa_server.get_instance().add_cmd(content)

                            # æ¶“å¶†åŸ…é‚æ•œé–±æ•ç˜é”›å±¾æš£é™ãƒ¨ç¹˜é?
                            question = text
                            self._open_wake_session("front_wake_word")
                            from utils.stream_sentence import AudioPriorityQueue
                            self.__sisi.sound_query = AudioPriorityQueue()
                            time.sleep(0.3)
                            if hasattr(self, '_prewake_buffer') and self._prewake_buffer:
                                prev = " ".join(list(self._prewake_buffer))
                                question = f"{question} {prev}".strip()
                                util.log(1, f"[é¢„å”¤é†’æ‹¼æ¥] frontæ¨¡å¼åˆå¹¶: {question}")
                                self._prewake_buffer.clear()
                            self.on_speaking(question)
                            self._touch_wake_session()
                            self.processing = False
                        else:
                            util.printInfo(1, self.username, "[!] ç­‰å¾…å”¤é†’ï¼")
                            # ç¼‚æ’³ç“¨éˆæ•œé–±æ•é™?
                            try:
                                if text and len(text.strip()) > 0:
                                    self._prewake_buffer.append(text.strip())
                                    util.log(1, f"[é¢„å”¤é†’ç¼“å­˜] å·²ç¼“å­˜: {text.strip()}")
                            except Exception:
                                pass
                            if wsa_server.get_web_instance() and wsa_server.get_web_instance().is_connected(self.username):
                                wsa_server.get_web_instance().add_cmd({"panelMsg": "[!] ç­‰å¾…å”¤é†’ï¼", "agent_status": "wake_pending", "Username" : self.username , 'robot': f'http://{cfg.sisi_url}:5000/robot/Normal.jpg'})
                            if wsa_server.get_instance() and wsa_server.get_instance().is_connected(self.username):
                                content = {'Topic': 'Unreal', 'Data': {'Key': 'log', 'Value': "[!] ç­‰å¾…å”¤é†’ï¼"}, 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Normal.jpg'}
                                wsa_server.get_instance().add_cmd(content)

            #é—ˆç‚²æ•œé–±æ“Äå¯®?
            else:
                 # é¦ã„¦æ¹­éã‚‰å•‹é˜èˆµâ‚¬ä½·ç¬…é”›å±¼ç¹šç€›æ¨»æ¸¶æ©æˆ æ®‘1-2é™ãƒ§æ•¤é´ç–¯ç˜½éˆåŸŒæ£°å‹«æ•œé–±æ”ç´¦é?
                 try:
                     if text and len(text.strip()) > 0:
                         self._prewake_buffer.append(text.strip())
                 except Exception:
                     pass
                 self.on_speaking(text)
                 self.processing = False
        else:
            #TODO æ¶“è½°ç²ˆæ¶”å £ç¹–æ¶“æ¶“ç¯alse
            # if self.wakeup_matched:
            #     self.wakeup_matched = False
            self.processing = False
            util.printInfo(1, self.username, "[!] è¯­éŸ³æœªæ£€æµ‹åˆ°å†…å®¹ï¼")
            self.dynamic_threshold = self.__get_history_percentage(30)
            if wsa_server.get_web_instance() and wsa_server.get_web_instance().is_connected(self.username):
                wsa_server.get_web_instance().add_cmd({"panelMsg": "", "agent_status": "idle", 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Normal.jpg'})
            if wsa_server.get_instance() and wsa_server.get_instance().is_connected(self.username):
                content = {'Topic': 'Unreal', 'Data': {'Key': 'log', 'Value': ""}, 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Normal.jpg'}
                wsa_server.get_instance().add_cmd(content)

    def __record(self):   
        try:
            stream = self.get_stream() #é–«æ°³ç¹ƒå§ã‚†æŸŸå¨‰æ› æ®‘é—ƒè¯²é‰ãƒ¨ç»‹å¬ªç°­å¯°â‚¬æ¶“å¬«å¢½ç›?
            self._load_audio_controls()
            
            # å¨£è¯²å§ç€µç¡…ç¹™ç»‹å¬­æ¾¶å›©æ®‘é€å¯” - éä½½streamæ¶“ç¯˜one
            if stream is None and hasattr(self, 'is_remote') and self.is_remote():
                print(f"[æ©æ»…â–¼è¤°æ›¢ç…¶] {self.username} æµ£è·¨æ•¤æ©æ»…â–¼é—ŠæŠ½å©§æ„¶ç´ç’ºå® ç¹ƒéˆæ¹´æ¥¹ï¹€å æ¤‹åº¡åµæ¿®å¬ªå¯²")
                # æµ£è·¨æ•¤ç» â‚¬é–æ «æ®‘è¤°æ›¢ç…¶å¯°å¹†
                while self.__running:
                    time.sleep(0.1)  # é—„å¶„ç¶†CPUæµ£è·¨æ•¤éœ?                    # æ¾¶å‹­æ‚Šæ©æ»…â–¼ç’æƒ§é¨å‹­å£’å¨ˆå©‡â‚¬æ˜ç·«
                    continue
                return
            if stream is None:
                util.printInfo(1, self.username, "è¯·æ£€æŸ¥å½•éŸ³è®¾å¤‡æ˜¯å¦æœ‰è¯¯ï¼Œå†é‡æ–°å¯åŠ¨")
                return
            
        except Exception as e:
                print(e)
                util.printInfo(1, self.username, "è¯·æ£€æŸ¥å½•éŸ³è®¾å¤‡æ˜¯å¦æœ‰è¯¯ï¼Œå†é‡æ–°å¯åŠ¨")
                return
        
        isSpeaking = False
        last_mute_time = time.time() #é¢ã„¦åŸ›æ¶“å©ƒç’‡ç£‹ç˜½ç€¹å²ƒç˜½é¨å‹¬æ¤‚é’ä¼™ç´é¢ã„¤ç°¬VADé¨å‹«ç´‘æ¿®å¬ªå½é‚ç´™æ¶”ç†¶ç´°è¤°åæ·sisiç’‡æ‘ç•¬ç’‡æ¿†åŸŒé€è·ºæƒ‰é¢ã„¦åŸ›ç’‡ç£‹ç˜½é¨å‹¬æ¤‚é—‚æ’®æ£¿é—…æ—“ç´š 
        last_speaking_time = time.time()#é¢ã„¦åŸ›æ¶“å©ƒç’‡ç£‹ç˜½é¨å‹¬æ¤‚é’ä¼™ç´é¢ã„¤ç°¬VADé¨å‹­ç²¨é‰ç†·å½é‚?
        data = None
        concatenated_audio = bytearray()
        audio_data_list = []
        active_capture_epoch = int(self._wake_session_epoch)
        ref_key = "broadcast" if self.is_remote() else None
        while self.__running:
            try:
                cfg.load_config()
                source_cfg = cfg.config.get('source', {}) if isinstance(cfg.config, dict) else {}
                record_cfg = source_cfg.get('record', {}) if isinstance(source_cfg.get('record', {}), dict) else {}
                input_mode = str(source_cfg.get('input_mode', 'device_only') or 'device_only').strip().lower()
                local_capture_enabled = _is_enabled_flag(record_cfg.get('enabled', False)) and input_mode != 'device_only'

                if not local_capture_enabled and not self.is_remote():
                    time.sleep(0.2)
                    continue
                self.is_reading = True
                data = stream.read(1024, exception_on_overflow=False)
                self.is_reading = False
            except Exception as e:
                data = None
                print(e)
                util.log(1, "è¯·æ£€æŸ¥å½•éŸ³è®¾å¤‡æ˜¯å¦æœ‰è¯¯ï¼Œå†é‡æ–°å¯åŠ¨")
                self.__running = False
            if not data:
                continue 
            #é„æƒé™äº’é·é¹ƒç…¶,æ¶“å¶…å½²æµ ãƒ¥æ°¨éºå¤Šç´”è¤°æ›¢ç…¶
            # AEC / ???
            mono_data = self._mix_to_mono(data)
            mic_rms_pre = audioop.rms(mono_data, 2) if mono_data else 0
            ref_rms = 0
            aec_ready = False
            server_aec_applied = False
            bypass_server_aec = self._should_bypass_server_aec()
            if self._half_duplex_enabled:
                try:
                    from utils import audio_ref
                    if audio_ref.should_suppress_input(time.time(), self._half_duplex_hold_ms, self._half_duplex_tail_ms, key=ref_key):
                        self._aec_diag_half_duplex_drops += 1
                        continue
                except Exception:
                    pass
            if self._aec_enabled and not bypass_server_aec:
                try:
                    self._ensure_aec()
                    from utils import audio_ref
                    ref_data = audio_ref.pop_reference_pcm(len(mono_data), key=ref_key)
                    ref_rms = audioop.rms(ref_data, 2) if ref_data else 0
                    if self._aec:
                        try:
                            aec_ready = bool(self._aec.is_ready())
                        except Exception:
                            aec_ready = True
                        mono_data = self._aec.process(mono_data, ref_data)
                        server_aec_applied = True
                except Exception:
                    if self._aec_required:
                        util.log(3, '[é–¿æ¬’][AEC] required backend not available')
                        self.__running = False
                        return
                    pass
            elif bypass_server_aec:
                aec_ready = True
            mic_rms_post = audioop.rms(mono_data, 2) if mono_data else 0
            self._aec_diag_frames += 1
            if ref_rms <= 0:
                self._aec_diag_ref_silent_frames += 1
            now_diag = time.time()
            if now_diag - self._aec_diag_last_ts >= self._aec_diag_interval_s:
                util.log(
                    1,
                    "[AEC_DIAG] remote={} key={} aec_enabled={} aec_ready={} server_aec_applied={} server_aec_bypassed={} frames={} ref_silent={} half_duplex_drops={} mic_rms_pre={} mic_rms_post={} ref_rms={}".format(
                        bool(self.is_remote()),
                        ref_key or "default",
                        bool(self._aec_enabled),
                        bool(aec_ready),
                        int(bool(server_aec_applied)),
                        int(bool(bypass_server_aec)),
                        int(self._aec_diag_frames),
                        int(self._aec_diag_ref_silent_frames),
                        int(self._aec_diag_half_duplex_drops),
                        int(mic_rms_pre),
                        int(mic_rms_post),
                        int(ref_rms),
                    ),
                )
                self._aec_diag_last_ts = now_diag
                self._aec_diag_frames = 0
                self._aec_diag_ref_silent_frames = 0
                self._aec_diag_half_duplex_drops = 0
            level = audioop.rms(mono_data, 2)
            if len(self.__history_data) >= 10:#æ·‡æ¿†ç“¨å©µâ‚¬å¨²è¯²å¢ é¨å‹¯ç…¶æ£°æˆ¯ç´æµ ãƒ¥å¤æ·‡â„ƒä¼…éºå¤Šã‘
                self.__history_data.pop(0)
            if len(self.__history_level) >= 500:
                self.__history_level.pop(0)
            self.__history_data.append(mono_data)
            self.__history_level.append(level)
            percentage = level / self.__MAX_LEVEL
            history_percentage = self.__get_history_percentage(30)
            if history_percentage > self.__dynamic_threshold:
                self.__dynamic_threshold += (history_percentage - self.__dynamic_threshold) * 0.0025
            elif history_percentage < self.__dynamic_threshold:
                self.__dynamic_threshold += (history_percentage - self.__dynamic_threshold) * 1
            
           
            #é¢ã„¦åŸ›å§ï½…æ¹ªç’‡ç£‹ç˜½é”›å±¾ç¸ºå¨²ç»˜å¬€é—Š?
            try:
                if percentage > self.__dynamic_threshold:
                    last_speaking_time = time.time() 

                    if not self.__processing and not isSpeaking and time.time() - last_mute_time > _ATTACK:
                        isSpeaking = True  #é¢ã„¦åŸ›å§ï½…æ¹ªç’‡ç£‹ç˜½
                        active_capture_epoch = int(self._wake_session_epoch)
                        util.printInfo(1, self.username, "è†å¬ä¸­...")
                        if wsa_server.get_web_instance() and wsa_server.get_web_instance().is_connected(self.username):
                            wsa_server.get_web_instance().add_cmd({"panelMsg": "è†å¬ä¸­...", "agent_status": "listening", 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Listening.jpg'})
                        if wsa_server.get_instance() and wsa_server.get_instance().is_connected(self.username):
                            content = {'Topic': 'Unreal', 'Data': {'Key': 'log', 'Value': "è†å¬ä¸­..."}, 'Username' : self.username, 'robot': f'http://{cfg.sisi_url}:5000/robot/Listening.jpg'}
                            wsa_server.get_instance().add_cmd(content)
                        concatenated_audio.clear()
                        self.__aLiNls = self.asrclient()
                        task_id = self.__aLiNls.start()
                        while not self.__aLiNls.started:
                            time.sleep(0.01)
                        for i in range(len(self.__history_data) - 1): #è¤°æ’³å¢ dataé¦ã„¤ç¬…é—ˆç´°é‹æ°¬å½‚é–«ä¾Šç´æ©æ¬“å™·é„å½‚é–«ä½¹ç¸ºå¨²è¯²å¢ é¨å‹¯ç…¶æ£°æˆæšŸé¹ç´æµ ãƒ¥å¤å©•å¿”å¸€æ·‡â„ƒä¼…
                            buf = self.__history_data[i]
                            audio_data_list.append(self._bytes_to_int16(buf))
                            if self.ASRMode == "ali":
                                self.__aLiNls.send(buf)
                            else:
                                concatenated_audio.extend(buf)
                        self.__history_data.clear()
                else:#ç¼æ’´æ½«é·é¹ƒç…¶
                    last_mute_time = time.time()
                    if isSpeaking:
                        if time.time() - last_speaking_time > _RELEASE:
                            isSpeaking = False
                            self.__aLiNls.end()
                            util.printInfo(1, self.username, "è¯­éŸ³å¤„ç†ä¸­...")
                            
                            mono_data = self.__concatenate_audio_data(audio_data_list)
                            self._asr_result_seq += 1
                            self.__waitingResult(
                                self.__aLiNls,
                                mono_data,
                                capture_epoch=active_capture_epoch,
                                asr_seq=self._asr_result_seq,
                            )
                            cache_root = cfg.cache_root or "cache_data"
                            os.makedirs(cache_root, exist_ok=True)
                            also_timestamp = not (self.ASRMode == "funasr" or self.ASRMode == "sensevoice")
                            self.__save_audio_to_wav(
                                mono_data,
                                self.sample_rate,
                                os.path.join(cache_root, "input.wav"),
                                also_save_timestamp=also_timestamp
                            )
                            audio_data_list = []
                            active_capture_epoch = int(self._wake_session_epoch)
                
                #é·é¹ƒç…¶æ¶“?
                if isSpeaking:
                    audio_data_list.append(self._bytes_to_int16(mono_data))
                    if self.ASRMode == "ali":
                        self.__aLiNls.send(mono_data)
                    else:
                        concatenated_audio.extend(mono_data)
            except Exception as e:
                util.printInfo(1, self.username, "è¤°æ›¢ç…¶æ¾¶è¾«è§¦: " + str(e))

    def __save_audio_to_wav(self, data, sample_rate, filename, also_save_timestamp=True):
        # ensure int16
        if data.dtype != np.int16:
            data = data.astype(np.int16)

        with wave.open(filename, 'wb') as wf:
            n_channels = 1
            sampwidth = 2
            wf.setnchannels(n_channels)
            wf.setsampwidth(sampwidth)
            wf.setframerate(sample_rate)
            wf.writeframes(data.tobytes())

        if also_save_timestamp:
            cache_root = cfg.cache_root or "cache_data"
            os.makedirs(cache_root, exist_ok=True)
            ts_path = self._make_timestamped_input_path(cache_root)
            with wave.open(ts_path, 'wb') as wf:
                wf.setnchannels(n_channels)
                wf.setsampwidth(sampwidth)
                wf.setframerate(sample_rate)
                wf.writeframes(data.tobytes())
            self.latest_audio_file = ts_path


    def __concatenate_audio_data(self, audio_data_list):
        # çå—™ç–®ç»‰æ®‘é—ŠæŠ½éç‰ˆåµé§æ¥„ç¹›éºãƒ¨æ£é‰?
        data = np.concatenate(audio_data_list)
        return data
    
    #æå½‰æ¶“å“„å´Ÿæ¾¹ä¼´äº¾np.int16
    def __process_audio_data(self, data, channels):
        data = bytearray(data)
        # çå——ç“§é‘ºå‚›æšŸé¹æµ†é¹è´Ÿ numpy éæ‰®ç²
        data = np.frombuffer(data, dtype=np.int16)
        # é–²å¶…éæ‰®ç²é”›å±½çš¢éç‰ˆåµé’å—™é´æ„¬æ¶“ï¼é–¬?
        data = np.reshape(data, (-1, channels))
        # ç€µè§„å¢éˆå¤Šï¼é–¬æ’¶æ®‘éç‰ˆåµæ©æ¶œéªå†²æ½é”›å²€æ•“é´æ„¬å´Ÿæ¾¹ä¼´äº¾
        mono_data = np.mean(data, axis=1).astype(np.int16)
        return mono_data
     
    def _send_to_background_brain(self, audio_data, audio_file_path=None):
        """Send audio to background brain processing asynchronously."""
        try:
            # å¦«â‚¬éŒãƒ¥å¢ é‘´æˆ éƒ´ç¼ç†¸æ§¸éšï¹€æƒé¢?
            if not hasattr(self, '_brain_enabled') or not self._brain_enabled:
                return

            # å¯®å‚›é™æˆ¦â‚¬ä½¸åŸŒé“å¶ˆå‰³ç»¯è¤ç²º
            import threading
            threading.Thread(
                target=self._background_brain_process,
                args=(audio_data, audio_file_path),
                daemon=True
            ).start()

        except Exception as e:
            # åå°å‰è„‘å¤„ç†å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            util.log(2, f"[éŸ³é¢‘åˆ†å‰] åå°å‰è„‘å¤„ç†å¤±è´¥: {e}")

    def _background_brain_process(self, audio_data, audio_file_path=None):
        """Background brain processing that reuses main-pipeline audio files."""
        try:
            # é¦ƒæ•Ÿ é‘¾å³°å½‡è¤°æ’³å¢ æé”›å±¼ç¬‰é–²å¶…é–«æ‘é”›å œå¯Œå¨´ä½ºâ–¼å®¸èŒ¬ç²¡é–«æ‘æ©å›¦ç°¡é”›?
            from sisi_brain.real_brain_system import get_real_brain_system
            brain_system = get_real_brain_system()

            # é‘¾å³°å½‡è¤°æ’³å¢ æé”›å œç¬‰é–«æ‘é”›å²„ä¼©éå¶‰å™¸æ¾¶å¶ˆéå¸®ç´š
            current_round = brain_system.current_round

            if current_round < 3:
                util.log(1, f"[audio_split] warmup_round={current_round}, skip background analysis")
                return

            util.log(1, f"[audio_split] enabled_round={current_round}, start background analysis")
            util.log(1, "[audio_split] backend analysis only, no duplicated full brain pipeline")

            # é¦ƒæ•Ÿ æµ¼æ¨ºå›æµ£è·¨æ•¤æ¶“ç»˜ç¥¦ç»‹å¬ªå‡¡æ·‡æ¿†ç“¨é¨å‹¬æƒæµ è®¹ç´é–¬å®å¤é–²å¶…æ·‡æ¿†ç“¨
            if audio_file_path and os.path.exists(audio_file_path):
                brain_audio_file = audio_file_path
                util.log(1, f"[éŸ³é¢‘åˆ†å‰] âœ… å¤ç”¨ä¸»æµç¨‹éŸ³é¢‘æ–‡ä»¶: {brain_audio_file}")
            else:
                # æ¾¶å›©æ•¤é”›æ°¬é‹æ»„ç—…éˆå¤‰ç´¶éãƒ¦æƒæµ æƒ°çŸ¾å¯°å‹¶ç´éµå¶„ç¹šç€›æ¨»æŸŠé‚å›¦æ¬¢é”›å ¥â‚¬æ°¬çˆ¶æ¶“å¶…ç°²ç’‡ãƒ¨è›‹é’æ‹Œç¹–é–²å²‹ç´š
                brain_audio_file = self.save_buffer_to_file(audio_data)
                util.log(1, f"[éŸ³é¢‘åˆ†å‰] â„¹ï¸ ä¸»æµç¨‹æ— æ–‡ä»¶ï¼Œæ–°å»ºéŸ³é¢‘æ–‡ä»¶: {brain_audio_file}")

            # é—ŠæŠ½é’å——å¼¶æ¶“å¶ˆçšŸé¢ã„¤ä¿Šé­é–¬æ“„ç´é–¬å®å¤é–²å¶…ç’‹å†ªæ•¤
            # é—ŠæŠ½é’å——å¼¶é¨å‹ªç¶”é¢ã„¦æ§¸éšåº¡å½´é—ŠæŠ½é’å—˜ç€½é”›å±¼ç¬‰é„ç•¬éå¯¸æ®‘é“å¶ˆå‰³æ¾¶å‹­æ‚Š
            # æ·‡â„ƒä¼…ç» ï¿ äº¾æ´æ—‡é™æ•±æ¶“è¯²å¢ é‘´æˆ éƒ´ç¼ç†»çšŸé¢?

            util.log(1, "[éŸ³é¢‘åˆ†å‰] ğŸš€ å¼€å§‹åå°éŸ³é¢‘åˆ†æï¼ˆä¸è°ƒç”¨ä¿¡å·é€šé“ï¼‰")

            # é¦ƒå¹† é­ãˆ é—ŠæŠ½é’å——å¼¶é¨å‡·martAudioCollectorç’‹å†ªæ•¤
            try:
                # é¦ƒæ•Ÿ ç’‹å†ªæ•¤SmartAudioCollectoræ©æ¶œéšåº¡å½´é—ŠæŠ½é’å—˜ç€½
                from core.smart_audio_collector import get_smart_audio_collector

                collector = get_smart_audio_collector()
                util.log(1, "[éŸ³é¢‘åˆ†å‰] ğŸ“Š å¼€å§‹SmartAudioCollectoråå°åˆ†æ")

                # é’å—˜ç€½é—ŠæŠ½é‚å›¦æ¬¢
                audio_type, confidence = collector._classify_audio_type(brain_audio_file)
                util.log(1, f"[éŸ³é¢‘åˆ†å‰] ğŸ¯ éŸ³é¢‘åˆ†ç±»ç»“æœ: {audio_type}, ç½®ä¿¡åº¦: {confidence:.3f}")

                # æ¿¡å‚›ç‰é„ç…¶æ¶”æ„¶ç´ç‘™ï¹€å½‚é—Šå……ç®°ç’‡å——åŸ†
                if audio_type == "music" and confidence > 0.6:
                    util.log(1, "[audio_split] music detected, trigger backend music recognition")
                    # é’æ¶˜ç¼“é—ŠæŠ½é—å›¨ç€µç¡…è–„
                    from core.smart_audio_collector import AudioSegment
                    from datetime import datetime

                    segment = AudioSegment(
                        file_path=brain_audio_file,
                        timestamp=datetime.now(),
                        duration=10.0,
                        audio_type=audio_type,
                        confidence=confidence,
                        features={}
                    )

                    # é™æˆ¦â‚¬ä½ºç²°é—Šå……ç®°ç’‡å——åŸ†
                    collector._send_to_music_recognition(segment)

                util.log(1, "[éŸ³é¢‘åˆ†å‰] âœ… SmartAudioCollectoråå°åˆ†æå®Œæˆ")

            except Exception as e:
                util.log(2, f"[éŸ³é¢‘åˆ†å‰] SmartAudioCollectoråˆ†æå¼‚å¸¸: {e}")
                # é¦ƒæ•Ÿ é’å——å¼¶æ¾¶è¾«è§¦æ¶“å¶…å¥–éå¶„å¯Œå¨´ä½ºâ–¼é”›å²€æˆ·ç¼ç¹ç›?

        except Exception as e:
            util.log(2, f"[éŸ³é¢‘åˆ†å‰] å‰è„‘ç³»ç»Ÿåå°å¤„ç†å¼‚å¸¸: {e}")

    def enable_brain_background(self, enabled=True):
        """å¯ç”¨æˆ–ç¦ç”¨åå°å‰è„‘ç³»ç»Ÿã€‚"""
        self._brain_enabled = enabled
        util.log(1, f"[éŸ³é¢‘åˆ†å‰] åå°å‰è„‘ç³»ç»Ÿ: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

    def set_processing(self, processing):
        self.__processing = processing

    def start(self):
        MyThread(target=self.__record).start()

    def stop(self):
        self.__running = False

    @abstractmethod
    def on_speaking(self, text):
        pass

    # TODO: ç€›æ„®è¢«ç€¹ç‚µå¹‡éèœ‚ç¶‹å¨´ä½¹æ½µå©§æ„¶ç´™æ¥¹ï¹€å æ¤‹åº›â‚¬ä½¹æ¹°é¦ç‰ˆæƒæµ èˆµå¨ç¼ƒæˆ ç²¶å¨´ä¾Šç´š
    @abstractmethod
    def get_stream(self):
        pass

    @abstractmethod
    def is_remote(self):
        pass

    def is_active(self):
        """
        å¦«â‚¬éŒãƒ¥ç¶é—Šå†²æ«’é„æƒæ¾¶å‹ªç°¬å¨²è¯²å§©é˜èˆµâ‚¬?
        """
        return self.__running

# æ·‡æ•¼æ¶“å“„æ¬¢æ©ç†·éãƒ¥åš±é?
def get_wsa_server():
    from core import wsa_server
    return wsa_server



